{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q parlai","metadata":{"execution":{"iopub.status.busy":"2022-12-03T15:53:10.761541Z","iopub.execute_input":"2022-12-03T15:53:10.762066Z","iopub.status.idle":"2022-12-03T15:53:54.804125Z","shell.execute_reply.started":"2022-12-03T15:53:10.761955Z","shell.execute_reply":"2022-12-03T15:53:54.802681Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nvirtualenv 20.16.5 requires importlib-metadata>=4.8.3; python_version < \"3.8\", but you have importlib-metadata 4.2.0 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.0 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\npytoolconfig 1.2.2 requires tomli>=2.0; python_version < \"3.11\", but you have tomli 1.2.3 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nnnabla 1.31.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.19.6 which is incompatible.\nmdit-py-plugins 0.3.0 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 0.5.8 which is incompatible.\njupytext 1.13.8 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 0.5.8 which is incompatible.\ngym 0.26.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 4.2.0 which is incompatible.\ngrpcio-status 1.47.0 requires grpcio>=1.47.0, but you have grpcio 1.43.0 which is incompatible.\ngcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2022.8.2 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.21.0 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"Choose completion prev1corr1type1","metadata":{}},{"cell_type":"markdown","source":"Run 1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"execution":{"iopub.status.busy":"2022-12-03T15:54:29.608146Z","iopub.execute_input":"2022-12-03T15:54:29.608580Z","iopub.status.idle":"2022-12-03T15:59:49.370551Z","shell.execute_reply.started":"2022-12-03T15:54:29.608543Z","shell.execute_reply":"2022-12-03T15:59:49.369217Z"},"scrolled":true,"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"15:54:43 | building data: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/pretrained_transformers.tgz\n15:54:43 | Downloading http://parl.ai/downloads/_models/pretrained_transformers/pretrained_transformers.tgz to /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/pretrained_transformers.tgz\nDownloading pretrained_transformers.tgz: 100%|â–ˆ| 4.22G/4.22G [01:06<00:00, 63.8M\n15:58:21 | building dictionary first...\n15:58:21 | No model with opt yet at: /tmp/model1(.opt)\n15:58:21 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n15:58:21 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n15:58:21 | Using CUDA\n15:58:21 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n15:58:21 | num words = 54944\n15:58:27 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n15:58:44 | Total parameters: 128,042,498 (128,042,498 trainable)\n15:58:44 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n15:58:44 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n15:58:44 | Opt:\n15:58:44 |     activation: gelu\n15:58:44 |     adafactor_eps: '(1e-30, 0.001)'\n15:58:44 |     adam_eps: 1e-08\n15:58:44 |     add_p1_after_newln: False\n15:58:44 |     aggregate_micro: False\n15:58:44 |     allow_missing_init_opts: False\n15:58:44 |     attention_dropout: 0.1\n15:58:44 |     batchsize: 20\n15:58:44 |     betas: '(0.9, 0.999)'\n15:58:44 |     bpe_add_prefix_space: None\n15:58:44 |     bpe_debug: False\n15:58:44 |     bpe_dropout: None\n15:58:44 |     bpe_merge: None\n15:58:44 |     bpe_vocab: None\n15:58:44 |     candidates: inline\n15:58:44 |     cap_num_predictions: 100\n15:58:44 |     checkpoint_activations: False\n15:58:44 |     class_weights: None\n15:58:44 |     classes: \"['__notok__', '__ok__']\"\n15:58:44 |     classes_from_file: None\n15:58:44 |     data_parallel: True\n15:58:44 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n15:58:44 |     datatype: train\n15:58:44 |     delimiter: '\\n'\n15:58:44 |     dict_class: parlai.core.dict:DictionaryAgent\n15:58:44 |     dict_endtoken: __start__\n15:58:44 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n15:58:44 |     dict_include_test: False\n15:58:44 |     dict_include_valid: False\n15:58:44 |     dict_initpath: None\n15:58:44 |     dict_language: english\n15:58:44 |     dict_loaded: True\n15:58:44 |     dict_lower: True\n15:58:44 |     dict_max_ngram_size: -1\n15:58:44 |     dict_maxexs: -1\n15:58:44 |     dict_maxtokens: -1\n15:58:44 |     dict_minfreq: 0\n15:58:44 |     dict_nulltoken: __null__\n15:58:44 |     dict_starttoken: __start__\n15:58:44 |     dict_textfields: text,labels\n15:58:44 |     dict_tokenizer: bpe\n15:58:44 |     dict_unktoken: __unk__\n15:58:44 |     display_examples: False\n15:58:44 |     download_path: None\n15:58:44 |     dropout: 0.1\n15:58:44 |     dynamic_batching: None\n15:58:44 |     embedding_projection: random\n15:58:44 |     embedding_size: 768\n15:58:44 |     embedding_type: random\n15:58:44 |     embeddings_scale: False\n15:58:44 |     encode_candidate_vecs: True\n15:58:44 |     encode_candidate_vecs_batchsize: 256\n15:58:44 |     eval_batchsize: None\n15:58:44 |     eval_candidates: inline\n15:58:44 |     eval_dynamic_batching: None\n15:58:44 |     evaltask: None\n15:58:44 |     ffn_size: 3072\n15:58:44 |     final_extra_opt: \n15:58:44 |     fixed_candidate_vecs: reuse\n15:58:44 |     fixed_candidates_path: None\n15:58:44 |     force_fp16_tokens: False\n15:58:44 |     fp16: True\n15:58:44 |     fp16_impl: safe\n15:58:44 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt\n15:58:44 |     fromfile_datatype_extension: False\n15:58:44 |     gpu: -1\n15:58:44 |     gradient_clip: 0.1\n15:58:44 |     hide_labels: False\n15:58:44 |     history_add_global_end_token: None\n15:58:44 |     history_reversed: False\n15:58:44 |     history_size: 20\n15:58:44 |     ignore_bad_candidates: False\n15:58:44 |     ignore_labels: None\n15:58:44 |     image_cropsize: 224\n15:58:44 |     image_mode: raw\n15:58:44 |     image_size: 256\n15:58:44 |     inference: max\n15:58:44 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n15:58:44 |     init_opt: None\n15:58:44 |     interactive_candidates: fixed\n15:58:44 |     interactive_mode: False\n15:58:44 |     invsqrt_lr_decay_gamma: -1\n15:58:44 |     is_debug: False\n15:58:44 |     label_truncate: 72\n15:58:44 |     learn_embeddings: True\n15:58:44 |     learn_positional_embeddings: True\n15:58:44 |     learningrate: 5e-05\n15:58:44 |     load_from_checkpoint: False\n15:58:44 |     load_from_pretrained_ranker: True\n15:58:44 |     log_every_n_secs: 10.0\n15:58:44 |     log_every_n_steps: 50\n15:58:44 |     log_keep_fields: all\n15:58:44 |     loglevel: info\n15:58:44 |     lr_scheduler: reduceonplateau\n15:58:44 |     lr_scheduler_decay: 0.5\n15:58:44 |     lr_scheduler_patience: 3\n15:58:44 |     max_train_steps: -1\n15:58:44 |     max_train_time: 7200.0\n15:58:44 |     memory_attention: sqrt\n15:58:44 |     metrics: default\n15:58:44 |     model: transformer/classifier\n15:58:44 |     model_file: /tmp/model1\n15:58:44 |     model_parallel: False\n15:58:44 |     momentum: 0\n15:58:44 |     multitask_weights: [1]\n15:58:44 |     mutators: None\n15:58:44 |     n_decoder_layers: -1\n15:58:44 |     n_encoder_layers: -1\n15:58:44 |     n_heads: 12\n15:58:44 |     n_layers: 12\n15:58:44 |     n_positions: 1024\n15:58:44 |     n_segments: 2\n15:58:44 |     nesterov: True\n15:58:44 |     no_cuda: False\n15:58:44 |     normalize_sent_emb: False\n15:58:44 |     num_epochs: -1\n15:58:44 |     num_workers: 0\n15:58:44 |     nus: (0.7,)\n15:58:44 |     optimizer: adamax\n15:58:44 |     output_scaling: 0.06\n15:58:44 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n15:58:44 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n15:58:44 |     person_tokens: False\n15:58:44 |     print_scores: False\n15:58:44 |     rank_candidates: False\n15:58:44 |     rank_top_k: -1\n15:58:44 |     reduction_type: mean\n15:58:44 |     ref_class: None\n15:58:44 |     relu_dropout: 0.0\n15:58:44 |     repeat_blocking_heuristic: True\n15:58:44 |     return_cand_scores: False\n15:58:44 |     save_after_valid: True\n15:58:44 |     save_every_n_secs: -1\n15:58:44 |     save_format: conversations\n15:58:44 |     share_encoders: False\n15:58:44 |     share_word_embeddings: False\n15:58:44 |     short_final_eval: False\n15:58:44 |     special_tok_lst: None\n15:58:44 |     split_lines: False\n15:58:44 |     starttime: Dec03_15-58\n15:58:44 |     task: fromfile:parlaiformat\n15:58:44 |     tensorboard_log: False\n15:58:44 |     tensorboard_logdir: None\n15:58:44 |     text_truncate: 360\n15:58:44 |     threshold: 0.5\n15:58:44 |     topk: 5\n15:58:44 |     train_predict: False\n15:58:44 |     truncate: 1024\n15:58:44 |     update_classifier_head_only: False\n15:58:44 |     update_freq: 1\n15:58:44 |     use_memories: False\n15:58:44 |     use_reply: none\n15:58:44 |     validation_cutoff: 1.0\n15:58:44 |     validation_every_n_epochs: -1\n15:58:44 |     validation_every_n_secs: 20.0\n15:58:44 |     validation_every_n_steps: -1\n15:58:44 |     validation_max_exs: -1\n15:58:44 |     validation_metric: accuracy\n15:58:44 |     validation_metric_mode: max\n15:58:44 |     validation_patience: 30\n15:58:44 |     validation_share_agent: False\n15:58:44 |     variant: xlm\n15:58:44 |     verbose: False\n15:58:44 |     wandb_entity: None\n15:58:44 |     wandb_log: False\n15:58:44 |     wandb_name: None\n15:58:44 |     wandb_project: None\n15:58:44 |     warmup_rate: 0.0001\n15:58:44 |     warmup_updates: 1000\n15:58:44 |     weight_decay: None\n15:58:44 |     world_logs: \n15:58:44 |     wrap_memory_encoder: False\n15:58:44 | creating task(s): fromfile:parlaiformat\n15:58:44 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt\n15:58:44 | training...\n15:58:55 | time:10s total_exs:320 total_steps:16 epochs:13.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5437 5.437e-10               .3596                 .7455   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2370            .6456              .5019   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9048 11.42     1 268.4 430.7       0          0 32.04  320   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5437             32768  2.794    .1189 6.081 .6758 8.049e-07 121.6 195.1   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   16 390.1 625.8 1.608        .4910\n\n15:59:04 | time:20s total_exs:1060 total_steps:53 epochs:44.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7703 7.703e-10               .7432                 .8512   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6595            .7922              .7184   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8828 11.47     1 269.3  1057       0          0 78.49  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7703             32768  2.631    .1189 6.008 .6404 2.655e-06 120.2 471.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   53 389.5 1528 3.934        .7675\n\n15:59:04 | creating task(s): fromfile:parlaiformat\n15:59:04 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n15:59:04 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt\n15:59:04 | running eval: valid\n15:59:04 | eval completed in 0.19s\n15:59:04 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9600                 .9231   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9565                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 11.46 161.5  1906       0          0 141.6   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5816 2.655e-06    72 849.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     53 233.5 2757        .9583\n\u001b[0m\n15:59:04 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n15:59:04 | saving best valid model: /tmp/model1\n15:59:04 | Saving dictionary to /tmp/model1.dict\n15:59:09 | saving model checkpoint: /tmp/model1.checkpoint\n15:59:09 | Saving dictionary to /tmp/model1.checkpoint.dict\n15:59:26 | time:42s total_exs:1820 total_steps:91 epochs:75.83\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9711 9.711e-10               .9704                 .9424   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9717                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9450 11.35     1 266.9  1014       0          0 75.99  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9711             32768  2.667    .1189 5.947 .5221 4.555e-06 118.9 451.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   91 385.9 1466 3.808        .9711\n\n15:59:29 | time:45s total_exs:2000 total_steps:100 epochs:83.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 12.01     1 280.1  1015       0          0 72.46  180   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  3.072    .1189 5.889 .3925 5.005e-06 117.8 426.7   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  100 397.9 1442 3.656            1\n\n15:59:29 | running eval: valid\n15:59:29 | eval completed in 0.19s\n15:59:29 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1916       0          0 142.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3293 5.005e-06    72   854       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    100 233.5 2770            1\n\u001b[0m\n15:59:29 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n15:59:29 | saving best valid model: /tmp/model1\n15:59:37 | task solved! stopping.\n15:59:38 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n15:59:38 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n15:59:38 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n15:59:38 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n15:59:38 | Using CUDA\n15:59:38 | loading dictionary from /tmp/model1.dict\n15:59:38 | num words = 54944\n15:59:42 | Loading existing model parameters from /tmp/model1\n15:59:44 | Total parameters: 128,042,498 (128,042,498 trainable)\n15:59:45 | creating task(s): fromfile:parlaiformat\n15:59:45 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n15:59:45 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt\n15:59:45 | running eval: valid\n15:59:45 | eval completed in 0.19s\n15:59:45 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1921       0          0 142.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3293 5.005e-06    72 856.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    100 233.5 2777            1\n\u001b[0m\n15:59:45 | creating task(s): fromfile:parlaiformat\n15:59:45 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n15:59:45 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt\n15:59:45 | running eval: test\n15:59:45 | eval completed in 0.19s\n15:59:45 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1916       0          0 142.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3293 5.005e-06    72   854       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    100 233.5 2770            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:00:39.746577Z","iopub.execute_input":"2022-12-03T16:00:39.746958Z","iopub.status.idle":"2022-12-03T16:01:15.464195Z","shell.execute_reply.started":"2022-12-03T16:00:39.746926Z","shell.execute_reply":"2022-12-03T16:01:15.462948Z"},"scrolled":true,"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"16:00:55 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt)\u001b[0m\n16:00:55 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:00:55 | Using CUDA\n16:00:55 | loading dictionary from /tmp/model1.dict\n16:00:55 | num words = 54944\n16:00:59 | Loading existing model parameters from /tmp/model1\n16:01:06 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:01:07 | Opt:\n16:01:07 |     activation: gelu\n16:01:07 |     adafactor_eps: '[1e-30, 0.001]'\n16:01:07 |     adam_eps: 1e-08\n16:01:07 |     add_p1_after_newln: False\n16:01:07 |     aggregate_micro: False\n16:01:07 |     allow_missing_init_opts: False\n16:01:07 |     area_under_curve_class: None\n16:01:07 |     area_under_curve_digits: -1\n16:01:07 |     attention_dropout: 0.1\n16:01:07 |     batchsize: 40\n16:01:07 |     betas: '[0.9, 0.999]'\n16:01:07 |     bpe_add_prefix_space: None\n16:01:07 |     bpe_debug: False\n16:01:07 |     bpe_dropout: None\n16:01:07 |     bpe_merge: None\n16:01:07 |     bpe_vocab: None\n16:01:07 |     candidates: inline\n16:01:07 |     cap_num_predictions: 100\n16:01:07 |     checkpoint_activations: False\n16:01:07 |     class_weights: None\n16:01:07 |     classes: \"['__notok__', '__ok__']\"\n16:01:07 |     classes_from_file: None\n16:01:07 |     data_parallel: True\n16:01:07 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:01:07 |     datatype: train\n16:01:07 |     delimiter: '\\n'\n16:01:07 |     dict_class: parlai.core.dict:DictionaryAgent\n16:01:07 |     dict_endtoken: __start__\n16:01:07 |     dict_file: /tmp/model1.dict\n16:01:07 |     dict_include_test: False\n16:01:07 |     dict_include_valid: False\n16:01:07 |     dict_initpath: None\n16:01:07 |     dict_language: english\n16:01:07 |     dict_loaded: True\n16:01:07 |     dict_lower: True\n16:01:07 |     dict_max_ngram_size: -1\n16:01:07 |     dict_maxexs: -1\n16:01:07 |     dict_maxtokens: -1\n16:01:07 |     dict_minfreq: 0\n16:01:07 |     dict_nulltoken: __null__\n16:01:07 |     dict_starttoken: __start__\n16:01:07 |     dict_textfields: text,labels\n16:01:07 |     dict_tokenizer: bpe\n16:01:07 |     dict_unktoken: __unk__\n16:01:07 |     display_examples: False\n16:01:07 |     download_path: None\n16:01:07 |     dropout: 0.1\n16:01:07 |     dynamic_batching: None\n16:01:07 |     embedding_projection: random\n16:01:07 |     embedding_size: 768\n16:01:07 |     embedding_type: random\n16:01:07 |     embeddings_scale: False\n16:01:07 |     encode_candidate_vecs: True\n16:01:07 |     encode_candidate_vecs_batchsize: 256\n16:01:07 |     eval_batchsize: None\n16:01:07 |     eval_candidates: inline\n16:01:07 |     eval_dynamic_batching: None\n16:01:07 |     evaltask: None\n16:01:07 |     ffn_size: 3072\n16:01:07 |     final_extra_opt: \n16:01:07 |     fixed_candidate_vecs: reuse\n16:01:07 |     fixed_candidates_path: None\n16:01:07 |     force_fp16_tokens: True\n16:01:07 |     fp16: True\n16:01:07 |     fp16_impl: safe\n16:01:07 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-a.txt\n16:01:07 |     fromfile_datatype_extension: False\n16:01:07 |     gpu: -1\n16:01:07 |     gradient_clip: 0.1\n16:01:07 |     hide_labels: False\n16:01:07 |     history_add_global_end_token: None\n16:01:07 |     history_reversed: False\n16:01:07 |     history_size: 20\n16:01:07 |     ignore_bad_candidates: False\n16:01:07 |     ignore_labels: None\n16:01:07 |     image_cropsize: 224\n16:01:07 |     image_mode: raw\n16:01:07 |     image_size: 256\n16:01:07 |     inference: max\n16:01:07 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:01:07 |     init_opt: None\n16:01:07 |     interactive_candidates: fixed\n16:01:07 |     interactive_mode: False\n16:01:07 |     invsqrt_lr_decay_gamma: -1\n16:01:07 |     is_debug: False\n16:01:07 |     label_truncate: 72\n16:01:07 |     learn_embeddings: True\n16:01:07 |     learn_positional_embeddings: True\n16:01:07 |     learningrate: 5e-05\n16:01:07 |     load_from_pretrained_ranker: True\n16:01:07 |     log_every_n_secs: 10.0\n16:01:07 |     log_every_n_steps: 50\n16:01:07 |     log_keep_fields: all\n16:01:07 |     loglevel: info\n16:01:07 |     lr_scheduler: reduceonplateau\n16:01:07 |     lr_scheduler_decay: 0.5\n16:01:07 |     lr_scheduler_patience: 3\n16:01:07 |     max_train_steps: -1\n16:01:07 |     max_train_time: 7200.0\n16:01:07 |     memory_attention: sqrt\n16:01:07 |     metrics: default\n16:01:07 |     model: transformer/classifier\n16:01:07 |     model_file: /tmp/model1\n16:01:07 |     model_parallel: False\n16:01:07 |     momentum: 0\n16:01:07 |     multitask_weights: [1]\n16:01:07 |     mutators: None\n16:01:07 |     n_decoder_layers: -1\n16:01:07 |     n_encoder_layers: -1\n16:01:07 |     n_heads: 12\n16:01:07 |     n_layers: 12\n16:01:07 |     n_positions: 1024\n16:01:07 |     n_segments: 2\n16:01:07 |     nesterov: True\n16:01:07 |     no_cuda: False\n16:01:07 |     normalize_sent_emb: False\n16:01:07 |     num_epochs: -1\n16:01:07 |     num_examples: -1\n16:01:07 |     num_workers: 0\n16:01:07 |     nus: [0.7]\n16:01:07 |     optimizer: adamax\n16:01:07 |     output_scaling: 0.06\n16:01:07 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n16:01:07 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:01:07 |     person_tokens: False\n16:01:07 |     print_scores: False\n16:01:07 |     rank_candidates: False\n16:01:07 |     rank_top_k: -1\n16:01:07 |     reduction_type: mean\n16:01:07 |     ref_class: None\n16:01:07 |     relu_dropout: 0.0\n16:01:07 |     repeat_blocking_heuristic: True\n16:01:07 |     report_filename: \n16:01:07 |     return_cand_scores: False\n16:01:07 |     save_after_valid: True\n16:01:07 |     save_every_n_secs: -1\n16:01:07 |     save_format: conversations\n16:01:07 |     share_encoders: False\n16:01:07 |     share_word_embeddings: False\n16:01:07 |     short_final_eval: False\n16:01:07 |     special_tok_lst: None\n16:01:07 |     split_lines: False\n16:01:07 |     starttime: Dec03_15-58\n16:01:07 |     task: fromfile:parlaiformat\n16:01:07 |     tensorboard_log: False\n16:01:07 |     tensorboard_logdir: None\n16:01:07 |     text_truncate: 360\n16:01:07 |     threshold: 0.5\n16:01:07 |     topk: 5\n16:01:07 |     train_predict: False\n16:01:07 |     truncate: 1024\n16:01:07 |     update_classifier_head_only: False\n16:01:07 |     update_freq: 1\n16:01:07 |     use_memories: False\n16:01:07 |     use_reply: none\n16:01:07 |     validation_cutoff: 1.0\n16:01:07 |     validation_every_n_epochs: -1\n16:01:07 |     validation_every_n_secs: 20.0\n16:01:07 |     validation_every_n_steps: -1\n16:01:07 |     validation_max_exs: -1\n16:01:07 |     validation_metric: accuracy\n16:01:07 |     validation_metric_mode: max\n16:01:07 |     validation_patience: 30\n16:01:07 |     validation_share_agent: False\n16:01:07 |     variant: xlm\n16:01:07 |     verbose: False\n16:01:07 |     wandb_entity: None\n16:01:07 |     wandb_log: False\n16:01:07 |     wandb_name: None\n16:01:07 |     wandb_project: None\n16:01:07 |     warmup_rate: 0.0001\n16:01:07 |     warmup_updates: 1000\n16:01:07 |     weight_decay: None\n16:01:07 |     world_logs: \n16:01:07 |     wrap_memory_encoder: False\n16:01:07 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:01:07 | creating task(s): fromfile:parlaiformat\n16:01:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:01:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-a.txt\n16:01:13 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8700 8.7e-10               .8646                 .9326   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8058            .8750              .8198   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9381 11.13 525.2 474.4       0          0 36.13  200 .8700   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.03 .5538 5.005e-06 241.2 217.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    100 766.4 692.2        .8696\u001b[0m\n16:01:13 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8700 8.7e-10               .8646                 .9326   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8058            .8750              .8198   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9381 11.13 525.2 474.4       0          0 36.13  200 .8700   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.03 .5538 5.005e-06 241.2 217.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    100 766.4 692.2        .8696\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:01:28.202155Z","iopub.execute_input":"2022-12-03T16:01:28.202557Z","iopub.status.idle":"2022-12-03T16:01:54.665973Z","shell.execute_reply.started":"2022-12-03T16:01:28.202521Z","shell.execute_reply":"2022-12-03T16:01:54.664780Z"},"scrolled":true,"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"16:01:35 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_valid.txt)\u001b[0m\n16:01:35 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:01:35 | Using CUDA\n16:01:35 | loading dictionary from /tmp/model1.dict\n16:01:35 | num words = 54944\n16:01:40 | Loading existing model parameters from /tmp/model1\n16:01:45 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:01:47 | Opt:\n16:01:47 |     activation: gelu\n16:01:47 |     adafactor_eps: '[1e-30, 0.001]'\n16:01:47 |     adam_eps: 1e-08\n16:01:47 |     add_p1_after_newln: False\n16:01:47 |     aggregate_micro: False\n16:01:47 |     allow_missing_init_opts: False\n16:01:47 |     area_under_curve_class: None\n16:01:47 |     area_under_curve_digits: -1\n16:01:47 |     attention_dropout: 0.1\n16:01:47 |     batchsize: 40\n16:01:47 |     betas: '[0.9, 0.999]'\n16:01:47 |     bpe_add_prefix_space: None\n16:01:47 |     bpe_debug: False\n16:01:47 |     bpe_dropout: None\n16:01:47 |     bpe_merge: None\n16:01:47 |     bpe_vocab: None\n16:01:47 |     candidates: inline\n16:01:47 |     cap_num_predictions: 100\n16:01:47 |     checkpoint_activations: False\n16:01:47 |     class_weights: None\n16:01:47 |     classes: \"['__notok__', '__ok__']\"\n16:01:47 |     classes_from_file: None\n16:01:47 |     data_parallel: True\n16:01:47 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:01:47 |     datatype: train\n16:01:47 |     delimiter: '\\n'\n16:01:47 |     dict_class: parlai.core.dict:DictionaryAgent\n16:01:47 |     dict_endtoken: __start__\n16:01:47 |     dict_file: /tmp/model1.dict\n16:01:47 |     dict_include_test: False\n16:01:47 |     dict_include_valid: False\n16:01:47 |     dict_initpath: None\n16:01:47 |     dict_language: english\n16:01:47 |     dict_loaded: True\n16:01:47 |     dict_lower: True\n16:01:47 |     dict_max_ngram_size: -1\n16:01:47 |     dict_maxexs: -1\n16:01:47 |     dict_maxtokens: -1\n16:01:47 |     dict_minfreq: 0\n16:01:47 |     dict_nulltoken: __null__\n16:01:47 |     dict_starttoken: __start__\n16:01:47 |     dict_textfields: text,labels\n16:01:47 |     dict_tokenizer: bpe\n16:01:47 |     dict_unktoken: __unk__\n16:01:47 |     display_examples: False\n16:01:47 |     download_path: None\n16:01:47 |     dropout: 0.1\n16:01:47 |     dynamic_batching: None\n16:01:47 |     embedding_projection: random\n16:01:47 |     embedding_size: 768\n16:01:47 |     embedding_type: random\n16:01:47 |     embeddings_scale: False\n16:01:47 |     encode_candidate_vecs: True\n16:01:47 |     encode_candidate_vecs_batchsize: 256\n16:01:47 |     eval_batchsize: None\n16:01:47 |     eval_candidates: inline\n16:01:47 |     eval_dynamic_batching: None\n16:01:47 |     evaltask: None\n16:01:47 |     ffn_size: 3072\n16:01:47 |     final_extra_opt: \n16:01:47 |     fixed_candidate_vecs: reuse\n16:01:47 |     fixed_candidates_path: None\n16:01:47 |     force_fp16_tokens: True\n16:01:47 |     fp16: True\n16:01:47 |     fp16_impl: safe\n16:01:47 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-b.txt\n16:01:47 |     fromfile_datatype_extension: False\n16:01:47 |     gpu: -1\n16:01:47 |     gradient_clip: 0.1\n16:01:47 |     hide_labels: False\n16:01:47 |     history_add_global_end_token: None\n16:01:47 |     history_reversed: False\n16:01:47 |     history_size: 20\n16:01:47 |     ignore_bad_candidates: False\n16:01:47 |     ignore_labels: None\n16:01:47 |     image_cropsize: 224\n16:01:47 |     image_mode: raw\n16:01:47 |     image_size: 256\n16:01:47 |     inference: max\n16:01:47 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:01:47 |     init_opt: None\n16:01:47 |     interactive_candidates: fixed\n16:01:47 |     interactive_mode: False\n16:01:47 |     invsqrt_lr_decay_gamma: -1\n16:01:47 |     is_debug: False\n16:01:47 |     label_truncate: 72\n16:01:47 |     learn_embeddings: True\n16:01:47 |     learn_positional_embeddings: True\n16:01:47 |     learningrate: 5e-05\n16:01:47 |     load_from_pretrained_ranker: True\n16:01:47 |     log_every_n_secs: 10.0\n16:01:47 |     log_every_n_steps: 50\n16:01:47 |     log_keep_fields: all\n16:01:47 |     loglevel: info\n16:01:47 |     lr_scheduler: reduceonplateau\n16:01:47 |     lr_scheduler_decay: 0.5\n16:01:47 |     lr_scheduler_patience: 3\n16:01:47 |     max_train_steps: -1\n16:01:47 |     max_train_time: 7200.0\n16:01:47 |     memory_attention: sqrt\n16:01:47 |     metrics: default\n16:01:47 |     model: transformer/classifier\n16:01:47 |     model_file: /tmp/model1\n16:01:47 |     model_parallel: False\n16:01:47 |     momentum: 0\n16:01:47 |     multitask_weights: [1]\n16:01:47 |     mutators: None\n16:01:47 |     n_decoder_layers: -1\n16:01:47 |     n_encoder_layers: -1\n16:01:47 |     n_heads: 12\n16:01:47 |     n_layers: 12\n16:01:47 |     n_positions: 1024\n16:01:47 |     n_segments: 2\n16:01:47 |     nesterov: True\n16:01:47 |     no_cuda: False\n16:01:47 |     normalize_sent_emb: False\n16:01:47 |     num_epochs: -1\n16:01:47 |     num_examples: -1\n16:01:47 |     num_workers: 0\n16:01:47 |     nus: [0.7]\n16:01:47 |     optimizer: adamax\n16:01:47 |     output_scaling: 0.06\n16:01:47 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n16:01:47 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:01:47 |     person_tokens: False\n16:01:47 |     print_scores: False\n16:01:47 |     rank_candidates: False\n16:01:47 |     rank_top_k: -1\n16:01:47 |     reduction_type: mean\n16:01:47 |     ref_class: None\n16:01:47 |     relu_dropout: 0.0\n16:01:47 |     repeat_blocking_heuristic: True\n16:01:47 |     report_filename: \n16:01:47 |     return_cand_scores: False\n16:01:47 |     save_after_valid: True\n16:01:47 |     save_every_n_secs: -1\n16:01:47 |     save_format: conversations\n16:01:47 |     share_encoders: False\n16:01:47 |     share_word_embeddings: False\n16:01:47 |     short_final_eval: False\n16:01:47 |     special_tok_lst: None\n16:01:47 |     split_lines: False\n16:01:47 |     starttime: Dec03_15-58\n16:01:47 |     task: fromfile:parlaiformat\n16:01:47 |     tensorboard_log: False\n16:01:47 |     tensorboard_logdir: None\n16:01:47 |     text_truncate: 360\n16:01:47 |     threshold: 0.5\n16:01:47 |     topk: 5\n16:01:47 |     train_predict: False\n16:01:47 |     truncate: 1024\n16:01:47 |     update_classifier_head_only: False\n16:01:47 |     update_freq: 1\n16:01:47 |     use_memories: False\n16:01:47 |     use_reply: none\n16:01:47 |     validation_cutoff: 1.0\n16:01:47 |     validation_every_n_epochs: -1\n16:01:47 |     validation_every_n_secs: 20.0\n16:01:47 |     validation_every_n_steps: -1\n16:01:47 |     validation_max_exs: -1\n16:01:47 |     validation_metric: accuracy\n16:01:47 |     validation_metric_mode: max\n16:01:47 |     validation_patience: 30\n16:01:47 |     validation_share_agent: False\n16:01:47 |     variant: xlm\n16:01:47 |     verbose: False\n16:01:47 |     wandb_entity: None\n16:01:47 |     wandb_log: False\n16:01:47 |     wandb_name: None\n16:01:47 |     wandb_project: None\n16:01:47 |     warmup_rate: 0.0001\n16:01:47 |     warmup_updates: 1000\n16:01:47 |     weight_decay: None\n16:01:47 |     world_logs: \n16:01:47 |     wrap_memory_encoder: False\n16:01:47 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:01:47 | creating task(s): fromfile:parlaiformat\n16:01:47 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:01:47 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run1/data_train-b.txt\n16:01:53 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1300 1.3e-10              .06452                .06742   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                    .06186            .1869              .1802   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1942 11.13 525.2 489.4       0          0 37.27  200 .1300   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.97 .8940 5.005e-06 238.8 222.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    100  764  712        .1276\u001b[0m\n16:01:53 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1300 1.3e-10              .06452                .06742   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                    .06186            .1869              .1802   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1942 11.13 525.2 489.4       0          0 37.27  200 .1300   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.97 .8940 5.005e-06 238.8 222.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    100  764  712        .1276\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:02:06.840540Z","iopub.execute_input":"2022-12-03T16:02:06.841037Z","iopub.status.idle":"2022-12-03T16:02:08.117689Z","shell.execute_reply.started":"2022-12-03T16:02:06.840998Z","shell.execute_reply":"2022-12-03T16:02:08.116412Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:30:57.880836Z","iopub.execute_input":"2022-12-03T16:30:57.881744Z","iopub.status.idle":"2022-12-03T16:32:23.642312Z","shell.execute_reply.started":"2022-12-03T16:30:57.881707Z","shell.execute_reply":"2022-12-03T16:32:23.641117Z"},"scrolled":true,"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"16:31:04 | building dictionary first...\n16:31:04 | No model with opt yet at: /tmp/model2(.opt)\n16:31:04 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:31:04 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:31:04 | Using CUDA\n16:31:04 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:31:05 | num words = 54944\n16:31:09 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:31:19 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:31:19 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:31:19 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:31:19 | Opt:\n16:31:19 |     activation: gelu\n16:31:19 |     adafactor_eps: '(1e-30, 0.001)'\n16:31:19 |     adam_eps: 1e-08\n16:31:19 |     add_p1_after_newln: False\n16:31:19 |     aggregate_micro: False\n16:31:19 |     allow_missing_init_opts: False\n16:31:19 |     attention_dropout: 0.1\n16:31:19 |     batchsize: 20\n16:31:19 |     betas: '(0.9, 0.999)'\n16:31:19 |     bpe_add_prefix_space: None\n16:31:19 |     bpe_debug: False\n16:31:19 |     bpe_dropout: None\n16:31:19 |     bpe_merge: None\n16:31:19 |     bpe_vocab: None\n16:31:19 |     candidates: inline\n16:31:19 |     cap_num_predictions: 100\n16:31:19 |     checkpoint_activations: False\n16:31:19 |     class_weights: None\n16:31:19 |     classes: \"['__notok__', '__ok__']\"\n16:31:19 |     classes_from_file: None\n16:31:19 |     data_parallel: True\n16:31:19 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:31:19 |     datatype: train\n16:31:19 |     delimiter: '\\n'\n16:31:19 |     dict_class: parlai.core.dict:DictionaryAgent\n16:31:19 |     dict_endtoken: __start__\n16:31:19 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:31:19 |     dict_include_test: False\n16:31:19 |     dict_include_valid: False\n16:31:19 |     dict_initpath: None\n16:31:19 |     dict_language: english\n16:31:19 |     dict_loaded: True\n16:31:19 |     dict_lower: True\n16:31:19 |     dict_max_ngram_size: -1\n16:31:19 |     dict_maxexs: -1\n16:31:19 |     dict_maxtokens: -1\n16:31:19 |     dict_minfreq: 0\n16:31:19 |     dict_nulltoken: __null__\n16:31:19 |     dict_starttoken: __start__\n16:31:19 |     dict_textfields: text,labels\n16:31:19 |     dict_tokenizer: bpe\n16:31:19 |     dict_unktoken: __unk__\n16:31:19 |     display_examples: False\n16:31:19 |     download_path: None\n16:31:19 |     dropout: 0.1\n16:31:19 |     dynamic_batching: None\n16:31:19 |     embedding_projection: random\n16:31:19 |     embedding_size: 768\n16:31:19 |     embedding_type: random\n16:31:19 |     embeddings_scale: False\n16:31:19 |     encode_candidate_vecs: True\n16:31:19 |     encode_candidate_vecs_batchsize: 256\n16:31:19 |     eval_batchsize: None\n16:31:19 |     eval_candidates: inline\n16:31:19 |     eval_dynamic_batching: None\n16:31:19 |     evaltask: None\n16:31:19 |     ffn_size: 3072\n16:31:19 |     final_extra_opt: \n16:31:19 |     fixed_candidate_vecs: reuse\n16:31:19 |     fixed_candidates_path: None\n16:31:19 |     force_fp16_tokens: False\n16:31:19 |     fp16: True\n16:31:19 |     fp16_impl: safe\n16:31:19 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt\n16:31:19 |     fromfile_datatype_extension: False\n16:31:19 |     gpu: -1\n16:31:19 |     gradient_clip: 0.1\n16:31:19 |     hide_labels: False\n16:31:19 |     history_add_global_end_token: None\n16:31:19 |     history_reversed: False\n16:31:19 |     history_size: 20\n16:31:19 |     ignore_bad_candidates: False\n16:31:19 |     ignore_labels: None\n16:31:19 |     image_cropsize: 224\n16:31:19 |     image_mode: raw\n16:31:19 |     image_size: 256\n16:31:19 |     inference: max\n16:31:19 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:31:19 |     init_opt: None\n16:31:19 |     interactive_candidates: fixed\n16:31:19 |     interactive_mode: False\n16:31:19 |     invsqrt_lr_decay_gamma: -1\n16:31:19 |     is_debug: False\n16:31:19 |     label_truncate: 72\n16:31:19 |     learn_embeddings: True\n16:31:19 |     learn_positional_embeddings: True\n16:31:19 |     learningrate: 5e-05\n16:31:19 |     load_from_checkpoint: False\n16:31:19 |     load_from_pretrained_ranker: True\n16:31:19 |     log_every_n_secs: 10.0\n16:31:19 |     log_every_n_steps: 50\n16:31:19 |     log_keep_fields: all\n16:31:19 |     loglevel: info\n16:31:19 |     lr_scheduler: reduceonplateau\n16:31:19 |     lr_scheduler_decay: 0.5\n16:31:19 |     lr_scheduler_patience: 3\n16:31:19 |     max_train_steps: -1\n16:31:19 |     max_train_time: 7200.0\n16:31:19 |     memory_attention: sqrt\n16:31:19 |     metrics: default\n16:31:19 |     model: transformer/classifier\n16:31:19 |     model_file: /tmp/model2\n16:31:19 |     model_parallel: False\n16:31:19 |     momentum: 0\n16:31:19 |     multitask_weights: [1]\n16:31:19 |     mutators: None\n16:31:19 |     n_decoder_layers: -1\n16:31:19 |     n_encoder_layers: -1\n16:31:19 |     n_heads: 12\n16:31:19 |     n_layers: 12\n16:31:19 |     n_positions: 1024\n16:31:19 |     n_segments: 2\n16:31:19 |     nesterov: True\n16:31:19 |     no_cuda: False\n16:31:19 |     normalize_sent_emb: False\n16:31:19 |     num_epochs: -1\n16:31:19 |     num_workers: 0\n16:31:19 |     nus: (0.7,)\n16:31:19 |     optimizer: adamax\n16:31:19 |     output_scaling: 0.06\n16:31:19 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n16:31:19 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:31:19 |     person_tokens: False\n16:31:19 |     print_scores: False\n16:31:19 |     rank_candidates: False\n16:31:19 |     rank_top_k: -1\n16:31:19 |     reduction_type: mean\n16:31:19 |     ref_class: None\n16:31:19 |     relu_dropout: 0.0\n16:31:19 |     repeat_blocking_heuristic: True\n16:31:19 |     return_cand_scores: False\n16:31:19 |     save_after_valid: True\n16:31:19 |     save_every_n_secs: -1\n16:31:19 |     save_format: conversations\n16:31:19 |     share_encoders: False\n16:31:19 |     share_word_embeddings: False\n16:31:19 |     short_final_eval: False\n16:31:19 |     special_tok_lst: None\n16:31:19 |     split_lines: False\n16:31:19 |     starttime: Dec03_16-31\n16:31:19 |     task: fromfile:parlaiformat\n16:31:19 |     tensorboard_log: False\n16:31:19 |     tensorboard_logdir: None\n16:31:19 |     text_truncate: 360\n16:31:19 |     threshold: 0.5\n16:31:19 |     topk: 5\n16:31:19 |     train_predict: False\n16:31:19 |     truncate: 1024\n16:31:19 |     update_classifier_head_only: False\n16:31:19 |     update_freq: 1\n16:31:19 |     use_memories: False\n16:31:19 |     use_reply: none\n16:31:19 |     validation_cutoff: 1.0\n16:31:19 |     validation_every_n_epochs: -1\n16:31:19 |     validation_every_n_secs: 20.0\n16:31:19 |     validation_every_n_steps: -1\n16:31:19 |     validation_max_exs: -1\n16:31:19 |     validation_metric: accuracy\n16:31:19 |     validation_metric_mode: max\n16:31:19 |     validation_patience: 30\n16:31:19 |     validation_share_agent: False\n16:31:19 |     variant: xlm\n16:31:19 |     verbose: False\n16:31:19 |     wandb_entity: None\n16:31:19 |     wandb_log: False\n16:31:19 |     wandb_name: None\n16:31:19 |     wandb_project: None\n16:31:19 |     warmup_rate: 0.0001\n16:31:19 |     warmup_updates: 1000\n16:31:19 |     weight_decay: None\n16:31:19 |     world_logs: \n16:31:19 |     wrap_memory_encoder: False\n16:31:20 | creating task(s): fromfile:parlaiformat\n16:31:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt\n16:31:20 | training...\n16:31:30 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4738 4.738e-10               .4348                 .4427   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4271            .5078              .5000   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5158 11.23     1 264.5 551.6       0          0  41.7  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4738             32768  2.828    .1206 5.948 .6990 1.055e-06   119   248   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps  ups  weighted_f1  \n         0          0                   21 383.5 799.6 2.09        .4732\n\n16:31:40 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7797 7.797e-10               .7578                 .8416   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6892            .7980              .7368   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8703 11.38     1 267.6  1026       0          0 76.67  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7797             32768  2.811    .1207     6 .6429 2.905e-06   120   460   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 387.6 1486 3.843        .7779\n\n16:31:40 | creating task(s): fromfile:parlaiformat\n16:31:40 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:31:40 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt\n16:31:40 | running eval: valid\n16:31:40 | eval completed in 0.19s\n16:31:40 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9167 9.167e-10               .9091                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8333            .9231              .8571   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                      1    11   156  1845       0          0 141.9   24 .9167   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5758 2.905e-06    72 851.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2696        .9161\n\u001b[0m\n16:31:40 | \u001b[1;32mnew best accuracy: 0.9167\u001b[0m\n16:31:40 | saving best valid model: /tmp/model2\n16:31:40 | Saving dictionary to /tmp/model2.dict\n16:31:44 | saving model checkpoint: /tmp/model2.checkpoint\n16:31:44 | Saving dictionary to /tmp/model2.checkpoint.dict\n16:32:00 | time:40s total_exs:1860 total_steps:93 epochs:77.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9329 9.329e-10               .9271                 .9967   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8667            .9377              .8850   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9972  11.3     1   266 919.8       0          0 69.15  700   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9329             32768  2.891    .1207 5.986 .5082 4.655e-06 119.7 413.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   93 385.7 1334 3.466        .9325\n\n16:32:04 | time:44s total_exs:2160 total_steps:108 epochs:90.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .9800 9.8e-10               .9821                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9649            .9773              .9556   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 10.57     1 251.5 966.8       0          0 76.89  300   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9800             32768  2.746    .1207  6.14 .3748 5.404e-06 122.8 472.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  108 374.3 1439 3.867        .9800\n\n16:32:04 | running eval: valid\n16:32:04 | eval completed in 0.19s\n16:32:04 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1836       0          0 141.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0809     6 .2747 5.404e-06    72 847.3       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    108  228 2683            1\n\u001b[0m\n16:32:04 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9167)\u001b[0m\n16:32:04 | saving best valid model: /tmp/model2\n16:32:14 | task solved! stopping.\n16:32:14 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:32:14 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:32:14 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:32:14 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:32:14 | Using CUDA\n16:32:14 | loading dictionary from /tmp/model2.dict\n16:32:14 | num words = 54944\n16:32:18 | Loading existing model parameters from /tmp/model2\n16:32:20 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:32:21 | creating task(s): fromfile:parlaiformat\n16:32:21 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:32:21 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt\n16:32:21 | running eval: valid\n16:32:21 | eval completed in 0.20s\n16:32:21 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1739       0          0 133.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2747 5.404e-06    72 802.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    108  228 2542            1\n\u001b[0m\n16:32:21 | creating task(s): fromfile:parlaiformat\n16:32:21 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:32:21 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt\n16:32:21 | running eval: test\n16:32:21 | eval completed in 0.19s\n16:32:21 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1812       0          0 139.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2747 5.404e-06    72 836.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    108  228 2649            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-03T16:32:42.538228Z","iopub.execute_input":"2022-12-03T16:32:42.539219Z","iopub.status.idle":"2022-12-03T16:33:09.829160Z","shell.execute_reply.started":"2022-12-03T16:32:42.539163Z","shell.execute_reply":"2022-12-03T16:33:09.827966Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"16:32:50 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt)\u001b[0m\n16:32:50 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:32:50 | Using CUDA\n16:32:50 | loading dictionary from /tmp/model2.dict\n16:32:51 | num words = 54944\n16:32:55 | Loading existing model parameters from /tmp/model2\n16:33:01 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:33:02 | Opt:\n16:33:02 |     activation: gelu\n16:33:02 |     adafactor_eps: '[1e-30, 0.001]'\n16:33:02 |     adam_eps: 1e-08\n16:33:02 |     add_p1_after_newln: False\n16:33:02 |     aggregate_micro: False\n16:33:02 |     allow_missing_init_opts: False\n16:33:02 |     area_under_curve_class: None\n16:33:02 |     area_under_curve_digits: -1\n16:33:02 |     attention_dropout: 0.1\n16:33:02 |     batchsize: 40\n16:33:02 |     betas: '[0.9, 0.999]'\n16:33:02 |     bpe_add_prefix_space: None\n16:33:02 |     bpe_debug: False\n16:33:02 |     bpe_dropout: None\n16:33:02 |     bpe_merge: None\n16:33:02 |     bpe_vocab: None\n16:33:02 |     candidates: inline\n16:33:02 |     cap_num_predictions: 100\n16:33:02 |     checkpoint_activations: False\n16:33:02 |     class_weights: None\n16:33:02 |     classes: \"['__notok__', '__ok__']\"\n16:33:02 |     classes_from_file: None\n16:33:02 |     data_parallel: True\n16:33:02 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:33:02 |     datatype: train\n16:33:02 |     delimiter: '\\n'\n16:33:02 |     dict_class: parlai.core.dict:DictionaryAgent\n16:33:02 |     dict_endtoken: __start__\n16:33:02 |     dict_file: /tmp/model2.dict\n16:33:02 |     dict_include_test: False\n16:33:02 |     dict_include_valid: False\n16:33:02 |     dict_initpath: None\n16:33:02 |     dict_language: english\n16:33:02 |     dict_loaded: True\n16:33:02 |     dict_lower: True\n16:33:02 |     dict_max_ngram_size: -1\n16:33:02 |     dict_maxexs: -1\n16:33:02 |     dict_maxtokens: -1\n16:33:02 |     dict_minfreq: 0\n16:33:02 |     dict_nulltoken: __null__\n16:33:02 |     dict_starttoken: __start__\n16:33:02 |     dict_textfields: text,labels\n16:33:02 |     dict_tokenizer: bpe\n16:33:02 |     dict_unktoken: __unk__\n16:33:02 |     display_examples: False\n16:33:02 |     download_path: None\n16:33:02 |     dropout: 0.1\n16:33:02 |     dynamic_batching: None\n16:33:02 |     embedding_projection: random\n16:33:02 |     embedding_size: 768\n16:33:02 |     embedding_type: random\n16:33:02 |     embeddings_scale: False\n16:33:02 |     encode_candidate_vecs: True\n16:33:02 |     encode_candidate_vecs_batchsize: 256\n16:33:02 |     eval_batchsize: None\n16:33:02 |     eval_candidates: inline\n16:33:02 |     eval_dynamic_batching: None\n16:33:02 |     evaltask: None\n16:33:02 |     ffn_size: 3072\n16:33:02 |     final_extra_opt: \n16:33:02 |     fixed_candidate_vecs: reuse\n16:33:02 |     fixed_candidates_path: None\n16:33:02 |     force_fp16_tokens: True\n16:33:02 |     fp16: True\n16:33:02 |     fp16_impl: safe\n16:33:02 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-a.txt\n16:33:02 |     fromfile_datatype_extension: False\n16:33:02 |     gpu: -1\n16:33:02 |     gradient_clip: 0.1\n16:33:02 |     hide_labels: False\n16:33:02 |     history_add_global_end_token: None\n16:33:02 |     history_reversed: False\n16:33:02 |     history_size: 20\n16:33:02 |     ignore_bad_candidates: False\n16:33:02 |     ignore_labels: None\n16:33:02 |     image_cropsize: 224\n16:33:02 |     image_mode: raw\n16:33:02 |     image_size: 256\n16:33:02 |     inference: max\n16:33:02 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:33:02 |     init_opt: None\n16:33:02 |     interactive_candidates: fixed\n16:33:02 |     interactive_mode: False\n16:33:02 |     invsqrt_lr_decay_gamma: -1\n16:33:02 |     is_debug: False\n16:33:02 |     label_truncate: 72\n16:33:02 |     learn_embeddings: True\n16:33:02 |     learn_positional_embeddings: True\n16:33:02 |     learningrate: 5e-05\n16:33:02 |     load_from_pretrained_ranker: True\n16:33:02 |     log_every_n_secs: 10.0\n16:33:02 |     log_every_n_steps: 50\n16:33:02 |     log_keep_fields: all\n16:33:02 |     loglevel: info\n16:33:02 |     lr_scheduler: reduceonplateau\n16:33:02 |     lr_scheduler_decay: 0.5\n16:33:02 |     lr_scheduler_patience: 3\n16:33:02 |     max_train_steps: -1\n16:33:02 |     max_train_time: 7200.0\n16:33:02 |     memory_attention: sqrt\n16:33:02 |     metrics: default\n16:33:02 |     model: transformer/classifier\n16:33:02 |     model_file: /tmp/model2\n16:33:02 |     model_parallel: False\n16:33:02 |     momentum: 0\n16:33:02 |     multitask_weights: [1]\n16:33:02 |     mutators: None\n16:33:02 |     n_decoder_layers: -1\n16:33:02 |     n_encoder_layers: -1\n16:33:02 |     n_heads: 12\n16:33:02 |     n_layers: 12\n16:33:02 |     n_positions: 1024\n16:33:02 |     n_segments: 2\n16:33:02 |     nesterov: True\n16:33:02 |     no_cuda: False\n16:33:02 |     normalize_sent_emb: False\n16:33:02 |     num_epochs: -1\n16:33:02 |     num_examples: -1\n16:33:02 |     num_workers: 0\n16:33:02 |     nus: [0.7]\n16:33:02 |     optimizer: adamax\n16:33:02 |     output_scaling: 0.06\n16:33:02 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n16:33:02 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:33:02 |     person_tokens: False\n16:33:02 |     print_scores: False\n16:33:02 |     rank_candidates: False\n16:33:02 |     rank_top_k: -1\n16:33:02 |     reduction_type: mean\n16:33:02 |     ref_class: None\n16:33:02 |     relu_dropout: 0.0\n16:33:02 |     repeat_blocking_heuristic: True\n16:33:02 |     report_filename: \n16:33:02 |     return_cand_scores: False\n16:33:02 |     save_after_valid: True\n16:33:02 |     save_every_n_secs: -1\n16:33:02 |     save_format: conversations\n16:33:02 |     share_encoders: False\n16:33:02 |     share_word_embeddings: False\n16:33:02 |     short_final_eval: False\n16:33:02 |     special_tok_lst: None\n16:33:02 |     split_lines: False\n16:33:02 |     starttime: Dec03_16-31\n16:33:02 |     task: fromfile:parlaiformat\n16:33:02 |     tensorboard_log: False\n16:33:02 |     tensorboard_logdir: None\n16:33:02 |     text_truncate: 360\n16:33:02 |     threshold: 0.5\n16:33:02 |     topk: 5\n16:33:02 |     train_predict: False\n16:33:02 |     truncate: 1024\n16:33:02 |     update_classifier_head_only: False\n16:33:02 |     update_freq: 1\n16:33:02 |     use_memories: False\n16:33:02 |     use_reply: none\n16:33:02 |     validation_cutoff: 1.0\n16:33:02 |     validation_every_n_epochs: -1\n16:33:02 |     validation_every_n_secs: 20.0\n16:33:02 |     validation_every_n_steps: -1\n16:33:02 |     validation_max_exs: -1\n16:33:02 |     validation_metric: accuracy\n16:33:02 |     validation_metric_mode: max\n16:33:02 |     validation_patience: 30\n16:33:02 |     validation_share_agent: False\n16:33:02 |     variant: xlm\n16:33:02 |     verbose: False\n16:33:02 |     wandb_entity: None\n16:33:02 |     wandb_log: False\n16:33:02 |     wandb_name: None\n16:33:02 |     wandb_project: None\n16:33:02 |     warmup_rate: 0.0001\n16:33:02 |     warmup_updates: 1000\n16:33:02 |     weight_decay: None\n16:33:02 |     world_logs: \n16:33:02 |     wrap_memory_encoder: False\n16:33:02 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:33:02 | creating task(s): fromfile:parlaiformat\n16:33:02 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:33:02 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-a.txt\n16:33:08 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8150 8.15e-10               .8063                 .8556   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7624            .8230              .7818   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8687  11.7 547.8 530.3       0          0 38.72  200 .8150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5403 5.404e-06 240.4 232.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 788.2 763.1        .8145\u001b[0m\n16:33:08 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8150 8.15e-10               .8063                 .8556   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7624            .8230              .7818   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8687  11.7 547.8 530.3       0          0 38.72  200 .8150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5403 5.404e-06 240.4 232.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 788.2 763.1        .8145\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-03T16:33:36.101285Z","iopub.execute_input":"2022-12-03T16:33:36.101700Z","iopub.status.idle":"2022-12-03T16:34:02.332524Z","shell.execute_reply.started":"2022-12-03T16:33:36.101659Z","shell.execute_reply":"2022-12-03T16:34:02.331371Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"16:33:43 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_valid.txt)\u001b[0m\n16:33:43 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:33:43 | Using CUDA\n16:33:43 | loading dictionary from /tmp/model2.dict\n16:33:43 | num words = 54944\n16:33:48 | Loading existing model parameters from /tmp/model2\n16:33:53 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:33:54 | Opt:\n16:33:54 |     activation: gelu\n16:33:54 |     adafactor_eps: '[1e-30, 0.001]'\n16:33:54 |     adam_eps: 1e-08\n16:33:54 |     add_p1_after_newln: False\n16:33:54 |     aggregate_micro: False\n16:33:54 |     allow_missing_init_opts: False\n16:33:54 |     area_under_curve_class: None\n16:33:54 |     area_under_curve_digits: -1\n16:33:54 |     attention_dropout: 0.1\n16:33:54 |     batchsize: 40\n16:33:54 |     betas: '[0.9, 0.999]'\n16:33:54 |     bpe_add_prefix_space: None\n16:33:54 |     bpe_debug: False\n16:33:54 |     bpe_dropout: None\n16:33:54 |     bpe_merge: None\n16:33:54 |     bpe_vocab: None\n16:33:54 |     candidates: inline\n16:33:54 |     cap_num_predictions: 100\n16:33:54 |     checkpoint_activations: False\n16:33:54 |     class_weights: None\n16:33:54 |     classes: \"['__notok__', '__ok__']\"\n16:33:54 |     classes_from_file: None\n16:33:54 |     data_parallel: True\n16:33:54 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:33:54 |     datatype: train\n16:33:54 |     delimiter: '\\n'\n16:33:54 |     dict_class: parlai.core.dict:DictionaryAgent\n16:33:54 |     dict_endtoken: __start__\n16:33:54 |     dict_file: /tmp/model2.dict\n16:33:54 |     dict_include_test: False\n16:33:54 |     dict_include_valid: False\n16:33:54 |     dict_initpath: None\n16:33:54 |     dict_language: english\n16:33:54 |     dict_loaded: True\n16:33:54 |     dict_lower: True\n16:33:54 |     dict_max_ngram_size: -1\n16:33:54 |     dict_maxexs: -1\n16:33:54 |     dict_maxtokens: -1\n16:33:54 |     dict_minfreq: 0\n16:33:54 |     dict_nulltoken: __null__\n16:33:54 |     dict_starttoken: __start__\n16:33:54 |     dict_textfields: text,labels\n16:33:54 |     dict_tokenizer: bpe\n16:33:54 |     dict_unktoken: __unk__\n16:33:54 |     display_examples: False\n16:33:54 |     download_path: None\n16:33:54 |     dropout: 0.1\n16:33:54 |     dynamic_batching: None\n16:33:54 |     embedding_projection: random\n16:33:54 |     embedding_size: 768\n16:33:54 |     embedding_type: random\n16:33:54 |     embeddings_scale: False\n16:33:54 |     encode_candidate_vecs: True\n16:33:54 |     encode_candidate_vecs_batchsize: 256\n16:33:54 |     eval_batchsize: None\n16:33:54 |     eval_candidates: inline\n16:33:54 |     eval_dynamic_batching: None\n16:33:54 |     evaltask: None\n16:33:54 |     ffn_size: 3072\n16:33:54 |     final_extra_opt: \n16:33:54 |     fixed_candidate_vecs: reuse\n16:33:54 |     fixed_candidates_path: None\n16:33:54 |     force_fp16_tokens: True\n16:33:54 |     fp16: True\n16:33:54 |     fp16_impl: safe\n16:33:54 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-b.txt\n16:33:54 |     fromfile_datatype_extension: False\n16:33:54 |     gpu: -1\n16:33:54 |     gradient_clip: 0.1\n16:33:54 |     hide_labels: False\n16:33:54 |     history_add_global_end_token: None\n16:33:54 |     history_reversed: False\n16:33:54 |     history_size: 20\n16:33:54 |     ignore_bad_candidates: False\n16:33:54 |     ignore_labels: None\n16:33:54 |     image_cropsize: 224\n16:33:54 |     image_mode: raw\n16:33:54 |     image_size: 256\n16:33:54 |     inference: max\n16:33:54 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:33:54 |     init_opt: None\n16:33:54 |     interactive_candidates: fixed\n16:33:54 |     interactive_mode: False\n16:33:54 |     invsqrt_lr_decay_gamma: -1\n16:33:54 |     is_debug: False\n16:33:54 |     label_truncate: 72\n16:33:54 |     learn_embeddings: True\n16:33:54 |     learn_positional_embeddings: True\n16:33:54 |     learningrate: 5e-05\n16:33:54 |     load_from_pretrained_ranker: True\n16:33:54 |     log_every_n_secs: 10.0\n16:33:54 |     log_every_n_steps: 50\n16:33:54 |     log_keep_fields: all\n16:33:54 |     loglevel: info\n16:33:54 |     lr_scheduler: reduceonplateau\n16:33:54 |     lr_scheduler_decay: 0.5\n16:33:54 |     lr_scheduler_patience: 3\n16:33:54 |     max_train_steps: -1\n16:33:54 |     max_train_time: 7200.0\n16:33:54 |     memory_attention: sqrt\n16:33:54 |     metrics: default\n16:33:54 |     model: transformer/classifier\n16:33:54 |     model_file: /tmp/model2\n16:33:54 |     model_parallel: False\n16:33:54 |     momentum: 0\n16:33:54 |     multitask_weights: [1]\n16:33:54 |     mutators: None\n16:33:54 |     n_decoder_layers: -1\n16:33:54 |     n_encoder_layers: -1\n16:33:54 |     n_heads: 12\n16:33:54 |     n_layers: 12\n16:33:54 |     n_positions: 1024\n16:33:54 |     n_segments: 2\n16:33:54 |     nesterov: True\n16:33:54 |     no_cuda: False\n16:33:54 |     normalize_sent_emb: False\n16:33:54 |     num_epochs: -1\n16:33:54 |     num_examples: -1\n16:33:54 |     num_workers: 0\n16:33:54 |     nus: [0.7]\n16:33:54 |     optimizer: adamax\n16:33:54 |     output_scaling: 0.06\n16:33:54 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n16:33:54 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:33:54 |     person_tokens: False\n16:33:54 |     print_scores: False\n16:33:54 |     rank_candidates: False\n16:33:54 |     rank_top_k: -1\n16:33:54 |     reduction_type: mean\n16:33:54 |     ref_class: None\n16:33:54 |     relu_dropout: 0.0\n16:33:54 |     repeat_blocking_heuristic: True\n16:33:54 |     report_filename: \n16:33:54 |     return_cand_scores: False\n16:33:54 |     save_after_valid: True\n16:33:54 |     save_every_n_secs: -1\n16:33:54 |     save_format: conversations\n16:33:54 |     share_encoders: False\n16:33:54 |     share_word_embeddings: False\n16:33:54 |     short_final_eval: False\n16:33:54 |     special_tok_lst: None\n16:33:54 |     split_lines: False\n16:33:54 |     starttime: Dec03_16-31\n16:33:54 |     task: fromfile:parlaiformat\n16:33:54 |     tensorboard_log: False\n16:33:54 |     tensorboard_logdir: None\n16:33:54 |     text_truncate: 360\n16:33:54 |     threshold: 0.5\n16:33:54 |     topk: 5\n16:33:54 |     train_predict: False\n16:33:54 |     truncate: 1024\n16:33:54 |     update_classifier_head_only: False\n16:33:54 |     update_freq: 1\n16:33:54 |     use_memories: False\n16:33:54 |     use_reply: none\n16:33:54 |     validation_cutoff: 1.0\n16:33:54 |     validation_every_n_epochs: -1\n16:33:54 |     validation_every_n_secs: 20.0\n16:33:54 |     validation_every_n_steps: -1\n16:33:54 |     validation_max_exs: -1\n16:33:54 |     validation_metric: accuracy\n16:33:54 |     validation_metric_mode: max\n16:33:54 |     validation_patience: 30\n16:33:54 |     validation_share_agent: False\n16:33:54 |     variant: xlm\n16:33:54 |     verbose: False\n16:33:54 |     wandb_entity: None\n16:33:54 |     wandb_log: False\n16:33:54 |     wandb_name: None\n16:33:54 |     wandb_project: None\n16:33:54 |     warmup_rate: 0.0001\n16:33:54 |     warmup_updates: 1000\n16:33:54 |     weight_decay: None\n16:33:54 |     world_logs: \n16:33:54 |     wrap_memory_encoder: False\n16:33:55 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:33:55 | creating task(s): fromfile:parlaiformat\n16:33:55 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:33:55 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run2/data_train-b.txt\n16:34:00 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1850 1.85e-10               .1376                 .1444   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1313            .2275              .2182   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2376  11.7 547.8 524.7       0          0 38.31  200 .1850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9200 5.404e-06 239.6 229.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 787.4 754.2        .1830\u001b[0m\n16:34:00 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1850 1.85e-10               .1376                 .1444   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1313            .2275              .2182   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2376  11.7 547.8 524.7       0          0 38.31  200 .1850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9200 5.404e-06 239.6 229.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 787.4 754.2        .1830\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:34:18.514068Z","iopub.execute_input":"2022-12-03T16:34:18.514457Z","iopub.status.idle":"2022-12-03T16:34:19.724621Z","shell.execute_reply.started":"2022-12-03T16:34:18.514415Z","shell.execute_reply":"2022-12-03T16:34:19.723290Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:34:28.014195Z","iopub.execute_input":"2022-12-03T16:34:28.014650Z","iopub.status.idle":"2022-12-03T16:35:24.518500Z","shell.execute_reply.started":"2022-12-03T16:34:28.014616Z","shell.execute_reply":"2022-12-03T16:35:24.517309Z"},"scrolled":true,"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"16:34:35 | building dictionary first...\n16:34:35 | No model with opt yet at: /tmp/model3(.opt)\n16:34:35 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:34:35 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:34:35 | Using CUDA\n16:34:35 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:34:35 | num words = 54944\n16:34:39 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:34:49 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:34:49 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:34:49 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:34:49 | Opt:\n16:34:49 |     activation: gelu\n16:34:49 |     adafactor_eps: '(1e-30, 0.001)'\n16:34:49 |     adam_eps: 1e-08\n16:34:49 |     add_p1_after_newln: False\n16:34:49 |     aggregate_micro: False\n16:34:49 |     allow_missing_init_opts: False\n16:34:49 |     attention_dropout: 0.1\n16:34:49 |     batchsize: 20\n16:34:49 |     betas: '(0.9, 0.999)'\n16:34:49 |     bpe_add_prefix_space: None\n16:34:49 |     bpe_debug: False\n16:34:49 |     bpe_dropout: None\n16:34:49 |     bpe_merge: None\n16:34:49 |     bpe_vocab: None\n16:34:49 |     candidates: inline\n16:34:49 |     cap_num_predictions: 100\n16:34:49 |     checkpoint_activations: False\n16:34:49 |     class_weights: None\n16:34:49 |     classes: \"['__notok__', '__ok__']\"\n16:34:49 |     classes_from_file: None\n16:34:49 |     data_parallel: True\n16:34:49 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:34:49 |     datatype: train\n16:34:49 |     delimiter: '\\n'\n16:34:49 |     dict_class: parlai.core.dict:DictionaryAgent\n16:34:49 |     dict_endtoken: __start__\n16:34:49 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:34:49 |     dict_include_test: False\n16:34:49 |     dict_include_valid: False\n16:34:49 |     dict_initpath: None\n16:34:49 |     dict_language: english\n16:34:49 |     dict_loaded: True\n16:34:49 |     dict_lower: True\n16:34:49 |     dict_max_ngram_size: -1\n16:34:49 |     dict_maxexs: -1\n16:34:49 |     dict_maxtokens: -1\n16:34:49 |     dict_minfreq: 0\n16:34:49 |     dict_nulltoken: __null__\n16:34:49 |     dict_starttoken: __start__\n16:34:49 |     dict_textfields: text,labels\n16:34:49 |     dict_tokenizer: bpe\n16:34:49 |     dict_unktoken: __unk__\n16:34:49 |     display_examples: False\n16:34:49 |     download_path: None\n16:34:49 |     dropout: 0.1\n16:34:49 |     dynamic_batching: None\n16:34:49 |     embedding_projection: random\n16:34:49 |     embedding_size: 768\n16:34:49 |     embedding_type: random\n16:34:49 |     embeddings_scale: False\n16:34:49 |     encode_candidate_vecs: True\n16:34:49 |     encode_candidate_vecs_batchsize: 256\n16:34:49 |     eval_batchsize: None\n16:34:49 |     eval_candidates: inline\n16:34:49 |     eval_dynamic_batching: None\n16:34:49 |     evaltask: None\n16:34:49 |     ffn_size: 3072\n16:34:49 |     final_extra_opt: \n16:34:49 |     fixed_candidate_vecs: reuse\n16:34:49 |     fixed_candidates_path: None\n16:34:49 |     force_fp16_tokens: False\n16:34:49 |     fp16: True\n16:34:49 |     fp16_impl: safe\n16:34:49 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt\n16:34:49 |     fromfile_datatype_extension: False\n16:34:49 |     gpu: -1\n16:34:49 |     gradient_clip: 0.1\n16:34:49 |     hide_labels: False\n16:34:49 |     history_add_global_end_token: None\n16:34:49 |     history_reversed: False\n16:34:49 |     history_size: 20\n16:34:49 |     ignore_bad_candidates: False\n16:34:49 |     ignore_labels: None\n16:34:49 |     image_cropsize: 224\n16:34:49 |     image_mode: raw\n16:34:49 |     image_size: 256\n16:34:49 |     inference: max\n16:34:49 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:34:49 |     init_opt: None\n16:34:49 |     interactive_candidates: fixed\n16:34:49 |     interactive_mode: False\n16:34:49 |     invsqrt_lr_decay_gamma: -1\n16:34:49 |     is_debug: False\n16:34:49 |     label_truncate: 72\n16:34:49 |     learn_embeddings: True\n16:34:49 |     learn_positional_embeddings: True\n16:34:49 |     learningrate: 5e-05\n16:34:49 |     load_from_checkpoint: False\n16:34:49 |     load_from_pretrained_ranker: True\n16:34:49 |     log_every_n_secs: 10.0\n16:34:49 |     log_every_n_steps: 50\n16:34:49 |     log_keep_fields: all\n16:34:49 |     loglevel: info\n16:34:49 |     lr_scheduler: reduceonplateau\n16:34:49 |     lr_scheduler_decay: 0.5\n16:34:49 |     lr_scheduler_patience: 3\n16:34:49 |     max_train_steps: -1\n16:34:49 |     max_train_time: 7200.0\n16:34:49 |     memory_attention: sqrt\n16:34:49 |     metrics: default\n16:34:49 |     model: transformer/classifier\n16:34:49 |     model_file: /tmp/model3\n16:34:49 |     model_parallel: False\n16:34:49 |     momentum: 0\n16:34:49 |     multitask_weights: [1]\n16:34:49 |     mutators: None\n16:34:49 |     n_decoder_layers: -1\n16:34:49 |     n_encoder_layers: -1\n16:34:49 |     n_heads: 12\n16:34:49 |     n_layers: 12\n16:34:49 |     n_positions: 1024\n16:34:49 |     n_segments: 2\n16:34:49 |     nesterov: True\n16:34:49 |     no_cuda: False\n16:34:49 |     normalize_sent_emb: False\n16:34:49 |     num_epochs: -1\n16:34:49 |     num_workers: 0\n16:34:49 |     nus: (0.7,)\n16:34:49 |     optimizer: adamax\n16:34:49 |     output_scaling: 0.06\n16:34:49 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n16:34:49 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:34:49 |     person_tokens: False\n16:34:49 |     print_scores: False\n16:34:49 |     rank_candidates: False\n16:34:49 |     rank_top_k: -1\n16:34:49 |     reduction_type: mean\n16:34:49 |     ref_class: None\n16:34:49 |     relu_dropout: 0.0\n16:34:49 |     repeat_blocking_heuristic: True\n16:34:49 |     return_cand_scores: False\n16:34:49 |     save_after_valid: True\n16:34:49 |     save_every_n_secs: -1\n16:34:49 |     save_format: conversations\n16:34:49 |     share_encoders: False\n16:34:49 |     share_word_embeddings: False\n16:34:49 |     short_final_eval: False\n16:34:49 |     special_tok_lst: None\n16:34:49 |     split_lines: False\n16:34:49 |     starttime: Dec03_16-34\n16:34:49 |     task: fromfile:parlaiformat\n16:34:49 |     tensorboard_log: False\n16:34:49 |     tensorboard_logdir: None\n16:34:49 |     text_truncate: 360\n16:34:49 |     threshold: 0.5\n16:34:49 |     topk: 5\n16:34:49 |     train_predict: False\n16:34:49 |     truncate: 1024\n16:34:49 |     update_classifier_head_only: False\n16:34:49 |     update_freq: 1\n16:34:49 |     use_memories: False\n16:34:49 |     use_reply: none\n16:34:49 |     validation_cutoff: 1.0\n16:34:49 |     validation_every_n_epochs: -1\n16:34:49 |     validation_every_n_secs: 20.0\n16:34:49 |     validation_every_n_steps: -1\n16:34:49 |     validation_max_exs: -1\n16:34:49 |     validation_metric: accuracy\n16:34:49 |     validation_metric_mode: max\n16:34:49 |     validation_patience: 30\n16:34:49 |     validation_share_agent: False\n16:34:49 |     variant: xlm\n16:34:49 |     verbose: False\n16:34:49 |     wandb_entity: None\n16:34:49 |     wandb_log: False\n16:34:49 |     wandb_name: None\n16:34:49 |     wandb_project: None\n16:34:49 |     warmup_rate: 0.0001\n16:34:49 |     warmup_updates: 1000\n16:34:49 |     weight_decay: None\n16:34:49 |     world_logs: \n16:34:49 |     wrap_memory_encoder: False\n16:34:49 | creating task(s): fromfile:parlaiformat\n16:34:49 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt\n16:34:49 | training...\n16:35:00 | time:10s total_exs:440 total_steps:22 epochs:18.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6841 6.841e-10               .4462                 .9825   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2887            .7790              .6397   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9959 11.61     1 272.3 586.7       0          0  43.1  440   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6841             32768  2.519    .1206 5.882 .6417 1.105e-06 117.6 253.5   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps  ups  weighted_f1  \n         0          0                   22 389.9 840.2 2.16        .6323\n\n16:35:09 | time:20s total_exs:1200 total_steps:60 epochs:50.00\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8408 8.408e-10               .8153                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6881            .8601              .7546   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.76     1 275.1  1091       0          0 79.29  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8408             32768  3.034    .1207 6.021 .5976 3.005e-06 120.4 477.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   60 395.6 1568 3.974        .8372\n\n16:35:09 | creating task(s): fromfile:parlaiformat\n16:35:09 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:35:09 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt\n16:35:09 | running eval: valid\n16:35:10 | eval completed in 0.19s\n16:35:10 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1910       0          0 139.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5195 3.005e-06    72 835.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     60 236.5 2746            1\n\u001b[0m\n16:35:10 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:35:10 | saving best valid model: /tmp/model3\n16:35:10 | Saving dictionary to /tmp/model3.dict\n16:35:13 | task solved! stopping.\n16:35:13 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:35:13 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:35:13 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:35:13 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:35:13 | Using CUDA\n16:35:13 | loading dictionary from /tmp/model3.dict\n16:35:14 | num words = 54944\n16:35:19 | Loading existing model parameters from /tmp/model3\n16:35:21 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:35:22 | creating task(s): fromfile:parlaiformat\n16:35:22 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:35:22 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt\n16:35:22 | running eval: valid\n16:35:22 | eval completed in 0.20s\n16:35:22 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1866       0          0 136.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5195 3.005e-06    72 816.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     60 236.5 2683            1\n\u001b[0m\n16:35:22 | creating task(s): fromfile:parlaiformat\n16:35:22 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:35:22 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt\n16:35:22 | running eval: test\n16:35:22 | eval completed in 0.20s\n16:35:22 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1857       0          0 135.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5195 3.005e-06    72 812.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     60 236.5 2670            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:35:24.521242Z","iopub.execute_input":"2022-12-03T16:35:24.521687Z","iopub.status.idle":"2022-12-03T16:35:51.699633Z","shell.execute_reply.started":"2022-12-03T16:35:24.521626Z","shell.execute_reply":"2022-12-03T16:35:51.698401Z"},"scrolled":true,"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"16:35:32 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt)\u001b[0m\n16:35:32 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:35:32 | Using CUDA\n16:35:32 | loading dictionary from /tmp/model3.dict\n16:35:32 | num words = 54944\n16:35:36 | Loading existing model parameters from /tmp/model3\n16:35:42 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:35:43 | Opt:\n16:35:43 |     activation: gelu\n16:35:43 |     adafactor_eps: '[1e-30, 0.001]'\n16:35:43 |     adam_eps: 1e-08\n16:35:43 |     add_p1_after_newln: False\n16:35:43 |     aggregate_micro: False\n16:35:43 |     allow_missing_init_opts: False\n16:35:43 |     area_under_curve_class: None\n16:35:43 |     area_under_curve_digits: -1\n16:35:43 |     attention_dropout: 0.1\n16:35:43 |     batchsize: 40\n16:35:43 |     betas: '[0.9, 0.999]'\n16:35:43 |     bpe_add_prefix_space: None\n16:35:43 |     bpe_debug: False\n16:35:43 |     bpe_dropout: None\n16:35:43 |     bpe_merge: None\n16:35:43 |     bpe_vocab: None\n16:35:43 |     candidates: inline\n16:35:43 |     cap_num_predictions: 100\n16:35:43 |     checkpoint_activations: False\n16:35:43 |     class_weights: None\n16:35:43 |     classes: \"['__notok__', '__ok__']\"\n16:35:43 |     classes_from_file: None\n16:35:43 |     data_parallel: True\n16:35:43 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:35:43 |     datatype: train\n16:35:43 |     delimiter: '\\n'\n16:35:43 |     dict_class: parlai.core.dict:DictionaryAgent\n16:35:43 |     dict_endtoken: __start__\n16:35:43 |     dict_file: /tmp/model3.dict\n16:35:43 |     dict_include_test: False\n16:35:43 |     dict_include_valid: False\n16:35:43 |     dict_initpath: None\n16:35:43 |     dict_language: english\n16:35:43 |     dict_loaded: True\n16:35:43 |     dict_lower: True\n16:35:43 |     dict_max_ngram_size: -1\n16:35:43 |     dict_maxexs: -1\n16:35:43 |     dict_maxtokens: -1\n16:35:43 |     dict_minfreq: 0\n16:35:43 |     dict_nulltoken: __null__\n16:35:43 |     dict_starttoken: __start__\n16:35:43 |     dict_textfields: text,labels\n16:35:43 |     dict_tokenizer: bpe\n16:35:43 |     dict_unktoken: __unk__\n16:35:43 |     display_examples: False\n16:35:43 |     download_path: None\n16:35:43 |     dropout: 0.1\n16:35:43 |     dynamic_batching: None\n16:35:43 |     embedding_projection: random\n16:35:43 |     embedding_size: 768\n16:35:43 |     embedding_type: random\n16:35:43 |     embeddings_scale: False\n16:35:43 |     encode_candidate_vecs: True\n16:35:43 |     encode_candidate_vecs_batchsize: 256\n16:35:43 |     eval_batchsize: None\n16:35:43 |     eval_candidates: inline\n16:35:43 |     eval_dynamic_batching: None\n16:35:43 |     evaltask: None\n16:35:43 |     ffn_size: 3072\n16:35:43 |     final_extra_opt: \n16:35:43 |     fixed_candidate_vecs: reuse\n16:35:43 |     fixed_candidates_path: None\n16:35:43 |     force_fp16_tokens: True\n16:35:43 |     fp16: True\n16:35:43 |     fp16_impl: safe\n16:35:43 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-a.txt\n16:35:43 |     fromfile_datatype_extension: False\n16:35:43 |     gpu: -1\n16:35:43 |     gradient_clip: 0.1\n16:35:43 |     hide_labels: False\n16:35:43 |     history_add_global_end_token: None\n16:35:43 |     history_reversed: False\n16:35:43 |     history_size: 20\n16:35:43 |     ignore_bad_candidates: False\n16:35:43 |     ignore_labels: None\n16:35:43 |     image_cropsize: 224\n16:35:43 |     image_mode: raw\n16:35:43 |     image_size: 256\n16:35:43 |     inference: max\n16:35:43 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:35:43 |     init_opt: None\n16:35:43 |     interactive_candidates: fixed\n16:35:43 |     interactive_mode: False\n16:35:43 |     invsqrt_lr_decay_gamma: -1\n16:35:43 |     is_debug: False\n16:35:43 |     label_truncate: 72\n16:35:43 |     learn_embeddings: True\n16:35:43 |     learn_positional_embeddings: True\n16:35:43 |     learningrate: 5e-05\n16:35:43 |     load_from_pretrained_ranker: True\n16:35:43 |     log_every_n_secs: 10.0\n16:35:43 |     log_every_n_steps: 50\n16:35:43 |     log_keep_fields: all\n16:35:43 |     loglevel: info\n16:35:43 |     lr_scheduler: reduceonplateau\n16:35:43 |     lr_scheduler_decay: 0.5\n16:35:43 |     lr_scheduler_patience: 3\n16:35:43 |     max_train_steps: -1\n16:35:43 |     max_train_time: 7200.0\n16:35:43 |     memory_attention: sqrt\n16:35:43 |     metrics: default\n16:35:43 |     model: transformer/classifier\n16:35:43 |     model_file: /tmp/model3\n16:35:43 |     model_parallel: False\n16:35:43 |     momentum: 0\n16:35:43 |     multitask_weights: [1]\n16:35:43 |     mutators: None\n16:35:43 |     n_decoder_layers: -1\n16:35:43 |     n_encoder_layers: -1\n16:35:43 |     n_heads: 12\n16:35:43 |     n_layers: 12\n16:35:43 |     n_positions: 1024\n16:35:43 |     n_segments: 2\n16:35:43 |     nesterov: True\n16:35:43 |     no_cuda: False\n16:35:43 |     normalize_sent_emb: False\n16:35:43 |     num_epochs: -1\n16:35:43 |     num_examples: -1\n16:35:43 |     num_workers: 0\n16:35:43 |     nus: [0.7]\n16:35:43 |     optimizer: adamax\n16:35:43 |     output_scaling: 0.06\n16:35:43 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n16:35:43 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:35:43 |     person_tokens: False\n16:35:43 |     print_scores: False\n16:35:43 |     rank_candidates: False\n16:35:43 |     rank_top_k: -1\n16:35:43 |     reduction_type: mean\n16:35:43 |     ref_class: None\n16:35:43 |     relu_dropout: 0.0\n16:35:43 |     repeat_blocking_heuristic: True\n16:35:43 |     report_filename: \n16:35:43 |     return_cand_scores: False\n16:35:43 |     save_after_valid: True\n16:35:43 |     save_every_n_secs: -1\n16:35:43 |     save_format: conversations\n16:35:43 |     share_encoders: False\n16:35:43 |     share_word_embeddings: False\n16:35:43 |     short_final_eval: False\n16:35:43 |     special_tok_lst: None\n16:35:43 |     split_lines: False\n16:35:43 |     starttime: Dec03_16-34\n16:35:43 |     task: fromfile:parlaiformat\n16:35:43 |     tensorboard_log: False\n16:35:43 |     tensorboard_logdir: None\n16:35:43 |     text_truncate: 360\n16:35:43 |     threshold: 0.5\n16:35:43 |     topk: 5\n16:35:43 |     train_predict: False\n16:35:43 |     truncate: 1024\n16:35:43 |     update_classifier_head_only: False\n16:35:43 |     update_freq: 1\n16:35:43 |     use_memories: False\n16:35:43 |     use_reply: none\n16:35:43 |     validation_cutoff: 1.0\n16:35:43 |     validation_every_n_epochs: -1\n16:35:43 |     validation_every_n_secs: 20.0\n16:35:43 |     validation_every_n_steps: -1\n16:35:43 |     validation_max_exs: -1\n16:35:43 |     validation_metric: accuracy\n16:35:43 |     validation_metric_mode: max\n16:35:43 |     validation_patience: 30\n16:35:43 |     validation_share_agent: False\n16:35:43 |     variant: xlm\n16:35:43 |     verbose: False\n16:35:43 |     wandb_entity: None\n16:35:43 |     wandb_log: False\n16:35:43 |     wandb_name: None\n16:35:43 |     wandb_project: None\n16:35:43 |     warmup_rate: 0.0001\n16:35:43 |     warmup_updates: 1000\n16:35:43 |     weight_decay: None\n16:35:43 |     world_logs: \n16:35:43 |     wrap_memory_encoder: False\n16:35:44 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:35:44 | creating task(s): fromfile:parlaiformat\n16:35:44 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:35:44 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-a.txt\n16:35:49 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7300 7.3e-10               .6707                 .8209   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5670            .7712              .6842   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8835 11.47   539 528.9       0          0 39.25  200 .7300   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.97 .6402 3.005e-06 238.8 234.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     60 777.8 763.2        .7225\u001b[0m\n16:35:49 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7300 7.3e-10               .6707                 .8209   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5670            .7712              .6842   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8835 11.47   539 528.9       0          0 39.25  200 .7300   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.97 .6402 3.005e-06 238.8 234.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     60 777.8 763.2        .7225\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-03T16:36:17.725527Z","iopub.execute_input":"2022-12-03T16:36:17.726466Z","iopub.status.idle":"2022-12-03T16:36:43.913739Z","shell.execute_reply.started":"2022-12-03T16:36:17.726426Z","shell.execute_reply":"2022-12-03T16:36:43.912542Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"16:36:25 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_valid.txt)\u001b[0m\n16:36:25 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:36:25 | Using CUDA\n16:36:25 | loading dictionary from /tmp/model3.dict\n16:36:25 | num words = 54944\n16:36:29 | Loading existing model parameters from /tmp/model3\n16:36:35 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:36:36 | Opt:\n16:36:36 |     activation: gelu\n16:36:36 |     adafactor_eps: '[1e-30, 0.001]'\n16:36:36 |     adam_eps: 1e-08\n16:36:36 |     add_p1_after_newln: False\n16:36:36 |     aggregate_micro: False\n16:36:36 |     allow_missing_init_opts: False\n16:36:36 |     area_under_curve_class: None\n16:36:36 |     area_under_curve_digits: -1\n16:36:36 |     attention_dropout: 0.1\n16:36:36 |     batchsize: 40\n16:36:36 |     betas: '[0.9, 0.999]'\n16:36:36 |     bpe_add_prefix_space: None\n16:36:36 |     bpe_debug: False\n16:36:36 |     bpe_dropout: None\n16:36:36 |     bpe_merge: None\n16:36:36 |     bpe_vocab: None\n16:36:36 |     candidates: inline\n16:36:36 |     cap_num_predictions: 100\n16:36:36 |     checkpoint_activations: False\n16:36:36 |     class_weights: None\n16:36:36 |     classes: \"['__notok__', '__ok__']\"\n16:36:36 |     classes_from_file: None\n16:36:36 |     data_parallel: True\n16:36:36 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:36:36 |     datatype: train\n16:36:36 |     delimiter: '\\n'\n16:36:36 |     dict_class: parlai.core.dict:DictionaryAgent\n16:36:36 |     dict_endtoken: __start__\n16:36:36 |     dict_file: /tmp/model3.dict\n16:36:36 |     dict_include_test: False\n16:36:36 |     dict_include_valid: False\n16:36:36 |     dict_initpath: None\n16:36:36 |     dict_language: english\n16:36:36 |     dict_loaded: True\n16:36:36 |     dict_lower: True\n16:36:36 |     dict_max_ngram_size: -1\n16:36:36 |     dict_maxexs: -1\n16:36:36 |     dict_maxtokens: -1\n16:36:36 |     dict_minfreq: 0\n16:36:36 |     dict_nulltoken: __null__\n16:36:36 |     dict_starttoken: __start__\n16:36:36 |     dict_textfields: text,labels\n16:36:36 |     dict_tokenizer: bpe\n16:36:36 |     dict_unktoken: __unk__\n16:36:36 |     display_examples: False\n16:36:36 |     download_path: None\n16:36:36 |     dropout: 0.1\n16:36:36 |     dynamic_batching: None\n16:36:36 |     embedding_projection: random\n16:36:36 |     embedding_size: 768\n16:36:36 |     embedding_type: random\n16:36:36 |     embeddings_scale: False\n16:36:36 |     encode_candidate_vecs: True\n16:36:36 |     encode_candidate_vecs_batchsize: 256\n16:36:36 |     eval_batchsize: None\n16:36:36 |     eval_candidates: inline\n16:36:36 |     eval_dynamic_batching: None\n16:36:36 |     evaltask: None\n16:36:36 |     ffn_size: 3072\n16:36:36 |     final_extra_opt: \n16:36:36 |     fixed_candidate_vecs: reuse\n16:36:36 |     fixed_candidates_path: None\n16:36:36 |     force_fp16_tokens: True\n16:36:36 |     fp16: True\n16:36:36 |     fp16_impl: safe\n16:36:36 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-b.txt\n16:36:36 |     fromfile_datatype_extension: False\n16:36:36 |     gpu: -1\n16:36:36 |     gradient_clip: 0.1\n16:36:36 |     hide_labels: False\n16:36:36 |     history_add_global_end_token: None\n16:36:36 |     history_reversed: False\n16:36:36 |     history_size: 20\n16:36:36 |     ignore_bad_candidates: False\n16:36:36 |     ignore_labels: None\n16:36:36 |     image_cropsize: 224\n16:36:36 |     image_mode: raw\n16:36:36 |     image_size: 256\n16:36:36 |     inference: max\n16:36:36 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:36:36 |     init_opt: None\n16:36:36 |     interactive_candidates: fixed\n16:36:36 |     interactive_mode: False\n16:36:36 |     invsqrt_lr_decay_gamma: -1\n16:36:36 |     is_debug: False\n16:36:36 |     label_truncate: 72\n16:36:36 |     learn_embeddings: True\n16:36:36 |     learn_positional_embeddings: True\n16:36:36 |     learningrate: 5e-05\n16:36:36 |     load_from_pretrained_ranker: True\n16:36:36 |     log_every_n_secs: 10.0\n16:36:36 |     log_every_n_steps: 50\n16:36:36 |     log_keep_fields: all\n16:36:36 |     loglevel: info\n16:36:36 |     lr_scheduler: reduceonplateau\n16:36:36 |     lr_scheduler_decay: 0.5\n16:36:36 |     lr_scheduler_patience: 3\n16:36:36 |     max_train_steps: -1\n16:36:36 |     max_train_time: 7200.0\n16:36:36 |     memory_attention: sqrt\n16:36:36 |     metrics: default\n16:36:36 |     model: transformer/classifier\n16:36:36 |     model_file: /tmp/model3\n16:36:36 |     model_parallel: False\n16:36:36 |     momentum: 0\n16:36:36 |     multitask_weights: [1]\n16:36:36 |     mutators: None\n16:36:36 |     n_decoder_layers: -1\n16:36:36 |     n_encoder_layers: -1\n16:36:36 |     n_heads: 12\n16:36:36 |     n_layers: 12\n16:36:36 |     n_positions: 1024\n16:36:36 |     n_segments: 2\n16:36:36 |     nesterov: True\n16:36:36 |     no_cuda: False\n16:36:36 |     normalize_sent_emb: False\n16:36:36 |     num_epochs: -1\n16:36:36 |     num_examples: -1\n16:36:36 |     num_workers: 0\n16:36:36 |     nus: [0.7]\n16:36:36 |     optimizer: adamax\n16:36:36 |     output_scaling: 0.06\n16:36:36 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n16:36:36 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:36:36 |     person_tokens: False\n16:36:36 |     print_scores: False\n16:36:36 |     rank_candidates: False\n16:36:36 |     rank_top_k: -1\n16:36:36 |     reduction_type: mean\n16:36:36 |     ref_class: None\n16:36:36 |     relu_dropout: 0.0\n16:36:36 |     repeat_blocking_heuristic: True\n16:36:36 |     report_filename: \n16:36:36 |     return_cand_scores: False\n16:36:36 |     save_after_valid: True\n16:36:36 |     save_every_n_secs: -1\n16:36:36 |     save_format: conversations\n16:36:36 |     share_encoders: False\n16:36:36 |     share_word_embeddings: False\n16:36:36 |     short_final_eval: False\n16:36:36 |     special_tok_lst: None\n16:36:36 |     split_lines: False\n16:36:36 |     starttime: Dec03_16-34\n16:36:36 |     task: fromfile:parlaiformat\n16:36:36 |     tensorboard_log: False\n16:36:36 |     tensorboard_logdir: None\n16:36:36 |     text_truncate: 360\n16:36:36 |     threshold: 0.5\n16:36:36 |     topk: 5\n16:36:36 |     train_predict: False\n16:36:36 |     truncate: 1024\n16:36:36 |     update_classifier_head_only: False\n16:36:36 |     update_freq: 1\n16:36:36 |     use_memories: False\n16:36:36 |     use_reply: none\n16:36:36 |     validation_cutoff: 1.0\n16:36:36 |     validation_every_n_epochs: -1\n16:36:36 |     validation_every_n_secs: 20.0\n16:36:36 |     validation_every_n_steps: -1\n16:36:36 |     validation_max_exs: -1\n16:36:36 |     validation_metric: accuracy\n16:36:36 |     validation_metric_mode: max\n16:36:36 |     validation_patience: 30\n16:36:36 |     validation_share_agent: False\n16:36:36 |     variant: xlm\n16:36:36 |     verbose: False\n16:36:36 |     wandb_entity: None\n16:36:36 |     wandb_log: False\n16:36:36 |     wandb_name: None\n16:36:36 |     wandb_project: None\n16:36:36 |     warmup_rate: 0.0001\n16:36:36 |     warmup_updates: 1000\n16:36:36 |     weight_decay: None\n16:36:36 |     world_logs: \n16:36:36 |     wrap_memory_encoder: False\n16:36:36 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:36:36 | creating task(s): fromfile:parlaiformat\n16:36:36 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:36:36 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run3/data_train-b.txt\n16:36:42 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2700 2.7e-10               .1412                 .1791   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1165            .3652              .3158   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4330 11.47   539 497.9       0          0 36.95  200 .2700   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.03 .7611 3.005e-06 241.2 222.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     60 780.2 720.7        .2498\u001b[0m\n16:36:42 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2700 2.7e-10               .1412                 .1791   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1165            .3652              .3158   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4330 11.47   539 497.9       0          0 36.95  200 .2700   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.03 .7611 3.005e-06 241.2 222.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     60 780.2 720.7        .2498\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:36:45.099700Z","iopub.execute_input":"2022-12-03T16:36:45.100002Z","iopub.status.idle":"2022-12-03T16:36:46.057382Z","shell.execute_reply.started":"2022-12-03T16:36:45.099974Z","shell.execute_reply":"2022-12-03T16:36:46.055969Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"rm: cannot remove '/tmp/model3*': No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:37:11.993104Z","iopub.execute_input":"2022-12-03T16:37:11.993531Z","iopub.status.idle":"2022-12-03T16:38:06.440066Z","shell.execute_reply.started":"2022-12-03T16:37:11.993473Z","shell.execute_reply":"2022-12-03T16:38:06.438772Z"},"scrolled":true,"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"16:37:18 | building dictionary first...\n16:37:18 | No model with opt yet at: /tmp/model4(.opt)\n16:37:18 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:37:18 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:37:18 | Using CUDA\n16:37:18 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:37:19 | num words = 54944\n16:37:23 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:37:31 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:37:31 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:37:31 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:37:31 | Opt:\n16:37:31 |     activation: gelu\n16:37:31 |     adafactor_eps: '(1e-30, 0.001)'\n16:37:31 |     adam_eps: 1e-08\n16:37:31 |     add_p1_after_newln: False\n16:37:31 |     aggregate_micro: False\n16:37:31 |     allow_missing_init_opts: False\n16:37:31 |     attention_dropout: 0.1\n16:37:31 |     batchsize: 20\n16:37:31 |     betas: '(0.9, 0.999)'\n16:37:31 |     bpe_add_prefix_space: None\n16:37:31 |     bpe_debug: False\n16:37:31 |     bpe_dropout: None\n16:37:31 |     bpe_merge: None\n16:37:31 |     bpe_vocab: None\n16:37:31 |     candidates: inline\n16:37:31 |     cap_num_predictions: 100\n16:37:31 |     checkpoint_activations: False\n16:37:31 |     class_weights: None\n16:37:31 |     classes: \"['__notok__', '__ok__']\"\n16:37:31 |     classes_from_file: None\n16:37:31 |     data_parallel: True\n16:37:31 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:37:31 |     datatype: train\n16:37:31 |     delimiter: '\\n'\n16:37:31 |     dict_class: parlai.core.dict:DictionaryAgent\n16:37:31 |     dict_endtoken: __start__\n16:37:31 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:37:31 |     dict_include_test: False\n16:37:31 |     dict_include_valid: False\n16:37:31 |     dict_initpath: None\n16:37:31 |     dict_language: english\n16:37:31 |     dict_loaded: True\n16:37:31 |     dict_lower: True\n16:37:31 |     dict_max_ngram_size: -1\n16:37:31 |     dict_maxexs: -1\n16:37:31 |     dict_maxtokens: -1\n16:37:31 |     dict_minfreq: 0\n16:37:31 |     dict_nulltoken: __null__\n16:37:31 |     dict_starttoken: __start__\n16:37:31 |     dict_textfields: text,labels\n16:37:31 |     dict_tokenizer: bpe\n16:37:31 |     dict_unktoken: __unk__\n16:37:31 |     display_examples: False\n16:37:31 |     download_path: None\n16:37:31 |     dropout: 0.1\n16:37:31 |     dynamic_batching: None\n16:37:31 |     embedding_projection: random\n16:37:31 |     embedding_size: 768\n16:37:31 |     embedding_type: random\n16:37:31 |     embeddings_scale: False\n16:37:31 |     encode_candidate_vecs: True\n16:37:31 |     encode_candidate_vecs_batchsize: 256\n16:37:31 |     eval_batchsize: None\n16:37:31 |     eval_candidates: inline\n16:37:31 |     eval_dynamic_batching: None\n16:37:31 |     evaltask: None\n16:37:31 |     ffn_size: 3072\n16:37:31 |     final_extra_opt: \n16:37:31 |     fixed_candidate_vecs: reuse\n16:37:31 |     fixed_candidates_path: None\n16:37:31 |     force_fp16_tokens: False\n16:37:31 |     fp16: True\n16:37:31 |     fp16_impl: safe\n16:37:31 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt\n16:37:31 |     fromfile_datatype_extension: False\n16:37:31 |     gpu: -1\n16:37:31 |     gradient_clip: 0.1\n16:37:31 |     hide_labels: False\n16:37:31 |     history_add_global_end_token: None\n16:37:31 |     history_reversed: False\n16:37:31 |     history_size: 20\n16:37:31 |     ignore_bad_candidates: False\n16:37:31 |     ignore_labels: None\n16:37:31 |     image_cropsize: 224\n16:37:31 |     image_mode: raw\n16:37:31 |     image_size: 256\n16:37:31 |     inference: max\n16:37:31 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:37:31 |     init_opt: None\n16:37:31 |     interactive_candidates: fixed\n16:37:31 |     interactive_mode: False\n16:37:31 |     invsqrt_lr_decay_gamma: -1\n16:37:31 |     is_debug: False\n16:37:31 |     label_truncate: 72\n16:37:31 |     learn_embeddings: True\n16:37:31 |     learn_positional_embeddings: True\n16:37:31 |     learningrate: 5e-05\n16:37:31 |     load_from_checkpoint: False\n16:37:31 |     load_from_pretrained_ranker: True\n16:37:31 |     log_every_n_secs: 10.0\n16:37:31 |     log_every_n_steps: 50\n16:37:31 |     log_keep_fields: all\n16:37:31 |     loglevel: info\n16:37:31 |     lr_scheduler: reduceonplateau\n16:37:31 |     lr_scheduler_decay: 0.5\n16:37:31 |     lr_scheduler_patience: 3\n16:37:31 |     max_train_steps: -1\n16:37:31 |     max_train_time: 7200.0\n16:37:31 |     memory_attention: sqrt\n16:37:31 |     metrics: default\n16:37:31 |     model: transformer/classifier\n16:37:31 |     model_file: /tmp/model4\n16:37:31 |     model_parallel: False\n16:37:31 |     momentum: 0\n16:37:31 |     multitask_weights: [1]\n16:37:31 |     mutators: None\n16:37:31 |     n_decoder_layers: -1\n16:37:31 |     n_encoder_layers: -1\n16:37:31 |     n_heads: 12\n16:37:31 |     n_layers: 12\n16:37:31 |     n_positions: 1024\n16:37:31 |     n_segments: 2\n16:37:31 |     nesterov: True\n16:37:31 |     no_cuda: False\n16:37:31 |     normalize_sent_emb: False\n16:37:31 |     num_epochs: -1\n16:37:31 |     num_workers: 0\n16:37:31 |     nus: (0.7,)\n16:37:31 |     optimizer: adamax\n16:37:31 |     output_scaling: 0.06\n16:37:31 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n16:37:31 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:37:31 |     person_tokens: False\n16:37:31 |     print_scores: False\n16:37:31 |     rank_candidates: False\n16:37:31 |     rank_top_k: -1\n16:37:31 |     reduction_type: mean\n16:37:31 |     ref_class: None\n16:37:31 |     relu_dropout: 0.0\n16:37:31 |     repeat_blocking_heuristic: True\n16:37:31 |     return_cand_scores: False\n16:37:31 |     save_after_valid: True\n16:37:31 |     save_every_n_secs: -1\n16:37:31 |     save_format: conversations\n16:37:31 |     share_encoders: False\n16:37:31 |     share_word_embeddings: False\n16:37:31 |     short_final_eval: False\n16:37:31 |     special_tok_lst: None\n16:37:31 |     split_lines: False\n16:37:31 |     starttime: Dec03_16-37\n16:37:31 |     task: fromfile:parlaiformat\n16:37:31 |     tensorboard_log: False\n16:37:31 |     tensorboard_logdir: None\n16:37:31 |     text_truncate: 360\n16:37:31 |     threshold: 0.5\n16:37:31 |     topk: 5\n16:37:31 |     train_predict: False\n16:37:31 |     truncate: 1024\n16:37:31 |     update_classifier_head_only: False\n16:37:31 |     update_freq: 1\n16:37:31 |     use_memories: False\n16:37:31 |     use_reply: none\n16:37:31 |     validation_cutoff: 1.0\n16:37:31 |     validation_every_n_epochs: -1\n16:37:31 |     validation_every_n_secs: 20.0\n16:37:31 |     validation_every_n_steps: -1\n16:37:31 |     validation_max_exs: -1\n16:37:31 |     validation_metric: accuracy\n16:37:31 |     validation_metric_mode: max\n16:37:31 |     validation_patience: 30\n16:37:31 |     validation_share_agent: False\n16:37:31 |     variant: xlm\n16:37:31 |     verbose: False\n16:37:31 |     wandb_entity: None\n16:37:31 |     wandb_log: False\n16:37:31 |     wandb_name: None\n16:37:31 |     wandb_project: None\n16:37:31 |     warmup_rate: 0.0001\n16:37:31 |     warmup_updates: 1000\n16:37:31 |     weight_decay: None\n16:37:31 |     world_logs: \n16:37:31 |     wrap_memory_encoder: False\n16:37:31 | creating task(s): fromfile:parlaiformat\n16:37:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt\n16:37:31 | training...\n16:37:42 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4976 4.976e-10               .6602                 .5112   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9318           .03653              .2105   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .0200 12.15     1   283 586.6       0          0 41.45  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4976             32768   2.69    .1189 6.048 .7005 1.055e-06   121 250.7   \n    ltrunc  ltrunclen  total_train_updates  tpb   tps   ups  weighted_f1  \n         0          0                   21  404 837.3 2.077        .3632\n\n16:37:51 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7079 7.079e-10               .7643                 .6349   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9600            .6159              .9223   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .4623    12     1 280.1  1088       0          0 77.72  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7079             32768   2.76    .1189 5.987 .6479 2.955e-06 119.7 465.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   59 399.8 1554 3.895        .6891\n\n16:37:51 | creating task(s): fromfile:parlaiformat\n16:37:51 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:37:51 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt\n16:37:52 | running eval: valid\n16:37:52 | eval completed in 0.19s\n16:37:52 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1973       0          0 140.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5716 2.955e-06    72   843       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 240.5 2816            1\n\u001b[0m\n16:37:52 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:37:52 | saving best valid model: /tmp/model4\n16:37:52 | Saving dictionary to /tmp/model4.dict\n16:37:56 | task solved! stopping.\n16:37:56 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:37:56 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:37:56 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:37:56 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:37:56 | Using CUDA\n16:37:56 | loading dictionary from /tmp/model4.dict\n16:37:57 | num words = 54944\n16:38:01 | Loading existing model parameters from /tmp/model4\n16:38:03 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:38:04 | creating task(s): fromfile:parlaiformat\n16:38:04 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:38:04 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt\n16:38:04 | running eval: valid\n16:38:04 | eval completed in 0.20s\n16:38:04 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1916       0          0 136.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5716 2.955e-06    72 818.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 240.5 2735            1\n\u001b[0m\n16:38:04 | creating task(s): fromfile:parlaiformat\n16:38:04 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:38:04 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt\n16:38:04 | running eval: test\n16:38:04 | eval completed in 0.19s\n16:38:04 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1910       0          0   136   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5716 2.955e-06    72 816.1       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 240.5 2726            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-03T16:39:07.550229Z","iopub.execute_input":"2022-12-03T16:39:07.550670Z","iopub.status.idle":"2022-12-03T16:39:33.809562Z","shell.execute_reply.started":"2022-12-03T16:39:07.550632Z","shell.execute_reply":"2022-12-03T16:39:33.808356Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"16:39:14 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt)\u001b[0m\n16:39:14 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:39:14 | Using CUDA\n16:39:14 | loading dictionary from /tmp/model4.dict\n16:39:14 | num words = 54944\n16:39:19 | Loading existing model parameters from /tmp/model4\n16:39:24 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:39:26 | Opt:\n16:39:26 |     activation: gelu\n16:39:26 |     adafactor_eps: '[1e-30, 0.001]'\n16:39:26 |     adam_eps: 1e-08\n16:39:26 |     add_p1_after_newln: False\n16:39:26 |     aggregate_micro: False\n16:39:26 |     allow_missing_init_opts: False\n16:39:26 |     area_under_curve_class: None\n16:39:26 |     area_under_curve_digits: -1\n16:39:26 |     attention_dropout: 0.1\n16:39:26 |     batchsize: 40\n16:39:26 |     betas: '[0.9, 0.999]'\n16:39:26 |     bpe_add_prefix_space: None\n16:39:26 |     bpe_debug: False\n16:39:26 |     bpe_dropout: None\n16:39:26 |     bpe_merge: None\n16:39:26 |     bpe_vocab: None\n16:39:26 |     candidates: inline\n16:39:26 |     cap_num_predictions: 100\n16:39:26 |     checkpoint_activations: False\n16:39:26 |     class_weights: None\n16:39:26 |     classes: \"['__notok__', '__ok__']\"\n16:39:26 |     classes_from_file: None\n16:39:26 |     data_parallel: True\n16:39:26 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:39:26 |     datatype: train\n16:39:26 |     delimiter: '\\n'\n16:39:26 |     dict_class: parlai.core.dict:DictionaryAgent\n16:39:26 |     dict_endtoken: __start__\n16:39:26 |     dict_file: /tmp/model4.dict\n16:39:26 |     dict_include_test: False\n16:39:26 |     dict_include_valid: False\n16:39:26 |     dict_initpath: None\n16:39:26 |     dict_language: english\n16:39:26 |     dict_loaded: True\n16:39:26 |     dict_lower: True\n16:39:26 |     dict_max_ngram_size: -1\n16:39:26 |     dict_maxexs: -1\n16:39:26 |     dict_maxtokens: -1\n16:39:26 |     dict_minfreq: 0\n16:39:26 |     dict_nulltoken: __null__\n16:39:26 |     dict_starttoken: __start__\n16:39:26 |     dict_textfields: text,labels\n16:39:26 |     dict_tokenizer: bpe\n16:39:26 |     dict_unktoken: __unk__\n16:39:26 |     display_examples: False\n16:39:26 |     download_path: None\n16:39:26 |     dropout: 0.1\n16:39:26 |     dynamic_batching: None\n16:39:26 |     embedding_projection: random\n16:39:26 |     embedding_size: 768\n16:39:26 |     embedding_type: random\n16:39:26 |     embeddings_scale: False\n16:39:26 |     encode_candidate_vecs: True\n16:39:26 |     encode_candidate_vecs_batchsize: 256\n16:39:26 |     eval_batchsize: None\n16:39:26 |     eval_candidates: inline\n16:39:26 |     eval_dynamic_batching: None\n16:39:26 |     evaltask: None\n16:39:26 |     ffn_size: 3072\n16:39:26 |     final_extra_opt: \n16:39:26 |     fixed_candidate_vecs: reuse\n16:39:26 |     fixed_candidates_path: None\n16:39:26 |     force_fp16_tokens: True\n16:39:26 |     fp16: True\n16:39:26 |     fp16_impl: safe\n16:39:26 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-a.txt\n16:39:26 |     fromfile_datatype_extension: False\n16:39:26 |     gpu: -1\n16:39:26 |     gradient_clip: 0.1\n16:39:26 |     hide_labels: False\n16:39:26 |     history_add_global_end_token: None\n16:39:26 |     history_reversed: False\n16:39:26 |     history_size: 20\n16:39:26 |     ignore_bad_candidates: False\n16:39:26 |     ignore_labels: None\n16:39:26 |     image_cropsize: 224\n16:39:26 |     image_mode: raw\n16:39:26 |     image_size: 256\n16:39:26 |     inference: max\n16:39:26 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:39:26 |     init_opt: None\n16:39:26 |     interactive_candidates: fixed\n16:39:26 |     interactive_mode: False\n16:39:26 |     invsqrt_lr_decay_gamma: -1\n16:39:26 |     is_debug: False\n16:39:26 |     label_truncate: 72\n16:39:26 |     learn_embeddings: True\n16:39:26 |     learn_positional_embeddings: True\n16:39:26 |     learningrate: 5e-05\n16:39:26 |     load_from_pretrained_ranker: True\n16:39:26 |     log_every_n_secs: 10.0\n16:39:26 |     log_every_n_steps: 50\n16:39:26 |     log_keep_fields: all\n16:39:26 |     loglevel: info\n16:39:26 |     lr_scheduler: reduceonplateau\n16:39:26 |     lr_scheduler_decay: 0.5\n16:39:26 |     lr_scheduler_patience: 3\n16:39:26 |     max_train_steps: -1\n16:39:26 |     max_train_time: 7200.0\n16:39:26 |     memory_attention: sqrt\n16:39:26 |     metrics: default\n16:39:26 |     model: transformer/classifier\n16:39:26 |     model_file: /tmp/model4\n16:39:26 |     model_parallel: False\n16:39:26 |     momentum: 0\n16:39:26 |     multitask_weights: [1]\n16:39:26 |     mutators: None\n16:39:26 |     n_decoder_layers: -1\n16:39:26 |     n_encoder_layers: -1\n16:39:26 |     n_heads: 12\n16:39:26 |     n_layers: 12\n16:39:26 |     n_positions: 1024\n16:39:26 |     n_segments: 2\n16:39:26 |     nesterov: True\n16:39:26 |     no_cuda: False\n16:39:26 |     normalize_sent_emb: False\n16:39:26 |     num_epochs: -1\n16:39:26 |     num_examples: -1\n16:39:26 |     num_workers: 0\n16:39:26 |     nus: [0.7]\n16:39:26 |     optimizer: adamax\n16:39:26 |     output_scaling: 0.06\n16:39:26 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n16:39:26 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:39:26 |     person_tokens: False\n16:39:26 |     print_scores: False\n16:39:26 |     rank_candidates: False\n16:39:26 |     rank_top_k: -1\n16:39:26 |     reduction_type: mean\n16:39:26 |     ref_class: None\n16:39:26 |     relu_dropout: 0.0\n16:39:26 |     repeat_blocking_heuristic: True\n16:39:26 |     report_filename: \n16:39:26 |     return_cand_scores: False\n16:39:26 |     save_after_valid: True\n16:39:26 |     save_every_n_secs: -1\n16:39:26 |     save_format: conversations\n16:39:26 |     share_encoders: False\n16:39:26 |     share_word_embeddings: False\n16:39:26 |     short_final_eval: False\n16:39:26 |     special_tok_lst: None\n16:39:26 |     split_lines: False\n16:39:26 |     starttime: Dec03_16-37\n16:39:26 |     task: fromfile:parlaiformat\n16:39:26 |     tensorboard_log: False\n16:39:26 |     tensorboard_logdir: None\n16:39:26 |     text_truncate: 360\n16:39:26 |     threshold: 0.5\n16:39:26 |     topk: 5\n16:39:26 |     train_predict: False\n16:39:26 |     truncate: 1024\n16:39:26 |     update_classifier_head_only: False\n16:39:26 |     update_freq: 1\n16:39:26 |     use_memories: False\n16:39:26 |     use_reply: none\n16:39:26 |     validation_cutoff: 1.0\n16:39:26 |     validation_every_n_epochs: -1\n16:39:26 |     validation_every_n_secs: 20.0\n16:39:26 |     validation_every_n_steps: -1\n16:39:26 |     validation_max_exs: -1\n16:39:26 |     validation_metric: accuracy\n16:39:26 |     validation_metric_mode: max\n16:39:26 |     validation_patience: 30\n16:39:26 |     validation_share_agent: False\n16:39:26 |     variant: xlm\n16:39:26 |     verbose: False\n16:39:26 |     wandb_entity: None\n16:39:26 |     wandb_log: False\n16:39:26 |     wandb_name: None\n16:39:26 |     wandb_project: None\n16:39:26 |     warmup_rate: 0.0001\n16:39:26 |     warmup_updates: 1000\n16:39:26 |     weight_decay: None\n16:39:26 |     world_logs: \n16:39:26 |     wrap_memory_encoder: False\n16:39:26 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:39:26 | creating task(s): fromfile:parlaiformat\n16:39:26 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:39:26 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-a.txt\n16:39:32 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7250 7.25e-10               .7418                 .6695   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8316            .7059              .8049   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6286 11.46 538.2 478.2       0          0 35.54  200 .7250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.95 .6588 2.955e-06   238 211.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 776.2 689.7        .7229\u001b[0m\n16:39:32 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7250 7.25e-10               .7418                 .6695   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8316            .7059              .8049   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6286 11.46 538.2 478.2       0          0 35.54  200 .7250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.95 .6588 2.955e-06   238 211.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 776.2 689.7        .7229\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:39:33.812144Z","iopub.execute_input":"2022-12-03T16:39:33.812576Z","iopub.status.idle":"2022-12-03T16:39:59.833714Z","shell.execute_reply.started":"2022-12-03T16:39:33.812532Z","shell.execute_reply":"2022-12-03T16:39:59.832548Z"},"scrolled":true,"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"16:39:40 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_valid.txt)\u001b[0m\n16:39:40 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:39:40 | Using CUDA\n16:39:40 | loading dictionary from /tmp/model4.dict\n16:39:41 | num words = 54944\n16:39:45 | Loading existing model parameters from /tmp/model4\n16:39:51 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:39:52 | Opt:\n16:39:52 |     activation: gelu\n16:39:52 |     adafactor_eps: '[1e-30, 0.001]'\n16:39:52 |     adam_eps: 1e-08\n16:39:52 |     add_p1_after_newln: False\n16:39:52 |     aggregate_micro: False\n16:39:52 |     allow_missing_init_opts: False\n16:39:52 |     area_under_curve_class: None\n16:39:52 |     area_under_curve_digits: -1\n16:39:52 |     attention_dropout: 0.1\n16:39:52 |     batchsize: 40\n16:39:52 |     betas: '[0.9, 0.999]'\n16:39:52 |     bpe_add_prefix_space: None\n16:39:52 |     bpe_debug: False\n16:39:52 |     bpe_dropout: None\n16:39:52 |     bpe_merge: None\n16:39:52 |     bpe_vocab: None\n16:39:52 |     candidates: inline\n16:39:52 |     cap_num_predictions: 100\n16:39:52 |     checkpoint_activations: False\n16:39:52 |     class_weights: None\n16:39:52 |     classes: \"['__notok__', '__ok__']\"\n16:39:52 |     classes_from_file: None\n16:39:52 |     data_parallel: True\n16:39:52 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:39:52 |     datatype: train\n16:39:52 |     delimiter: '\\n'\n16:39:52 |     dict_class: parlai.core.dict:DictionaryAgent\n16:39:52 |     dict_endtoken: __start__\n16:39:52 |     dict_file: /tmp/model4.dict\n16:39:52 |     dict_include_test: False\n16:39:52 |     dict_include_valid: False\n16:39:52 |     dict_initpath: None\n16:39:52 |     dict_language: english\n16:39:52 |     dict_loaded: True\n16:39:52 |     dict_lower: True\n16:39:52 |     dict_max_ngram_size: -1\n16:39:52 |     dict_maxexs: -1\n16:39:52 |     dict_maxtokens: -1\n16:39:52 |     dict_minfreq: 0\n16:39:52 |     dict_nulltoken: __null__\n16:39:52 |     dict_starttoken: __start__\n16:39:52 |     dict_textfields: text,labels\n16:39:52 |     dict_tokenizer: bpe\n16:39:52 |     dict_unktoken: __unk__\n16:39:52 |     display_examples: False\n16:39:52 |     download_path: None\n16:39:52 |     dropout: 0.1\n16:39:52 |     dynamic_batching: None\n16:39:52 |     embedding_projection: random\n16:39:52 |     embedding_size: 768\n16:39:52 |     embedding_type: random\n16:39:52 |     embeddings_scale: False\n16:39:52 |     encode_candidate_vecs: True\n16:39:52 |     encode_candidate_vecs_batchsize: 256\n16:39:52 |     eval_batchsize: None\n16:39:52 |     eval_candidates: inline\n16:39:52 |     eval_dynamic_batching: None\n16:39:52 |     evaltask: None\n16:39:52 |     ffn_size: 3072\n16:39:52 |     final_extra_opt: \n16:39:52 |     fixed_candidate_vecs: reuse\n16:39:52 |     fixed_candidates_path: None\n16:39:52 |     force_fp16_tokens: True\n16:39:52 |     fp16: True\n16:39:52 |     fp16_impl: safe\n16:39:52 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-b.txt\n16:39:52 |     fromfile_datatype_extension: False\n16:39:52 |     gpu: -1\n16:39:52 |     gradient_clip: 0.1\n16:39:52 |     hide_labels: False\n16:39:52 |     history_add_global_end_token: None\n16:39:52 |     history_reversed: False\n16:39:52 |     history_size: 20\n16:39:52 |     ignore_bad_candidates: False\n16:39:52 |     ignore_labels: None\n16:39:52 |     image_cropsize: 224\n16:39:52 |     image_mode: raw\n16:39:52 |     image_size: 256\n16:39:52 |     inference: max\n16:39:52 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:39:52 |     init_opt: None\n16:39:52 |     interactive_candidates: fixed\n16:39:52 |     interactive_mode: False\n16:39:52 |     invsqrt_lr_decay_gamma: -1\n16:39:52 |     is_debug: False\n16:39:52 |     label_truncate: 72\n16:39:52 |     learn_embeddings: True\n16:39:52 |     learn_positional_embeddings: True\n16:39:52 |     learningrate: 5e-05\n16:39:52 |     load_from_pretrained_ranker: True\n16:39:52 |     log_every_n_secs: 10.0\n16:39:52 |     log_every_n_steps: 50\n16:39:52 |     log_keep_fields: all\n16:39:52 |     loglevel: info\n16:39:52 |     lr_scheduler: reduceonplateau\n16:39:52 |     lr_scheduler_decay: 0.5\n16:39:52 |     lr_scheduler_patience: 3\n16:39:52 |     max_train_steps: -1\n16:39:52 |     max_train_time: 7200.0\n16:39:52 |     memory_attention: sqrt\n16:39:52 |     metrics: default\n16:39:52 |     model: transformer/classifier\n16:39:52 |     model_file: /tmp/model4\n16:39:52 |     model_parallel: False\n16:39:52 |     momentum: 0\n16:39:52 |     multitask_weights: [1]\n16:39:52 |     mutators: None\n16:39:52 |     n_decoder_layers: -1\n16:39:52 |     n_encoder_layers: -1\n16:39:52 |     n_heads: 12\n16:39:52 |     n_layers: 12\n16:39:52 |     n_positions: 1024\n16:39:52 |     n_segments: 2\n16:39:52 |     nesterov: True\n16:39:52 |     no_cuda: False\n16:39:52 |     normalize_sent_emb: False\n16:39:52 |     num_epochs: -1\n16:39:52 |     num_examples: -1\n16:39:52 |     num_workers: 0\n16:39:52 |     nus: [0.7]\n16:39:52 |     optimizer: adamax\n16:39:52 |     output_scaling: 0.06\n16:39:52 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n16:39:52 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:39:52 |     person_tokens: False\n16:39:52 |     print_scores: False\n16:39:52 |     rank_candidates: False\n16:39:52 |     rank_top_k: -1\n16:39:52 |     reduction_type: mean\n16:39:52 |     ref_class: None\n16:39:52 |     relu_dropout: 0.0\n16:39:52 |     repeat_blocking_heuristic: True\n16:39:52 |     report_filename: \n16:39:52 |     return_cand_scores: False\n16:39:52 |     save_after_valid: True\n16:39:52 |     save_every_n_secs: -1\n16:39:52 |     save_format: conversations\n16:39:52 |     share_encoders: False\n16:39:52 |     share_word_embeddings: False\n16:39:52 |     short_final_eval: False\n16:39:52 |     special_tok_lst: None\n16:39:52 |     split_lines: False\n16:39:52 |     starttime: Dec03_16-37\n16:39:52 |     task: fromfile:parlaiformat\n16:39:52 |     tensorboard_log: False\n16:39:52 |     tensorboard_logdir: None\n16:39:52 |     text_truncate: 360\n16:39:52 |     threshold: 0.5\n16:39:52 |     topk: 5\n16:39:52 |     train_predict: False\n16:39:52 |     truncate: 1024\n16:39:52 |     update_classifier_head_only: False\n16:39:52 |     update_freq: 1\n16:39:52 |     use_memories: False\n16:39:52 |     use_reply: none\n16:39:52 |     validation_cutoff: 1.0\n16:39:52 |     validation_every_n_epochs: -1\n16:39:52 |     validation_every_n_secs: 20.0\n16:39:52 |     validation_every_n_steps: -1\n16:39:52 |     validation_max_exs: -1\n16:39:52 |     validation_metric: accuracy\n16:39:52 |     validation_metric_mode: max\n16:39:52 |     validation_patience: 30\n16:39:52 |     validation_share_agent: False\n16:39:52 |     variant: xlm\n16:39:52 |     verbose: False\n16:39:52 |     wandb_entity: None\n16:39:52 |     wandb_log: False\n16:39:52 |     wandb_name: None\n16:39:52 |     wandb_project: None\n16:39:52 |     warmup_rate: 0.0001\n16:39:52 |     warmup_updates: 1000\n16:39:52 |     weight_decay: None\n16:39:52 |     world_logs: \n16:39:52 |     wrap_memory_encoder: False\n16:39:52 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:39:52 | creating task(s): fromfile:parlaiformat\n16:39:52 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:39:52 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run4/data_train-b.txt\n16:39:58 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2750 2.75e-10               .3498                 .3305   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3714            .1808              .1951   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1684 11.46 538.2 503.2       0          0  37.4  200 .2750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.05 .7338 2.955e-06   242 226.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 780.2 729.5        .2695\u001b[0m\n16:39:58 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2750 2.75e-10               .3498                 .3305   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3714            .1808              .1951   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1684 11.46 538.2 503.2       0          0  37.4  200 .2750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.05 .7338 2.955e-06   242 226.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 780.2 729.5        .2695\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:39:59.835649Z","iopub.execute_input":"2022-12-03T16:39:59.836042Z","iopub.status.idle":"2022-12-03T16:40:01.037360Z","shell.execute_reply.started":"2022-12-03T16:39:59.836003Z","shell.execute_reply":"2022-12-03T16:40:01.036064Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:40:01.041234Z","iopub.execute_input":"2022-12-03T16:40:01.041613Z","iopub.status.idle":"2022-12-03T16:40:54.390693Z","shell.execute_reply.started":"2022-12-03T16:40:01.041582Z","shell.execute_reply":"2022-12-03T16:40:54.388825Z"},"scrolled":true,"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"16:40:08 | building dictionary first...\n16:40:08 | No model with opt yet at: /tmp/model5(.opt)\n16:40:08 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:40:08 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:40:08 | Using CUDA\n16:40:08 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:40:08 | num words = 54944\n16:40:12 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:40:20 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:40:20 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:40:20 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:40:20 | Opt:\n16:40:20 |     activation: gelu\n16:40:20 |     adafactor_eps: '(1e-30, 0.001)'\n16:40:20 |     adam_eps: 1e-08\n16:40:20 |     add_p1_after_newln: False\n16:40:20 |     aggregate_micro: False\n16:40:20 |     allow_missing_init_opts: False\n16:40:20 |     attention_dropout: 0.1\n16:40:20 |     batchsize: 20\n16:40:20 |     betas: '(0.9, 0.999)'\n16:40:20 |     bpe_add_prefix_space: None\n16:40:20 |     bpe_debug: False\n16:40:20 |     bpe_dropout: None\n16:40:20 |     bpe_merge: None\n16:40:20 |     bpe_vocab: None\n16:40:20 |     candidates: inline\n16:40:20 |     cap_num_predictions: 100\n16:40:20 |     checkpoint_activations: False\n16:40:20 |     class_weights: None\n16:40:20 |     classes: \"['__notok__', '__ok__']\"\n16:40:20 |     classes_from_file: None\n16:40:20 |     data_parallel: True\n16:40:20 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:40:20 |     datatype: train\n16:40:20 |     delimiter: '\\n'\n16:40:20 |     dict_class: parlai.core.dict:DictionaryAgent\n16:40:20 |     dict_endtoken: __start__\n16:40:20 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:40:20 |     dict_include_test: False\n16:40:20 |     dict_include_valid: False\n16:40:20 |     dict_initpath: None\n16:40:20 |     dict_language: english\n16:40:20 |     dict_loaded: True\n16:40:20 |     dict_lower: True\n16:40:20 |     dict_max_ngram_size: -1\n16:40:20 |     dict_maxexs: -1\n16:40:20 |     dict_maxtokens: -1\n16:40:20 |     dict_minfreq: 0\n16:40:20 |     dict_nulltoken: __null__\n16:40:20 |     dict_starttoken: __start__\n16:40:20 |     dict_textfields: text,labels\n16:40:20 |     dict_tokenizer: bpe\n16:40:20 |     dict_unktoken: __unk__\n16:40:20 |     display_examples: False\n16:40:20 |     download_path: None\n16:40:20 |     dropout: 0.1\n16:40:20 |     dynamic_batching: None\n16:40:20 |     embedding_projection: random\n16:40:20 |     embedding_size: 768\n16:40:20 |     embedding_type: random\n16:40:20 |     embeddings_scale: False\n16:40:20 |     encode_candidate_vecs: True\n16:40:20 |     encode_candidate_vecs_batchsize: 256\n16:40:20 |     eval_batchsize: None\n16:40:20 |     eval_candidates: inline\n16:40:20 |     eval_dynamic_batching: None\n16:40:20 |     evaltask: None\n16:40:20 |     ffn_size: 3072\n16:40:20 |     final_extra_opt: \n16:40:20 |     fixed_candidate_vecs: reuse\n16:40:20 |     fixed_candidates_path: None\n16:40:20 |     force_fp16_tokens: False\n16:40:20 |     fp16: True\n16:40:20 |     fp16_impl: safe\n16:40:20 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt\n16:40:20 |     fromfile_datatype_extension: False\n16:40:20 |     gpu: -1\n16:40:20 |     gradient_clip: 0.1\n16:40:20 |     hide_labels: False\n16:40:20 |     history_add_global_end_token: None\n16:40:20 |     history_reversed: False\n16:40:20 |     history_size: 20\n16:40:20 |     ignore_bad_candidates: False\n16:40:20 |     ignore_labels: None\n16:40:20 |     image_cropsize: 224\n16:40:20 |     image_mode: raw\n16:40:20 |     image_size: 256\n16:40:20 |     inference: max\n16:40:20 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:40:20 |     init_opt: None\n16:40:20 |     interactive_candidates: fixed\n16:40:20 |     interactive_mode: False\n16:40:20 |     invsqrt_lr_decay_gamma: -1\n16:40:20 |     is_debug: False\n16:40:20 |     label_truncate: 72\n16:40:20 |     learn_embeddings: True\n16:40:20 |     learn_positional_embeddings: True\n16:40:20 |     learningrate: 5e-05\n16:40:20 |     load_from_checkpoint: False\n16:40:20 |     load_from_pretrained_ranker: True\n16:40:20 |     log_every_n_secs: 10.0\n16:40:20 |     log_every_n_steps: 50\n16:40:20 |     log_keep_fields: all\n16:40:20 |     loglevel: info\n16:40:20 |     lr_scheduler: reduceonplateau\n16:40:20 |     lr_scheduler_decay: 0.5\n16:40:20 |     lr_scheduler_patience: 3\n16:40:20 |     max_train_steps: -1\n16:40:20 |     max_train_time: 7200.0\n16:40:20 |     memory_attention: sqrt\n16:40:20 |     metrics: default\n16:40:20 |     model: transformer/classifier\n16:40:20 |     model_file: /tmp/model5\n16:40:20 |     model_parallel: False\n16:40:20 |     momentum: 0\n16:40:20 |     multitask_weights: [1]\n16:40:20 |     mutators: None\n16:40:20 |     n_decoder_layers: -1\n16:40:20 |     n_encoder_layers: -1\n16:40:20 |     n_heads: 12\n16:40:20 |     n_layers: 12\n16:40:20 |     n_positions: 1024\n16:40:20 |     n_segments: 2\n16:40:20 |     nesterov: True\n16:40:20 |     no_cuda: False\n16:40:20 |     normalize_sent_emb: False\n16:40:20 |     num_epochs: -1\n16:40:20 |     num_workers: 0\n16:40:20 |     nus: (0.7,)\n16:40:20 |     optimizer: adamax\n16:40:20 |     output_scaling: 0.06\n16:40:20 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n16:40:20 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:40:20 |     person_tokens: False\n16:40:20 |     print_scores: False\n16:40:20 |     rank_candidates: False\n16:40:20 |     rank_top_k: -1\n16:40:20 |     reduction_type: mean\n16:40:20 |     ref_class: None\n16:40:20 |     relu_dropout: 0.0\n16:40:20 |     repeat_blocking_heuristic: True\n16:40:20 |     return_cand_scores: False\n16:40:20 |     save_after_valid: True\n16:40:20 |     save_every_n_secs: -1\n16:40:20 |     save_format: conversations\n16:40:20 |     share_encoders: False\n16:40:20 |     share_word_embeddings: False\n16:40:20 |     short_final_eval: False\n16:40:20 |     special_tok_lst: None\n16:40:20 |     split_lines: False\n16:40:20 |     starttime: Dec03_16-40\n16:40:20 |     task: fromfile:parlaiformat\n16:40:20 |     tensorboard_log: False\n16:40:20 |     tensorboard_logdir: None\n16:40:20 |     text_truncate: 360\n16:40:20 |     threshold: 0.5\n16:40:20 |     topk: 5\n16:40:20 |     train_predict: False\n16:40:20 |     truncate: 1024\n16:40:20 |     update_classifier_head_only: False\n16:40:20 |     update_freq: 1\n16:40:20 |     use_memories: False\n16:40:20 |     use_reply: none\n16:40:20 |     validation_cutoff: 1.0\n16:40:20 |     validation_every_n_epochs: -1\n16:40:20 |     validation_every_n_secs: 20.0\n16:40:20 |     validation_every_n_steps: -1\n16:40:20 |     validation_max_exs: -1\n16:40:20 |     validation_metric: accuracy\n16:40:20 |     validation_metric_mode: max\n16:40:20 |     validation_patience: 30\n16:40:20 |     validation_share_agent: False\n16:40:20 |     variant: xlm\n16:40:20 |     verbose: False\n16:40:20 |     wandb_entity: None\n16:40:20 |     wandb_log: False\n16:40:20 |     wandb_name: None\n16:40:20 |     wandb_project: None\n16:40:20 |     warmup_rate: 0.0001\n16:40:20 |     warmup_updates: 1000\n16:40:20 |     weight_decay: None\n16:40:20 |     world_logs: \n16:40:20 |     wrap_memory_encoder: False\n16:40:20 | creating task(s): fromfile:parlaiformat\n16:40:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt\n16:40:20 | training...\n16:40:30 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5595 5.595e-10               .5987                 .5633   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6389            .5119              .5543   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .4755 10.98     1 259.6 540.5       0          0 41.64  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5595             32768  3.119    .1206 6.029 .6829 1.055e-06 120.6   251   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 380.2 791.6 2.087        .5565\n\n16:40:40 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8908 8.908e-10               .8966                 .8612   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9351            .8842              .9269   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8453 10.73     1 254.6 987.6       0          0 77.58  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8908             32768  3.036    .1207 6.013 .6294 2.955e-06 120.3 466.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   59 374.9 1454 3.888        .8905\n\n16:40:40 | creating task(s): fromfile:parlaiformat\n16:40:40 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:40:40 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt\n16:40:40 | running eval: valid\n16:40:41 | eval completed in 0.22s\n16:40:41 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1603       0          0 126.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5562 2.955e-06    72 759.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  224 2362            1\n\u001b[0m\n16:40:41 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:40:41 | saving best valid model: /tmp/model5\n16:40:41 | Saving dictionary to /tmp/model5.dict\n16:40:44 | task solved! stopping.\n16:40:44 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:40:44 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:40:44 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:40:44 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:40:44 | Using CUDA\n16:40:44 | loading dictionary from /tmp/model5.dict\n16:40:44 | num words = 54944\n16:40:49 | Loading existing model parameters from /tmp/model5\n16:40:50 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:40:52 | creating task(s): fromfile:parlaiformat\n16:40:52 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:40:52 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt\n16:40:52 | running eval: valid\n16:40:52 | eval completed in 0.20s\n16:40:52 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1729       0          0 136.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5562 2.955e-06    72 818.8       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  224 2548            1\n\u001b[0m\n16:40:52 | creating task(s): fromfile:parlaiformat\n16:40:52 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:40:52 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt\n16:40:52 | running eval: test\n16:40:52 | eval completed in 0.19s\n16:40:52 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1729       0          0 136.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5562 2.955e-06    72   819       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  224 2548            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:40:54.395226Z","iopub.execute_input":"2022-12-03T16:40:54.395768Z","iopub.status.idle":"2022-12-03T16:41:21.160368Z","shell.execute_reply.started":"2022-12-03T16:40:54.395714Z","shell.execute_reply":"2022-12-03T16:41:21.159159Z"},"scrolled":true,"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"16:41:01 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt)\u001b[0m\n16:41:01 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:41:01 | Using CUDA\n16:41:01 | loading dictionary from /tmp/model5.dict\n16:41:01 | num words = 54944\n16:41:06 | Loading existing model parameters from /tmp/model5\n16:41:12 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:41:13 | Opt:\n16:41:13 |     activation: gelu\n16:41:13 |     adafactor_eps: '[1e-30, 0.001]'\n16:41:13 |     adam_eps: 1e-08\n16:41:13 |     add_p1_after_newln: False\n16:41:13 |     aggregate_micro: False\n16:41:13 |     allow_missing_init_opts: False\n16:41:13 |     area_under_curve_class: None\n16:41:13 |     area_under_curve_digits: -1\n16:41:13 |     attention_dropout: 0.1\n16:41:13 |     batchsize: 40\n16:41:13 |     betas: '[0.9, 0.999]'\n16:41:13 |     bpe_add_prefix_space: None\n16:41:13 |     bpe_debug: False\n16:41:13 |     bpe_dropout: None\n16:41:13 |     bpe_merge: None\n16:41:13 |     bpe_vocab: None\n16:41:13 |     candidates: inline\n16:41:13 |     cap_num_predictions: 100\n16:41:13 |     checkpoint_activations: False\n16:41:13 |     class_weights: None\n16:41:13 |     classes: \"['__notok__', '__ok__']\"\n16:41:13 |     classes_from_file: None\n16:41:13 |     data_parallel: True\n16:41:13 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:41:13 |     datatype: train\n16:41:13 |     delimiter: '\\n'\n16:41:13 |     dict_class: parlai.core.dict:DictionaryAgent\n16:41:13 |     dict_endtoken: __start__\n16:41:13 |     dict_file: /tmp/model5.dict\n16:41:13 |     dict_include_test: False\n16:41:13 |     dict_include_valid: False\n16:41:13 |     dict_initpath: None\n16:41:13 |     dict_language: english\n16:41:13 |     dict_loaded: True\n16:41:13 |     dict_lower: True\n16:41:13 |     dict_max_ngram_size: -1\n16:41:13 |     dict_maxexs: -1\n16:41:13 |     dict_maxtokens: -1\n16:41:13 |     dict_minfreq: 0\n16:41:13 |     dict_nulltoken: __null__\n16:41:13 |     dict_starttoken: __start__\n16:41:13 |     dict_textfields: text,labels\n16:41:13 |     dict_tokenizer: bpe\n16:41:13 |     dict_unktoken: __unk__\n16:41:13 |     display_examples: False\n16:41:13 |     download_path: None\n16:41:13 |     dropout: 0.1\n16:41:13 |     dynamic_batching: None\n16:41:13 |     embedding_projection: random\n16:41:13 |     embedding_size: 768\n16:41:13 |     embedding_type: random\n16:41:13 |     embeddings_scale: False\n16:41:13 |     encode_candidate_vecs: True\n16:41:13 |     encode_candidate_vecs_batchsize: 256\n16:41:13 |     eval_batchsize: None\n16:41:13 |     eval_candidates: inline\n16:41:13 |     eval_dynamic_batching: None\n16:41:13 |     evaltask: None\n16:41:13 |     ffn_size: 3072\n16:41:13 |     final_extra_opt: \n16:41:13 |     fixed_candidate_vecs: reuse\n16:41:13 |     fixed_candidates_path: None\n16:41:13 |     force_fp16_tokens: True\n16:41:13 |     fp16: True\n16:41:13 |     fp16_impl: safe\n16:41:13 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-a.txt\n16:41:13 |     fromfile_datatype_extension: False\n16:41:13 |     gpu: -1\n16:41:13 |     gradient_clip: 0.1\n16:41:13 |     hide_labels: False\n16:41:13 |     history_add_global_end_token: None\n16:41:13 |     history_reversed: False\n16:41:13 |     history_size: 20\n16:41:13 |     ignore_bad_candidates: False\n16:41:13 |     ignore_labels: None\n16:41:13 |     image_cropsize: 224\n16:41:13 |     image_mode: raw\n16:41:13 |     image_size: 256\n16:41:13 |     inference: max\n16:41:13 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:41:13 |     init_opt: None\n16:41:13 |     interactive_candidates: fixed\n16:41:13 |     interactive_mode: False\n16:41:13 |     invsqrt_lr_decay_gamma: -1\n16:41:13 |     is_debug: False\n16:41:13 |     label_truncate: 72\n16:41:13 |     learn_embeddings: True\n16:41:13 |     learn_positional_embeddings: True\n16:41:13 |     learningrate: 5e-05\n16:41:13 |     load_from_pretrained_ranker: True\n16:41:13 |     log_every_n_secs: 10.0\n16:41:13 |     log_every_n_steps: 50\n16:41:13 |     log_keep_fields: all\n16:41:13 |     loglevel: info\n16:41:13 |     lr_scheduler: reduceonplateau\n16:41:13 |     lr_scheduler_decay: 0.5\n16:41:13 |     lr_scheduler_patience: 3\n16:41:13 |     max_train_steps: -1\n16:41:13 |     max_train_time: 7200.0\n16:41:13 |     memory_attention: sqrt\n16:41:13 |     metrics: default\n16:41:13 |     model: transformer/classifier\n16:41:13 |     model_file: /tmp/model5\n16:41:13 |     model_parallel: False\n16:41:13 |     momentum: 0\n16:41:13 |     multitask_weights: [1]\n16:41:13 |     mutators: None\n16:41:13 |     n_decoder_layers: -1\n16:41:13 |     n_encoder_layers: -1\n16:41:13 |     n_heads: 12\n16:41:13 |     n_layers: 12\n16:41:13 |     n_positions: 1024\n16:41:13 |     n_segments: 2\n16:41:13 |     nesterov: True\n16:41:13 |     no_cuda: False\n16:41:13 |     normalize_sent_emb: False\n16:41:13 |     num_epochs: -1\n16:41:13 |     num_examples: -1\n16:41:13 |     num_workers: 0\n16:41:13 |     nus: [0.7]\n16:41:13 |     optimizer: adamax\n16:41:13 |     output_scaling: 0.06\n16:41:13 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n16:41:13 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:41:13 |     person_tokens: False\n16:41:13 |     print_scores: False\n16:41:13 |     rank_candidates: False\n16:41:13 |     rank_top_k: -1\n16:41:13 |     reduction_type: mean\n16:41:13 |     ref_class: None\n16:41:13 |     relu_dropout: 0.0\n16:41:13 |     repeat_blocking_heuristic: True\n16:41:13 |     report_filename: \n16:41:13 |     return_cand_scores: False\n16:41:13 |     save_after_valid: True\n16:41:13 |     save_every_n_secs: -1\n16:41:13 |     save_format: conversations\n16:41:13 |     share_encoders: False\n16:41:13 |     share_word_embeddings: False\n16:41:13 |     short_final_eval: False\n16:41:13 |     special_tok_lst: None\n16:41:13 |     split_lines: False\n16:41:13 |     starttime: Dec03_16-40\n16:41:13 |     task: fromfile:parlaiformat\n16:41:13 |     tensorboard_log: False\n16:41:13 |     tensorboard_logdir: None\n16:41:13 |     text_truncate: 360\n16:41:13 |     threshold: 0.5\n16:41:13 |     topk: 5\n16:41:13 |     train_predict: False\n16:41:13 |     truncate: 1024\n16:41:13 |     update_classifier_head_only: False\n16:41:13 |     update_freq: 1\n16:41:13 |     use_memories: False\n16:41:13 |     use_reply: none\n16:41:13 |     validation_cutoff: 1.0\n16:41:13 |     validation_every_n_epochs: -1\n16:41:13 |     validation_every_n_secs: 20.0\n16:41:13 |     validation_every_n_steps: -1\n16:41:13 |     validation_max_exs: -1\n16:41:13 |     validation_metric: accuracy\n16:41:13 |     validation_metric_mode: max\n16:41:13 |     validation_patience: 30\n16:41:13 |     validation_share_agent: False\n16:41:13 |     variant: xlm\n16:41:13 |     verbose: False\n16:41:13 |     wandb_entity: None\n16:41:13 |     wandb_log: False\n16:41:13 |     wandb_name: None\n16:41:13 |     wandb_project: None\n16:41:13 |     warmup_rate: 0.0001\n16:41:13 |     warmup_updates: 1000\n16:41:13 |     weight_decay: None\n16:41:13 |     world_logs: \n16:41:13 |     wrap_memory_encoder: False\n16:41:14 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:41:14 | creating task(s): fromfile:parlaiformat\n16:41:14 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:41:14 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-a.txt\n16:41:19 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6600 6.6e-10               .7094                 .6194   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8300            .5904              .7424   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4900 11.79 551.8 543.5       0          0  39.4  200 .6600   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6562 2.955e-06   240 236.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 791.8 779.9        .6499\u001b[0m\n16:41:19 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6600 6.6e-10               .7094                 .6194   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8300            .5904              .7424   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4900 11.79 551.8 543.5       0          0  39.4  200 .6600   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6562 2.955e-06   240 236.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 791.8 779.9        .6499\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:41:21.162296Z","iopub.execute_input":"2022-12-03T16:41:21.162718Z","iopub.status.idle":"2022-12-03T16:41:47.870616Z","shell.execute_reply.started":"2022-12-03T16:41:21.162678Z","shell.execute_reply":"2022-12-03T16:41:47.869441Z"},"scrolled":true,"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"16:41:28 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_valid.txt)\u001b[0m\n16:41:28 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:41:28 | Using CUDA\n16:41:28 | loading dictionary from /tmp/model5.dict\n16:41:28 | num words = 54944\n16:41:32 | Loading existing model parameters from /tmp/model5\n16:41:38 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:41:40 | Opt:\n16:41:40 |     activation: gelu\n16:41:40 |     adafactor_eps: '[1e-30, 0.001]'\n16:41:40 |     adam_eps: 1e-08\n16:41:40 |     add_p1_after_newln: False\n16:41:40 |     aggregate_micro: False\n16:41:40 |     allow_missing_init_opts: False\n16:41:40 |     area_under_curve_class: None\n16:41:40 |     area_under_curve_digits: -1\n16:41:40 |     attention_dropout: 0.1\n16:41:40 |     batchsize: 40\n16:41:40 |     betas: '[0.9, 0.999]'\n16:41:40 |     bpe_add_prefix_space: None\n16:41:40 |     bpe_debug: False\n16:41:40 |     bpe_dropout: None\n16:41:40 |     bpe_merge: None\n16:41:40 |     bpe_vocab: None\n16:41:40 |     candidates: inline\n16:41:40 |     cap_num_predictions: 100\n16:41:40 |     checkpoint_activations: False\n16:41:40 |     class_weights: None\n16:41:40 |     classes: \"['__notok__', '__ok__']\"\n16:41:40 |     classes_from_file: None\n16:41:40 |     data_parallel: True\n16:41:40 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:41:40 |     datatype: train\n16:41:40 |     delimiter: '\\n'\n16:41:40 |     dict_class: parlai.core.dict:DictionaryAgent\n16:41:40 |     dict_endtoken: __start__\n16:41:40 |     dict_file: /tmp/model5.dict\n16:41:40 |     dict_include_test: False\n16:41:40 |     dict_include_valid: False\n16:41:40 |     dict_initpath: None\n16:41:40 |     dict_language: english\n16:41:40 |     dict_loaded: True\n16:41:40 |     dict_lower: True\n16:41:40 |     dict_max_ngram_size: -1\n16:41:40 |     dict_maxexs: -1\n16:41:40 |     dict_maxtokens: -1\n16:41:40 |     dict_minfreq: 0\n16:41:40 |     dict_nulltoken: __null__\n16:41:40 |     dict_starttoken: __start__\n16:41:40 |     dict_textfields: text,labels\n16:41:40 |     dict_tokenizer: bpe\n16:41:40 |     dict_unktoken: __unk__\n16:41:40 |     display_examples: False\n16:41:40 |     download_path: None\n16:41:40 |     dropout: 0.1\n16:41:40 |     dynamic_batching: None\n16:41:40 |     embedding_projection: random\n16:41:40 |     embedding_size: 768\n16:41:40 |     embedding_type: random\n16:41:40 |     embeddings_scale: False\n16:41:40 |     encode_candidate_vecs: True\n16:41:40 |     encode_candidate_vecs_batchsize: 256\n16:41:40 |     eval_batchsize: None\n16:41:40 |     eval_candidates: inline\n16:41:40 |     eval_dynamic_batching: None\n16:41:40 |     evaltask: None\n16:41:40 |     ffn_size: 3072\n16:41:40 |     final_extra_opt: \n16:41:40 |     fixed_candidate_vecs: reuse\n16:41:40 |     fixed_candidates_path: None\n16:41:40 |     force_fp16_tokens: True\n16:41:40 |     fp16: True\n16:41:40 |     fp16_impl: safe\n16:41:40 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-b.txt\n16:41:40 |     fromfile_datatype_extension: False\n16:41:40 |     gpu: -1\n16:41:40 |     gradient_clip: 0.1\n16:41:40 |     hide_labels: False\n16:41:40 |     history_add_global_end_token: None\n16:41:40 |     history_reversed: False\n16:41:40 |     history_size: 20\n16:41:40 |     ignore_bad_candidates: False\n16:41:40 |     ignore_labels: None\n16:41:40 |     image_cropsize: 224\n16:41:40 |     image_mode: raw\n16:41:40 |     image_size: 256\n16:41:40 |     inference: max\n16:41:40 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:41:40 |     init_opt: None\n16:41:40 |     interactive_candidates: fixed\n16:41:40 |     interactive_mode: False\n16:41:40 |     invsqrt_lr_decay_gamma: -1\n16:41:40 |     is_debug: False\n16:41:40 |     label_truncate: 72\n16:41:40 |     learn_embeddings: True\n16:41:40 |     learn_positional_embeddings: True\n16:41:40 |     learningrate: 5e-05\n16:41:40 |     load_from_pretrained_ranker: True\n16:41:40 |     log_every_n_secs: 10.0\n16:41:40 |     log_every_n_steps: 50\n16:41:40 |     log_keep_fields: all\n16:41:40 |     loglevel: info\n16:41:40 |     lr_scheduler: reduceonplateau\n16:41:40 |     lr_scheduler_decay: 0.5\n16:41:40 |     lr_scheduler_patience: 3\n16:41:40 |     max_train_steps: -1\n16:41:40 |     max_train_time: 7200.0\n16:41:40 |     memory_attention: sqrt\n16:41:40 |     metrics: default\n16:41:40 |     model: transformer/classifier\n16:41:40 |     model_file: /tmp/model5\n16:41:40 |     model_parallel: False\n16:41:40 |     momentum: 0\n16:41:40 |     multitask_weights: [1]\n16:41:40 |     mutators: None\n16:41:40 |     n_decoder_layers: -1\n16:41:40 |     n_encoder_layers: -1\n16:41:40 |     n_heads: 12\n16:41:40 |     n_layers: 12\n16:41:40 |     n_positions: 1024\n16:41:40 |     n_segments: 2\n16:41:40 |     nesterov: True\n16:41:40 |     no_cuda: False\n16:41:40 |     normalize_sent_emb: False\n16:41:40 |     num_epochs: -1\n16:41:40 |     num_examples: -1\n16:41:40 |     num_workers: 0\n16:41:40 |     nus: [0.7]\n16:41:40 |     optimizer: adamax\n16:41:40 |     output_scaling: 0.06\n16:41:40 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n16:41:40 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:41:40 |     person_tokens: False\n16:41:40 |     print_scores: False\n16:41:40 |     rank_candidates: False\n16:41:40 |     rank_top_k: -1\n16:41:40 |     reduction_type: mean\n16:41:40 |     ref_class: None\n16:41:40 |     relu_dropout: 0.0\n16:41:40 |     repeat_blocking_heuristic: True\n16:41:40 |     report_filename: \n16:41:40 |     return_cand_scores: False\n16:41:40 |     save_after_valid: True\n16:41:40 |     save_every_n_secs: -1\n16:41:40 |     save_format: conversations\n16:41:40 |     share_encoders: False\n16:41:40 |     share_word_embeddings: False\n16:41:40 |     short_final_eval: False\n16:41:40 |     special_tok_lst: None\n16:41:40 |     split_lines: False\n16:41:40 |     starttime: Dec03_16-40\n16:41:40 |     task: fromfile:parlaiformat\n16:41:40 |     tensorboard_log: False\n16:41:40 |     tensorboard_logdir: None\n16:41:40 |     text_truncate: 360\n16:41:40 |     threshold: 0.5\n16:41:40 |     topk: 5\n16:41:40 |     train_predict: False\n16:41:40 |     truncate: 1024\n16:41:40 |     update_classifier_head_only: False\n16:41:40 |     update_freq: 1\n16:41:40 |     use_memories: False\n16:41:40 |     use_reply: none\n16:41:40 |     validation_cutoff: 1.0\n16:41:40 |     validation_every_n_epochs: -1\n16:41:40 |     validation_every_n_secs: 20.0\n16:41:40 |     validation_every_n_steps: -1\n16:41:40 |     validation_max_exs: -1\n16:41:40 |     validation_metric: accuracy\n16:41:40 |     validation_metric_mode: max\n16:41:40 |     validation_patience: 30\n16:41:40 |     validation_share_agent: False\n16:41:40 |     variant: xlm\n16:41:40 |     verbose: False\n16:41:40 |     wandb_entity: None\n16:41:40 |     wandb_log: False\n16:41:40 |     wandb_name: None\n16:41:40 |     wandb_project: None\n16:41:40 |     warmup_rate: 0.0001\n16:41:40 |     warmup_updates: 1000\n16:41:40 |     weight_decay: None\n16:41:40 |     world_logs: \n16:41:40 |     wrap_memory_encoder: False\n16:41:40 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:41:40 | creating task(s): fromfile:parlaiformat\n16:41:40 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:41:40 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type1/run5/data_train-b.txt\n16:41:46 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3400 3.4e-10               .4359                 .3806   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5100            .2048              .2576   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1700 11.79 551.8 504.5       0          0 36.57  200 .3400   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7412 2.955e-06   240 219.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 791.8 723.9        .3204\u001b[0m\n16:41:46 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3400 3.4e-10               .4359                 .3806   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5100            .2048              .2576   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1700 11.79 551.8 504.5       0          0 36.57  200 .3400   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7412 2.955e-06   240 219.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 791.8 723.9        .3204\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:41:47.872651Z","iopub.execute_input":"2022-12-03T16:41:47.873054Z","iopub.status.idle":"2022-12-03T16:41:49.017797Z","shell.execute_reply.started":"2022-12-03T16:41:47.873014Z","shell.execute_reply":"2022-12-03T16:41:49.016497Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Choose completion prev1corr1type2","metadata":{}},{"cell_type":"markdown","source":"run 1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:41:49.019846Z","iopub.execute_input":"2022-12-03T16:41:49.020243Z","iopub.status.idle":"2022-12-03T16:43:09.339597Z","shell.execute_reply.started":"2022-12-03T16:41:49.020204Z","shell.execute_reply":"2022-12-03T16:43:09.338417Z"},"scrolled":true,"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"16:41:56 | building dictionary first...\n16:41:56 | No model with opt yet at: /tmp/model1(.opt)\n16:41:56 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:41:56 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:41:56 | Using CUDA\n16:41:56 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:41:56 | num words = 54944\n16:42:00 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:42:09 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:42:09 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:42:09 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:42:09 | Opt:\n16:42:09 |     activation: gelu\n16:42:09 |     adafactor_eps: '(1e-30, 0.001)'\n16:42:09 |     adam_eps: 1e-08\n16:42:09 |     add_p1_after_newln: False\n16:42:09 |     aggregate_micro: False\n16:42:09 |     allow_missing_init_opts: False\n16:42:09 |     attention_dropout: 0.1\n16:42:09 |     batchsize: 20\n16:42:09 |     betas: '(0.9, 0.999)'\n16:42:09 |     bpe_add_prefix_space: None\n16:42:09 |     bpe_debug: False\n16:42:09 |     bpe_dropout: None\n16:42:09 |     bpe_merge: None\n16:42:09 |     bpe_vocab: None\n16:42:09 |     candidates: inline\n16:42:09 |     cap_num_predictions: 100\n16:42:09 |     checkpoint_activations: False\n16:42:09 |     class_weights: None\n16:42:09 |     classes: \"['__notok__', '__ok__']\"\n16:42:09 |     classes_from_file: None\n16:42:09 |     data_parallel: True\n16:42:09 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:42:09 |     datatype: train\n16:42:09 |     delimiter: '\\n'\n16:42:09 |     dict_class: parlai.core.dict:DictionaryAgent\n16:42:09 |     dict_endtoken: __start__\n16:42:09 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:42:09 |     dict_include_test: False\n16:42:09 |     dict_include_valid: False\n16:42:09 |     dict_initpath: None\n16:42:09 |     dict_language: english\n16:42:09 |     dict_loaded: True\n16:42:09 |     dict_lower: True\n16:42:09 |     dict_max_ngram_size: -1\n16:42:09 |     dict_maxexs: -1\n16:42:09 |     dict_maxtokens: -1\n16:42:09 |     dict_minfreq: 0\n16:42:09 |     dict_nulltoken: __null__\n16:42:09 |     dict_starttoken: __start__\n16:42:09 |     dict_textfields: text,labels\n16:42:09 |     dict_tokenizer: bpe\n16:42:09 |     dict_unktoken: __unk__\n16:42:09 |     display_examples: False\n16:42:09 |     download_path: None\n16:42:09 |     dropout: 0.1\n16:42:09 |     dynamic_batching: None\n16:42:09 |     embedding_projection: random\n16:42:09 |     embedding_size: 768\n16:42:09 |     embedding_type: random\n16:42:09 |     embeddings_scale: False\n16:42:09 |     encode_candidate_vecs: True\n16:42:09 |     encode_candidate_vecs_batchsize: 256\n16:42:09 |     eval_batchsize: None\n16:42:09 |     eval_candidates: inline\n16:42:09 |     eval_dynamic_batching: None\n16:42:09 |     evaltask: None\n16:42:09 |     ffn_size: 3072\n16:42:09 |     final_extra_opt: \n16:42:09 |     fixed_candidate_vecs: reuse\n16:42:09 |     fixed_candidates_path: None\n16:42:09 |     force_fp16_tokens: False\n16:42:09 |     fp16: True\n16:42:09 |     fp16_impl: safe\n16:42:09 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt\n16:42:09 |     fromfile_datatype_extension: False\n16:42:09 |     gpu: -1\n16:42:09 |     gradient_clip: 0.1\n16:42:09 |     hide_labels: False\n16:42:09 |     history_add_global_end_token: None\n16:42:09 |     history_reversed: False\n16:42:09 |     history_size: 20\n16:42:09 |     ignore_bad_candidates: False\n16:42:09 |     ignore_labels: None\n16:42:09 |     image_cropsize: 224\n16:42:09 |     image_mode: raw\n16:42:09 |     image_size: 256\n16:42:09 |     inference: max\n16:42:09 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:42:09 |     init_opt: None\n16:42:09 |     interactive_candidates: fixed\n16:42:09 |     interactive_mode: False\n16:42:09 |     invsqrt_lr_decay_gamma: -1\n16:42:09 |     is_debug: False\n16:42:09 |     label_truncate: 72\n16:42:09 |     learn_embeddings: True\n16:42:09 |     learn_positional_embeddings: True\n16:42:09 |     learningrate: 5e-05\n16:42:09 |     load_from_checkpoint: False\n16:42:09 |     load_from_pretrained_ranker: True\n16:42:09 |     log_every_n_secs: 10.0\n16:42:09 |     log_every_n_steps: 50\n16:42:09 |     log_keep_fields: all\n16:42:09 |     loglevel: info\n16:42:09 |     lr_scheduler: reduceonplateau\n16:42:09 |     lr_scheduler_decay: 0.5\n16:42:09 |     lr_scheduler_patience: 3\n16:42:09 |     max_train_steps: -1\n16:42:09 |     max_train_time: 7200.0\n16:42:09 |     memory_attention: sqrt\n16:42:09 |     metrics: default\n16:42:09 |     model: transformer/classifier\n16:42:09 |     model_file: /tmp/model1\n16:42:09 |     model_parallel: False\n16:42:09 |     momentum: 0\n16:42:09 |     multitask_weights: [1]\n16:42:09 |     mutators: None\n16:42:09 |     n_decoder_layers: -1\n16:42:09 |     n_encoder_layers: -1\n16:42:09 |     n_heads: 12\n16:42:09 |     n_layers: 12\n16:42:09 |     n_positions: 1024\n16:42:09 |     n_segments: 2\n16:42:09 |     nesterov: True\n16:42:09 |     no_cuda: False\n16:42:09 |     normalize_sent_emb: False\n16:42:09 |     num_epochs: -1\n16:42:09 |     num_workers: 0\n16:42:09 |     nus: (0.7,)\n16:42:09 |     optimizer: adamax\n16:42:09 |     output_scaling: 0.06\n16:42:09 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n16:42:09 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:42:09 |     person_tokens: False\n16:42:09 |     print_scores: False\n16:42:09 |     rank_candidates: False\n16:42:09 |     rank_top_k: -1\n16:42:09 |     reduction_type: mean\n16:42:09 |     ref_class: None\n16:42:09 |     relu_dropout: 0.0\n16:42:09 |     repeat_blocking_heuristic: True\n16:42:09 |     return_cand_scores: False\n16:42:09 |     save_after_valid: True\n16:42:09 |     save_every_n_secs: -1\n16:42:09 |     save_format: conversations\n16:42:09 |     share_encoders: False\n16:42:09 |     share_word_embeddings: False\n16:42:09 |     short_final_eval: False\n16:42:09 |     special_tok_lst: None\n16:42:09 |     split_lines: False\n16:42:09 |     starttime: Dec03_16-41\n16:42:09 |     task: fromfile:parlaiformat\n16:42:09 |     tensorboard_log: False\n16:42:09 |     tensorboard_logdir: None\n16:42:09 |     text_truncate: 360\n16:42:09 |     threshold: 0.5\n16:42:09 |     topk: 5\n16:42:09 |     train_predict: False\n16:42:09 |     truncate: 1024\n16:42:09 |     update_classifier_head_only: False\n16:42:09 |     update_freq: 1\n16:42:09 |     use_memories: False\n16:42:09 |     use_reply: none\n16:42:09 |     validation_cutoff: 1.0\n16:42:09 |     validation_every_n_epochs: -1\n16:42:09 |     validation_every_n_secs: 20.0\n16:42:09 |     validation_every_n_steps: -1\n16:42:09 |     validation_max_exs: -1\n16:42:09 |     validation_metric: accuracy\n16:42:09 |     validation_metric_mode: max\n16:42:09 |     validation_patience: 30\n16:42:09 |     validation_share_agent: False\n16:42:09 |     variant: xlm\n16:42:09 |     verbose: False\n16:42:09 |     wandb_entity: None\n16:42:09 |     wandb_log: False\n16:42:09 |     wandb_name: None\n16:42:09 |     wandb_project: None\n16:42:09 |     warmup_rate: 0.0001\n16:42:09 |     warmup_updates: 1000\n16:42:09 |     weight_decay: None\n16:42:09 |     world_logs: \n16:42:09 |     wrap_memory_encoder: False\n16:42:10 | creating task(s): fromfile:parlaiformat\n16:42:10 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt\n16:42:10 | training...\n16:42:20 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5619 5.619e-10               .5233                 .5580   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4927            .5947              .5649   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6279 11.39     1 267.8 553.7       0          0 41.35  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5619             32768  2.794    .1189 5.976 .6869 1.055e-06 119.5 247.1   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 387.3 800.8 2.072        .5599\n\n16:42:30 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8237 8.237e-10               .8237                 .8046   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8437            .8237              .8437   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8046 11.62     1 272.5  1070       0          0  78.5  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8237             32768  2.983    .1189 5.976 .6429 2.955e-06 119.5 469.2   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                   59  392 1539 3.934        .8237\n\n16:42:30 | creating task(s): fromfile:parlaiformat\n16:42:30 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:42:30 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt\n16:42:30 | running eval: valid\n16:42:30 | eval completed in 0.19s\n16:42:30 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9600                 .9231   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9565                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 11.46 161.5  1863       0          0 138.3   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5798 2.955e-06    72 830.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 233.5 2694        .9583\n\u001b[0m\n16:42:30 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n16:42:30 | saving best valid model: /tmp/model1\n16:42:30 | Saving dictionary to /tmp/model1.dict\n16:42:34 | saving model checkpoint: /tmp/model1.checkpoint\n16:42:34 | Saving dictionary to /tmp/model1.checkpoint.dict\n16:42:51 | time:41s total_exs:1880 total_steps:94 epochs:78.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9686 9.686e-10               .9691                 .9452   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9942            .9680              .9940   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9433 11.22     1 264.4 911.5       0          0 68.94  700   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9686             32768  3.104    .1189 5.991 .5198 4.705e-06 119.8 413.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   94 384.3 1325 3.455        .9686\n\n16:42:54 | time:44s total_exs:2080 total_steps:104 epochs:86.67\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .9950 9.95e-10               .9956                 .9912   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9942                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9885 11.53     1 270.5  1030       0          0 76.19  200   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9950             32768  2.946    .1189  6.13 .4024 5.204e-06 122.6   467   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  104 393.1 1497 3.842        .9950\n\n16:42:54 | running eval: valid\n16:42:54 | eval completed in 0.19s\n16:42:54 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1868       0          0 138.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3346 5.204e-06    72 832.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 233.5 2700            1\n\u001b[0m\n16:42:54 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n16:42:54 | saving best valid model: /tmp/model1\n16:42:59 | task solved! stopping.\n16:42:59 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:42:59 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:42:59 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:42:59 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:42:59 | Using CUDA\n16:42:59 | loading dictionary from /tmp/model1.dict\n16:42:59 | num words = 54944\n16:43:04 | Loading existing model parameters from /tmp/model1\n16:43:05 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:43:07 | creating task(s): fromfile:parlaiformat\n16:43:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:43:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt\n16:43:07 | running eval: valid\n16:43:07 | eval completed in 0.20s\n16:43:07 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1811       0          0 134.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3346 5.204e-06    72 807.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 233.5 2618            1\n\u001b[0m\n16:43:07 | creating task(s): fromfile:parlaiformat\n16:43:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:43:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt\n16:43:07 | running eval: test\n16:43:07 | eval completed in 0.19s\n16:43:07 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1849       0          0 137.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3346 5.204e-06    72 824.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 233.5 2674            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:43:09.341336Z","iopub.execute_input":"2022-12-03T16:43:09.341741Z","iopub.status.idle":"2022-12-03T16:43:37.907090Z","shell.execute_reply.started":"2022-12-03T16:43:09.341701Z","shell.execute_reply":"2022-12-03T16:43:37.905832Z"},"scrolled":true,"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"16:43:17 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt)\u001b[0m\n16:43:17 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:43:17 | Using CUDA\n16:43:17 | loading dictionary from /tmp/model1.dict\n16:43:17 | num words = 54944\n16:43:21 | Loading existing model parameters from /tmp/model1\n16:43:29 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:43:30 | Opt:\n16:43:30 |     activation: gelu\n16:43:30 |     adafactor_eps: '[1e-30, 0.001]'\n16:43:30 |     adam_eps: 1e-08\n16:43:30 |     add_p1_after_newln: False\n16:43:30 |     aggregate_micro: False\n16:43:30 |     allow_missing_init_opts: False\n16:43:30 |     area_under_curve_class: None\n16:43:30 |     area_under_curve_digits: -1\n16:43:30 |     attention_dropout: 0.1\n16:43:30 |     batchsize: 40\n16:43:30 |     betas: '[0.9, 0.999]'\n16:43:30 |     bpe_add_prefix_space: None\n16:43:30 |     bpe_debug: False\n16:43:30 |     bpe_dropout: None\n16:43:30 |     bpe_merge: None\n16:43:30 |     bpe_vocab: None\n16:43:30 |     candidates: inline\n16:43:30 |     cap_num_predictions: 100\n16:43:30 |     checkpoint_activations: False\n16:43:30 |     class_weights: None\n16:43:30 |     classes: \"['__notok__', '__ok__']\"\n16:43:30 |     classes_from_file: None\n16:43:30 |     data_parallel: True\n16:43:30 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:43:30 |     datatype: train\n16:43:30 |     delimiter: '\\n'\n16:43:30 |     dict_class: parlai.core.dict:DictionaryAgent\n16:43:30 |     dict_endtoken: __start__\n16:43:30 |     dict_file: /tmp/model1.dict\n16:43:30 |     dict_include_test: False\n16:43:30 |     dict_include_valid: False\n16:43:30 |     dict_initpath: None\n16:43:30 |     dict_language: english\n16:43:30 |     dict_loaded: True\n16:43:30 |     dict_lower: True\n16:43:30 |     dict_max_ngram_size: -1\n16:43:30 |     dict_maxexs: -1\n16:43:30 |     dict_maxtokens: -1\n16:43:30 |     dict_minfreq: 0\n16:43:30 |     dict_nulltoken: __null__\n16:43:30 |     dict_starttoken: __start__\n16:43:30 |     dict_textfields: text,labels\n16:43:30 |     dict_tokenizer: bpe\n16:43:30 |     dict_unktoken: __unk__\n16:43:30 |     display_examples: False\n16:43:30 |     download_path: None\n16:43:30 |     dropout: 0.1\n16:43:30 |     dynamic_batching: None\n16:43:30 |     embedding_projection: random\n16:43:30 |     embedding_size: 768\n16:43:30 |     embedding_type: random\n16:43:30 |     embeddings_scale: False\n16:43:30 |     encode_candidate_vecs: True\n16:43:30 |     encode_candidate_vecs_batchsize: 256\n16:43:30 |     eval_batchsize: None\n16:43:30 |     eval_candidates: inline\n16:43:30 |     eval_dynamic_batching: None\n16:43:30 |     evaltask: None\n16:43:30 |     ffn_size: 3072\n16:43:30 |     final_extra_opt: \n16:43:30 |     fixed_candidate_vecs: reuse\n16:43:30 |     fixed_candidates_path: None\n16:43:30 |     force_fp16_tokens: True\n16:43:30 |     fp16: True\n16:43:30 |     fp16_impl: safe\n16:43:30 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-a.txt\n16:43:30 |     fromfile_datatype_extension: False\n16:43:30 |     gpu: -1\n16:43:30 |     gradient_clip: 0.1\n16:43:30 |     hide_labels: False\n16:43:30 |     history_add_global_end_token: None\n16:43:30 |     history_reversed: False\n16:43:30 |     history_size: 20\n16:43:30 |     ignore_bad_candidates: False\n16:43:30 |     ignore_labels: None\n16:43:30 |     image_cropsize: 224\n16:43:30 |     image_mode: raw\n16:43:30 |     image_size: 256\n16:43:30 |     inference: max\n16:43:30 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:43:30 |     init_opt: None\n16:43:30 |     interactive_candidates: fixed\n16:43:30 |     interactive_mode: False\n16:43:30 |     invsqrt_lr_decay_gamma: -1\n16:43:30 |     is_debug: False\n16:43:30 |     label_truncate: 72\n16:43:30 |     learn_embeddings: True\n16:43:30 |     learn_positional_embeddings: True\n16:43:30 |     learningrate: 5e-05\n16:43:30 |     load_from_pretrained_ranker: True\n16:43:30 |     log_every_n_secs: 10.0\n16:43:30 |     log_every_n_steps: 50\n16:43:30 |     log_keep_fields: all\n16:43:30 |     loglevel: info\n16:43:30 |     lr_scheduler: reduceonplateau\n16:43:30 |     lr_scheduler_decay: 0.5\n16:43:30 |     lr_scheduler_patience: 3\n16:43:30 |     max_train_steps: -1\n16:43:30 |     max_train_time: 7200.0\n16:43:30 |     memory_attention: sqrt\n16:43:30 |     metrics: default\n16:43:30 |     model: transformer/classifier\n16:43:30 |     model_file: /tmp/model1\n16:43:30 |     model_parallel: False\n16:43:30 |     momentum: 0\n16:43:30 |     multitask_weights: [1]\n16:43:30 |     mutators: None\n16:43:30 |     n_decoder_layers: -1\n16:43:30 |     n_encoder_layers: -1\n16:43:30 |     n_heads: 12\n16:43:30 |     n_layers: 12\n16:43:30 |     n_positions: 1024\n16:43:30 |     n_segments: 2\n16:43:30 |     nesterov: True\n16:43:30 |     no_cuda: False\n16:43:30 |     normalize_sent_emb: False\n16:43:30 |     num_epochs: -1\n16:43:30 |     num_examples: -1\n16:43:30 |     num_workers: 0\n16:43:30 |     nus: [0.7]\n16:43:30 |     optimizer: adamax\n16:43:30 |     output_scaling: 0.06\n16:43:30 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n16:43:30 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:43:30 |     person_tokens: False\n16:43:30 |     print_scores: False\n16:43:30 |     rank_candidates: False\n16:43:30 |     rank_top_k: -1\n16:43:30 |     reduction_type: mean\n16:43:30 |     ref_class: None\n16:43:30 |     relu_dropout: 0.0\n16:43:30 |     repeat_blocking_heuristic: True\n16:43:30 |     report_filename: \n16:43:30 |     return_cand_scores: False\n16:43:30 |     save_after_valid: True\n16:43:30 |     save_every_n_secs: -1\n16:43:30 |     save_format: conversations\n16:43:30 |     share_encoders: False\n16:43:30 |     share_word_embeddings: False\n16:43:30 |     short_final_eval: False\n16:43:30 |     special_tok_lst: None\n16:43:30 |     split_lines: False\n16:43:30 |     starttime: Dec03_16-41\n16:43:30 |     task: fromfile:parlaiformat\n16:43:30 |     tensorboard_log: False\n16:43:30 |     tensorboard_logdir: None\n16:43:30 |     text_truncate: 360\n16:43:30 |     threshold: 0.5\n16:43:30 |     topk: 5\n16:43:30 |     train_predict: False\n16:43:30 |     truncate: 1024\n16:43:30 |     update_classifier_head_only: False\n16:43:30 |     update_freq: 1\n16:43:30 |     use_memories: False\n16:43:30 |     use_reply: none\n16:43:30 |     validation_cutoff: 1.0\n16:43:30 |     validation_every_n_epochs: -1\n16:43:30 |     validation_every_n_secs: 20.0\n16:43:30 |     validation_every_n_steps: -1\n16:43:30 |     validation_max_exs: -1\n16:43:30 |     validation_metric: accuracy\n16:43:30 |     validation_metric_mode: max\n16:43:30 |     validation_patience: 30\n16:43:30 |     validation_share_agent: False\n16:43:30 |     variant: xlm\n16:43:30 |     verbose: False\n16:43:30 |     wandb_entity: None\n16:43:30 |     wandb_log: False\n16:43:30 |     wandb_name: None\n16:43:30 |     wandb_project: None\n16:43:30 |     warmup_rate: 0.0001\n16:43:30 |     warmup_updates: 1000\n16:43:30 |     weight_decay: None\n16:43:30 |     world_logs: \n16:43:30 |     wrap_memory_encoder: False\n16:43:30 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:43:30 | creating task(s): fromfile:parlaiformat\n16:43:30 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:43:30 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-a.txt\n16:43:36 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2200 2.2e-10               .3097                 .2869   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3365            .1034              .1154   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                 .09375 11.13 525.2 507.5       0          0 38.65  200 .2200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.04 .8345 5.204e-06 241.6 233.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 766.8  741        .2107\u001b[0m\n16:43:36 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2200 2.2e-10               .3097                 .2869   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3365            .1034              .1154   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                 .09375 11.13 525.2 507.5       0          0 38.65  200 .2200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.04 .8345 5.204e-06 241.6 233.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 766.8  741        .2107\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:43:37.912669Z","iopub.execute_input":"2022-12-03T16:43:37.913005Z","iopub.status.idle":"2022-12-03T16:44:04.273105Z","shell.execute_reply.started":"2022-12-03T16:43:37.912973Z","shell.execute_reply":"2022-12-03T16:44:04.271947Z"},"scrolled":true,"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"16:43:45 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_valid.txt)\u001b[0m\n16:43:45 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:43:45 | Using CUDA\n16:43:45 | loading dictionary from /tmp/model1.dict\n16:43:45 | num words = 54944\n16:43:49 | Loading existing model parameters from /tmp/model1\n16:43:55 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:43:56 | Opt:\n16:43:56 |     activation: gelu\n16:43:56 |     adafactor_eps: '[1e-30, 0.001]'\n16:43:56 |     adam_eps: 1e-08\n16:43:56 |     add_p1_after_newln: False\n16:43:56 |     aggregate_micro: False\n16:43:56 |     allow_missing_init_opts: False\n16:43:56 |     area_under_curve_class: None\n16:43:56 |     area_under_curve_digits: -1\n16:43:56 |     attention_dropout: 0.1\n16:43:56 |     batchsize: 40\n16:43:56 |     betas: '[0.9, 0.999]'\n16:43:56 |     bpe_add_prefix_space: None\n16:43:56 |     bpe_debug: False\n16:43:56 |     bpe_dropout: None\n16:43:56 |     bpe_merge: None\n16:43:56 |     bpe_vocab: None\n16:43:56 |     candidates: inline\n16:43:56 |     cap_num_predictions: 100\n16:43:56 |     checkpoint_activations: False\n16:43:56 |     class_weights: None\n16:43:56 |     classes: \"['__notok__', '__ok__']\"\n16:43:56 |     classes_from_file: None\n16:43:56 |     data_parallel: True\n16:43:56 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:43:56 |     datatype: train\n16:43:56 |     delimiter: '\\n'\n16:43:56 |     dict_class: parlai.core.dict:DictionaryAgent\n16:43:56 |     dict_endtoken: __start__\n16:43:56 |     dict_file: /tmp/model1.dict\n16:43:56 |     dict_include_test: False\n16:43:56 |     dict_include_valid: False\n16:43:56 |     dict_initpath: None\n16:43:56 |     dict_language: english\n16:43:56 |     dict_loaded: True\n16:43:56 |     dict_lower: True\n16:43:56 |     dict_max_ngram_size: -1\n16:43:56 |     dict_maxexs: -1\n16:43:56 |     dict_maxtokens: -1\n16:43:56 |     dict_minfreq: 0\n16:43:56 |     dict_nulltoken: __null__\n16:43:56 |     dict_starttoken: __start__\n16:43:56 |     dict_textfields: text,labels\n16:43:56 |     dict_tokenizer: bpe\n16:43:56 |     dict_unktoken: __unk__\n16:43:56 |     display_examples: False\n16:43:56 |     download_path: None\n16:43:56 |     dropout: 0.1\n16:43:56 |     dynamic_batching: None\n16:43:56 |     embedding_projection: random\n16:43:56 |     embedding_size: 768\n16:43:56 |     embedding_type: random\n16:43:56 |     embeddings_scale: False\n16:43:56 |     encode_candidate_vecs: True\n16:43:56 |     encode_candidate_vecs_batchsize: 256\n16:43:56 |     eval_batchsize: None\n16:43:56 |     eval_candidates: inline\n16:43:56 |     eval_dynamic_batching: None\n16:43:56 |     evaltask: None\n16:43:56 |     ffn_size: 3072\n16:43:56 |     final_extra_opt: \n16:43:56 |     fixed_candidate_vecs: reuse\n16:43:56 |     fixed_candidates_path: None\n16:43:56 |     force_fp16_tokens: True\n16:43:56 |     fp16: True\n16:43:56 |     fp16_impl: safe\n16:43:56 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-b.txt\n16:43:56 |     fromfile_datatype_extension: False\n16:43:56 |     gpu: -1\n16:43:56 |     gradient_clip: 0.1\n16:43:56 |     hide_labels: False\n16:43:56 |     history_add_global_end_token: None\n16:43:56 |     history_reversed: False\n16:43:56 |     history_size: 20\n16:43:56 |     ignore_bad_candidates: False\n16:43:56 |     ignore_labels: None\n16:43:56 |     image_cropsize: 224\n16:43:56 |     image_mode: raw\n16:43:56 |     image_size: 256\n16:43:56 |     inference: max\n16:43:56 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:43:56 |     init_opt: None\n16:43:56 |     interactive_candidates: fixed\n16:43:56 |     interactive_mode: False\n16:43:56 |     invsqrt_lr_decay_gamma: -1\n16:43:56 |     is_debug: False\n16:43:56 |     label_truncate: 72\n16:43:56 |     learn_embeddings: True\n16:43:56 |     learn_positional_embeddings: True\n16:43:56 |     learningrate: 5e-05\n16:43:56 |     load_from_pretrained_ranker: True\n16:43:56 |     log_every_n_secs: 10.0\n16:43:56 |     log_every_n_steps: 50\n16:43:56 |     log_keep_fields: all\n16:43:56 |     loglevel: info\n16:43:56 |     lr_scheduler: reduceonplateau\n16:43:56 |     lr_scheduler_decay: 0.5\n16:43:56 |     lr_scheduler_patience: 3\n16:43:56 |     max_train_steps: -1\n16:43:56 |     max_train_time: 7200.0\n16:43:56 |     memory_attention: sqrt\n16:43:56 |     metrics: default\n16:43:56 |     model: transformer/classifier\n16:43:56 |     model_file: /tmp/model1\n16:43:56 |     model_parallel: False\n16:43:56 |     momentum: 0\n16:43:56 |     multitask_weights: [1]\n16:43:56 |     mutators: None\n16:43:56 |     n_decoder_layers: -1\n16:43:56 |     n_encoder_layers: -1\n16:43:56 |     n_heads: 12\n16:43:56 |     n_layers: 12\n16:43:56 |     n_positions: 1024\n16:43:56 |     n_segments: 2\n16:43:56 |     nesterov: True\n16:43:56 |     no_cuda: False\n16:43:56 |     normalize_sent_emb: False\n16:43:56 |     num_epochs: -1\n16:43:56 |     num_examples: -1\n16:43:56 |     num_workers: 0\n16:43:56 |     nus: [0.7]\n16:43:56 |     optimizer: adamax\n16:43:56 |     output_scaling: 0.06\n16:43:56 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n16:43:56 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:43:56 |     person_tokens: False\n16:43:56 |     print_scores: False\n16:43:56 |     rank_candidates: False\n16:43:56 |     rank_top_k: -1\n16:43:56 |     reduction_type: mean\n16:43:56 |     ref_class: None\n16:43:56 |     relu_dropout: 0.0\n16:43:56 |     repeat_blocking_heuristic: True\n16:43:56 |     report_filename: \n16:43:56 |     return_cand_scores: False\n16:43:56 |     save_after_valid: True\n16:43:56 |     save_every_n_secs: -1\n16:43:56 |     save_format: conversations\n16:43:56 |     share_encoders: False\n16:43:56 |     share_word_embeddings: False\n16:43:56 |     short_final_eval: False\n16:43:56 |     special_tok_lst: None\n16:43:56 |     split_lines: False\n16:43:56 |     starttime: Dec03_16-41\n16:43:56 |     task: fromfile:parlaiformat\n16:43:56 |     tensorboard_log: False\n16:43:56 |     tensorboard_logdir: None\n16:43:56 |     text_truncate: 360\n16:43:56 |     threshold: 0.5\n16:43:56 |     topk: 5\n16:43:56 |     train_predict: False\n16:43:56 |     truncate: 1024\n16:43:56 |     update_classifier_head_only: False\n16:43:56 |     update_freq: 1\n16:43:56 |     use_memories: False\n16:43:56 |     use_reply: none\n16:43:56 |     validation_cutoff: 1.0\n16:43:56 |     validation_every_n_epochs: -1\n16:43:56 |     validation_every_n_secs: 20.0\n16:43:56 |     validation_every_n_steps: -1\n16:43:56 |     validation_max_exs: -1\n16:43:56 |     validation_metric: accuracy\n16:43:56 |     validation_metric_mode: max\n16:43:56 |     validation_patience: 30\n16:43:56 |     validation_share_agent: False\n16:43:56 |     variant: xlm\n16:43:56 |     verbose: False\n16:43:56 |     wandb_entity: None\n16:43:56 |     wandb_log: False\n16:43:56 |     wandb_name: None\n16:43:56 |     wandb_project: None\n16:43:56 |     warmup_rate: 0.0001\n16:43:56 |     warmup_updates: 1000\n16:43:56 |     weight_decay: None\n16:43:56 |     world_logs: \n16:43:56 |     wrap_memory_encoder: False\n16:43:56 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:43:56 | creating task(s): fromfile:parlaiformat\n16:43:56 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:43:56 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run1/data_train-b.txt\n16:44:02 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7800 7.8e-10               .7982                 .7131   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9062            .7582              .8846   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6635 11.13 525.2   479       0          0 36.48  200 .7800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.96 .5972 5.204e-06 238.4 217.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 763.6 696.5        .7774\u001b[0m\n16:44:02 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7800 7.8e-10               .7982                 .7131   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9062            .7582              .8846   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6635 11.13 525.2   479       0          0 36.48  200 .7800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.96 .5972 5.204e-06 238.4 217.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 763.6 696.5        .7774\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:44:04.275162Z","iopub.execute_input":"2022-12-03T16:44:04.275566Z","iopub.status.idle":"2022-12-03T16:44:05.426754Z","shell.execute_reply.started":"2022-12-03T16:44:04.275528Z","shell.execute_reply":"2022-12-03T16:44:05.425456Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:44:05.428781Z","iopub.execute_input":"2022-12-03T16:44:05.429186Z","iopub.status.idle":"2022-12-03T16:45:27.513896Z","shell.execute_reply.started":"2022-12-03T16:44:05.429152Z","shell.execute_reply":"2022-12-03T16:45:27.512729Z"},"scrolled":true,"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"16:44:12 | building dictionary first...\n16:44:12 | No model with opt yet at: /tmp/model2(.opt)\n16:44:12 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:44:12 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:44:12 | Using CUDA\n16:44:12 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:44:12 | num words = 54944\n16:44:17 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:44:23 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:44:23 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:44:23 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:44:23 | Opt:\n16:44:23 |     activation: gelu\n16:44:23 |     adafactor_eps: '(1e-30, 0.001)'\n16:44:23 |     adam_eps: 1e-08\n16:44:23 |     add_p1_after_newln: False\n16:44:23 |     aggregate_micro: False\n16:44:23 |     allow_missing_init_opts: False\n16:44:23 |     attention_dropout: 0.1\n16:44:23 |     batchsize: 20\n16:44:23 |     betas: '(0.9, 0.999)'\n16:44:23 |     bpe_add_prefix_space: None\n16:44:23 |     bpe_debug: False\n16:44:23 |     bpe_dropout: None\n16:44:23 |     bpe_merge: None\n16:44:23 |     bpe_vocab: None\n16:44:23 |     candidates: inline\n16:44:23 |     cap_num_predictions: 100\n16:44:23 |     checkpoint_activations: False\n16:44:23 |     class_weights: None\n16:44:23 |     classes: \"['__notok__', '__ok__']\"\n16:44:23 |     classes_from_file: None\n16:44:23 |     data_parallel: True\n16:44:23 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:44:23 |     datatype: train\n16:44:23 |     delimiter: '\\n'\n16:44:23 |     dict_class: parlai.core.dict:DictionaryAgent\n16:44:23 |     dict_endtoken: __start__\n16:44:23 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:44:23 |     dict_include_test: False\n16:44:23 |     dict_include_valid: False\n16:44:23 |     dict_initpath: None\n16:44:23 |     dict_language: english\n16:44:23 |     dict_loaded: True\n16:44:23 |     dict_lower: True\n16:44:23 |     dict_max_ngram_size: -1\n16:44:23 |     dict_maxexs: -1\n16:44:23 |     dict_maxtokens: -1\n16:44:23 |     dict_minfreq: 0\n16:44:23 |     dict_nulltoken: __null__\n16:44:23 |     dict_starttoken: __start__\n16:44:23 |     dict_textfields: text,labels\n16:44:23 |     dict_tokenizer: bpe\n16:44:23 |     dict_unktoken: __unk__\n16:44:23 |     display_examples: False\n16:44:23 |     download_path: None\n16:44:23 |     dropout: 0.1\n16:44:23 |     dynamic_batching: None\n16:44:23 |     embedding_projection: random\n16:44:23 |     embedding_size: 768\n16:44:23 |     embedding_type: random\n16:44:23 |     embeddings_scale: False\n16:44:23 |     encode_candidate_vecs: True\n16:44:23 |     encode_candidate_vecs_batchsize: 256\n16:44:23 |     eval_batchsize: None\n16:44:23 |     eval_candidates: inline\n16:44:23 |     eval_dynamic_batching: None\n16:44:23 |     evaltask: None\n16:44:23 |     ffn_size: 3072\n16:44:23 |     final_extra_opt: \n16:44:23 |     fixed_candidate_vecs: reuse\n16:44:23 |     fixed_candidates_path: None\n16:44:23 |     force_fp16_tokens: False\n16:44:23 |     fp16: True\n16:44:23 |     fp16_impl: safe\n16:44:23 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt\n16:44:23 |     fromfile_datatype_extension: False\n16:44:23 |     gpu: -1\n16:44:23 |     gradient_clip: 0.1\n16:44:23 |     hide_labels: False\n16:44:23 |     history_add_global_end_token: None\n16:44:23 |     history_reversed: False\n16:44:23 |     history_size: 20\n16:44:23 |     ignore_bad_candidates: False\n16:44:23 |     ignore_labels: None\n16:44:23 |     image_cropsize: 224\n16:44:23 |     image_mode: raw\n16:44:23 |     image_size: 256\n16:44:23 |     inference: max\n16:44:23 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:44:23 |     init_opt: None\n16:44:23 |     interactive_candidates: fixed\n16:44:23 |     interactive_mode: False\n16:44:23 |     invsqrt_lr_decay_gamma: -1\n16:44:23 |     is_debug: False\n16:44:23 |     label_truncate: 72\n16:44:23 |     learn_embeddings: True\n16:44:23 |     learn_positional_embeddings: True\n16:44:23 |     learningrate: 5e-05\n16:44:23 |     load_from_checkpoint: False\n16:44:23 |     load_from_pretrained_ranker: True\n16:44:23 |     log_every_n_secs: 10.0\n16:44:23 |     log_every_n_steps: 50\n16:44:23 |     log_keep_fields: all\n16:44:23 |     loglevel: info\n16:44:23 |     lr_scheduler: reduceonplateau\n16:44:23 |     lr_scheduler_decay: 0.5\n16:44:23 |     lr_scheduler_patience: 3\n16:44:23 |     max_train_steps: -1\n16:44:23 |     max_train_time: 7200.0\n16:44:23 |     memory_attention: sqrt\n16:44:23 |     metrics: default\n16:44:23 |     model: transformer/classifier\n16:44:23 |     model_file: /tmp/model2\n16:44:23 |     model_parallel: False\n16:44:23 |     momentum: 0\n16:44:23 |     multitask_weights: [1]\n16:44:23 |     mutators: None\n16:44:23 |     n_decoder_layers: -1\n16:44:23 |     n_encoder_layers: -1\n16:44:23 |     n_heads: 12\n16:44:23 |     n_layers: 12\n16:44:23 |     n_positions: 1024\n16:44:23 |     n_segments: 2\n16:44:23 |     nesterov: True\n16:44:23 |     no_cuda: False\n16:44:23 |     normalize_sent_emb: False\n16:44:23 |     num_epochs: -1\n16:44:23 |     num_workers: 0\n16:44:23 |     nus: (0.7,)\n16:44:23 |     optimizer: adamax\n16:44:23 |     output_scaling: 0.06\n16:44:23 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n16:44:23 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:44:23 |     person_tokens: False\n16:44:23 |     print_scores: False\n16:44:23 |     rank_candidates: False\n16:44:23 |     rank_top_k: -1\n16:44:23 |     reduction_type: mean\n16:44:23 |     ref_class: None\n16:44:23 |     relu_dropout: 0.0\n16:44:23 |     repeat_blocking_heuristic: True\n16:44:23 |     return_cand_scores: False\n16:44:23 |     save_after_valid: True\n16:44:23 |     save_every_n_secs: -1\n16:44:23 |     save_format: conversations\n16:44:23 |     share_encoders: False\n16:44:23 |     share_word_embeddings: False\n16:44:23 |     short_final_eval: False\n16:44:23 |     special_tok_lst: None\n16:44:23 |     split_lines: False\n16:44:23 |     starttime: Dec03_16-44\n16:44:23 |     task: fromfile:parlaiformat\n16:44:23 |     tensorboard_log: False\n16:44:23 |     tensorboard_logdir: None\n16:44:23 |     text_truncate: 360\n16:44:23 |     threshold: 0.5\n16:44:23 |     topk: 5\n16:44:23 |     train_predict: False\n16:44:23 |     truncate: 1024\n16:44:23 |     update_classifier_head_only: False\n16:44:23 |     update_freq: 1\n16:44:23 |     use_memories: False\n16:44:23 |     use_reply: none\n16:44:23 |     validation_cutoff: 1.0\n16:44:23 |     validation_every_n_epochs: -1\n16:44:23 |     validation_every_n_secs: 20.0\n16:44:23 |     validation_every_n_steps: -1\n16:44:23 |     validation_max_exs: -1\n16:44:23 |     validation_metric: accuracy\n16:44:23 |     validation_metric_mode: max\n16:44:23 |     validation_patience: 30\n16:44:23 |     validation_share_agent: False\n16:44:23 |     variant: xlm\n16:44:23 |     verbose: False\n16:44:23 |     wandb_entity: None\n16:44:23 |     wandb_log: False\n16:44:23 |     wandb_name: None\n16:44:23 |     wandb_project: None\n16:44:23 |     warmup_rate: 0.0001\n16:44:23 |     warmup_updates: 1000\n16:44:23 |     weight_decay: None\n16:44:23 |     world_logs: \n16:44:23 |     wrap_memory_encoder: False\n16:44:23 | creating task(s): fromfile:parlaiformat\n16:44:23 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt\n16:44:23 | training...\n16:44:33 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4952 4.952e-10               .5411                 .4789   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6219            .4392              .5220   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .3790 11.14     1 262.7 546.5       0          0  41.6  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4952             32768  2.908    .1206 5.957 .6990 1.055e-06 119.1 247.8   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 381.9 794.3 2.085        .4880\n\n16:44:43 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7539 7.539e-10               .7530                 .7620   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7441            .7549              .7461   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7639 11.06     1 261.2  1016       0          0 77.82  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7539             32768  2.726    .1207 6.008 .6496 2.955e-06 120.2 467.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                   59 381.3 1484  3.9        .7539\n\n16:44:43 | creating task(s): fromfile:parlaiformat\n16:44:43 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:44:43 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt\n16:44:43 | running eval: valid\n16:44:43 | eval completed in 0.20s\n16:44:43 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9565                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9167            .9600              .9231   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                      1    11   156  1747       0          0 134.3   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5795 2.955e-06    72   806       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  228 2553        .9583\n\u001b[0m\n16:44:43 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n16:44:43 | saving best valid model: /tmp/model2\n16:44:43 | Saving dictionary to /tmp/model2.dict\n16:44:48 | saving model checkpoint: /tmp/model2.checkpoint\n16:44:48 | Saving dictionary to /tmp/model2.checkpoint.dict\n16:45:06 | time:43s total_exs:1940 total_steps:97 epochs:80.83\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9895 9.895e-10               .9894                 .9920   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9867            .9896              .9870   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9922 10.94     1 258.8   970       0          0 74.97  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9895             32768  2.727    .1207 5.992 .5108 4.855e-06 119.8 449.2   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   97 378.6 1419 3.757        .9895\n\n16:45:08 | time:45s total_exs:2140 total_steps:107 epochs:89.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.54     1 270.9  1067       0          0 78.79  200   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.688    .1207  5.95 .3740 5.354e-06   119 468.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  107 389.9 1536 3.975            1\n\n16:45:08 | running eval: valid\n16:45:09 | eval completed in 0.20s\n16:45:09 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1712       0          0 131.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0809     6 .3030 5.354e-06    72 790.1       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    107  228 2503            1\n\u001b[0m\n16:45:09 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n16:45:09 | saving best valid model: /tmp/model2\n16:45:17 | task solved! stopping.\n16:45:17 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:45:17 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:45:17 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:45:17 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:45:17 | Using CUDA\n16:45:17 | loading dictionary from /tmp/model2.dict\n16:45:17 | num words = 54944\n16:45:22 | Loading existing model parameters from /tmp/model2\n16:45:23 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:45:25 | creating task(s): fromfile:parlaiformat\n16:45:25 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:45:25 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt\n16:45:25 | running eval: valid\n16:45:25 | eval completed in 0.21s\n16:45:25 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1654       0          0 127.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3030 5.354e-06    72 763.1       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    107  228 2417            1\n\u001b[0m\n16:45:25 | creating task(s): fromfile:parlaiformat\n16:45:25 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:45:25 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt\n16:45:25 | running eval: test\n16:45:25 | eval completed in 0.22s\n16:45:25 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1515       0          0 116.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3030 5.354e-06    72 699.1       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    107  228 2214            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:45:27.517029Z","iopub.execute_input":"2022-12-03T16:45:27.517447Z","iopub.status.idle":"2022-12-03T16:45:55.581759Z","shell.execute_reply.started":"2022-12-03T16:45:27.517406Z","shell.execute_reply":"2022-12-03T16:45:55.580546Z"},"scrolled":true,"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"16:45:34 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt)\u001b[0m\n16:45:34 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:45:34 | Using CUDA\n16:45:34 | loading dictionary from /tmp/model2.dict\n16:45:34 | num words = 54944\n16:45:39 | Loading existing model parameters from /tmp/model2\n16:45:46 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:45:47 | Opt:\n16:45:47 |     activation: gelu\n16:45:47 |     adafactor_eps: '[1e-30, 0.001]'\n16:45:47 |     adam_eps: 1e-08\n16:45:47 |     add_p1_after_newln: False\n16:45:47 |     aggregate_micro: False\n16:45:47 |     allow_missing_init_opts: False\n16:45:47 |     area_under_curve_class: None\n16:45:47 |     area_under_curve_digits: -1\n16:45:47 |     attention_dropout: 0.1\n16:45:47 |     batchsize: 40\n16:45:47 |     betas: '[0.9, 0.999]'\n16:45:47 |     bpe_add_prefix_space: None\n16:45:47 |     bpe_debug: False\n16:45:47 |     bpe_dropout: None\n16:45:47 |     bpe_merge: None\n16:45:47 |     bpe_vocab: None\n16:45:47 |     candidates: inline\n16:45:47 |     cap_num_predictions: 100\n16:45:47 |     checkpoint_activations: False\n16:45:47 |     class_weights: None\n16:45:47 |     classes: \"['__notok__', '__ok__']\"\n16:45:47 |     classes_from_file: None\n16:45:47 |     data_parallel: True\n16:45:47 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:45:47 |     datatype: train\n16:45:47 |     delimiter: '\\n'\n16:45:47 |     dict_class: parlai.core.dict:DictionaryAgent\n16:45:47 |     dict_endtoken: __start__\n16:45:47 |     dict_file: /tmp/model2.dict\n16:45:47 |     dict_include_test: False\n16:45:47 |     dict_include_valid: False\n16:45:47 |     dict_initpath: None\n16:45:47 |     dict_language: english\n16:45:47 |     dict_loaded: True\n16:45:47 |     dict_lower: True\n16:45:47 |     dict_max_ngram_size: -1\n16:45:47 |     dict_maxexs: -1\n16:45:47 |     dict_maxtokens: -1\n16:45:47 |     dict_minfreq: 0\n16:45:47 |     dict_nulltoken: __null__\n16:45:47 |     dict_starttoken: __start__\n16:45:47 |     dict_textfields: text,labels\n16:45:47 |     dict_tokenizer: bpe\n16:45:47 |     dict_unktoken: __unk__\n16:45:47 |     display_examples: False\n16:45:47 |     download_path: None\n16:45:47 |     dropout: 0.1\n16:45:47 |     dynamic_batching: None\n16:45:47 |     embedding_projection: random\n16:45:47 |     embedding_size: 768\n16:45:47 |     embedding_type: random\n16:45:47 |     embeddings_scale: False\n16:45:47 |     encode_candidate_vecs: True\n16:45:47 |     encode_candidate_vecs_batchsize: 256\n16:45:47 |     eval_batchsize: None\n16:45:47 |     eval_candidates: inline\n16:45:47 |     eval_dynamic_batching: None\n16:45:47 |     evaltask: None\n16:45:47 |     ffn_size: 3072\n16:45:47 |     final_extra_opt: \n16:45:47 |     fixed_candidate_vecs: reuse\n16:45:47 |     fixed_candidates_path: None\n16:45:47 |     force_fp16_tokens: True\n16:45:47 |     fp16: True\n16:45:47 |     fp16_impl: safe\n16:45:47 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-a.txt\n16:45:47 |     fromfile_datatype_extension: False\n16:45:47 |     gpu: -1\n16:45:47 |     gradient_clip: 0.1\n16:45:47 |     hide_labels: False\n16:45:47 |     history_add_global_end_token: None\n16:45:47 |     history_reversed: False\n16:45:47 |     history_size: 20\n16:45:47 |     ignore_bad_candidates: False\n16:45:47 |     ignore_labels: None\n16:45:47 |     image_cropsize: 224\n16:45:47 |     image_mode: raw\n16:45:47 |     image_size: 256\n16:45:47 |     inference: max\n16:45:47 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:45:47 |     init_opt: None\n16:45:47 |     interactive_candidates: fixed\n16:45:47 |     interactive_mode: False\n16:45:47 |     invsqrt_lr_decay_gamma: -1\n16:45:47 |     is_debug: False\n16:45:47 |     label_truncate: 72\n16:45:47 |     learn_embeddings: True\n16:45:47 |     learn_positional_embeddings: True\n16:45:47 |     learningrate: 5e-05\n16:45:47 |     load_from_pretrained_ranker: True\n16:45:47 |     log_every_n_secs: 10.0\n16:45:47 |     log_every_n_steps: 50\n16:45:47 |     log_keep_fields: all\n16:45:47 |     loglevel: info\n16:45:47 |     lr_scheduler: reduceonplateau\n16:45:47 |     lr_scheduler_decay: 0.5\n16:45:47 |     lr_scheduler_patience: 3\n16:45:47 |     max_train_steps: -1\n16:45:47 |     max_train_time: 7200.0\n16:45:47 |     memory_attention: sqrt\n16:45:47 |     metrics: default\n16:45:47 |     model: transformer/classifier\n16:45:47 |     model_file: /tmp/model2\n16:45:47 |     model_parallel: False\n16:45:47 |     momentum: 0\n16:45:47 |     multitask_weights: [1]\n16:45:47 |     mutators: None\n16:45:47 |     n_decoder_layers: -1\n16:45:47 |     n_encoder_layers: -1\n16:45:47 |     n_heads: 12\n16:45:47 |     n_layers: 12\n16:45:47 |     n_positions: 1024\n16:45:47 |     n_segments: 2\n16:45:47 |     nesterov: True\n16:45:47 |     no_cuda: False\n16:45:47 |     normalize_sent_emb: False\n16:45:47 |     num_epochs: -1\n16:45:47 |     num_examples: -1\n16:45:47 |     num_workers: 0\n16:45:47 |     nus: [0.7]\n16:45:47 |     optimizer: adamax\n16:45:47 |     output_scaling: 0.06\n16:45:47 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n16:45:47 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:45:47 |     person_tokens: False\n16:45:47 |     print_scores: False\n16:45:47 |     rank_candidates: False\n16:45:47 |     rank_top_k: -1\n16:45:47 |     reduction_type: mean\n16:45:47 |     ref_class: None\n16:45:47 |     relu_dropout: 0.0\n16:45:47 |     repeat_blocking_heuristic: True\n16:45:47 |     report_filename: \n16:45:47 |     return_cand_scores: False\n16:45:47 |     save_after_valid: True\n16:45:47 |     save_every_n_secs: -1\n16:45:47 |     save_format: conversations\n16:45:47 |     share_encoders: False\n16:45:47 |     share_word_embeddings: False\n16:45:47 |     short_final_eval: False\n16:45:47 |     special_tok_lst: None\n16:45:47 |     split_lines: False\n16:45:47 |     starttime: Dec03_16-44\n16:45:47 |     task: fromfile:parlaiformat\n16:45:47 |     tensorboard_log: False\n16:45:47 |     tensorboard_logdir: None\n16:45:47 |     text_truncate: 360\n16:45:47 |     threshold: 0.5\n16:45:47 |     topk: 5\n16:45:47 |     train_predict: False\n16:45:47 |     truncate: 1024\n16:45:47 |     update_classifier_head_only: False\n16:45:47 |     update_freq: 1\n16:45:47 |     use_memories: False\n16:45:47 |     use_reply: none\n16:45:47 |     validation_cutoff: 1.0\n16:45:47 |     validation_every_n_epochs: -1\n16:45:47 |     validation_every_n_secs: 20.0\n16:45:47 |     validation_every_n_steps: -1\n16:45:47 |     validation_max_exs: -1\n16:45:47 |     validation_metric: accuracy\n16:45:47 |     validation_metric_mode: max\n16:45:47 |     validation_patience: 30\n16:45:47 |     validation_share_agent: False\n16:45:47 |     variant: xlm\n16:45:47 |     verbose: False\n16:45:47 |     wandb_entity: None\n16:45:47 |     wandb_log: False\n16:45:47 |     wandb_name: None\n16:45:47 |     wandb_project: None\n16:45:47 |     warmup_rate: 0.0001\n16:45:47 |     warmup_updates: 1000\n16:45:47 |     weight_decay: None\n16:45:47 |     world_logs: \n16:45:47 |     wrap_memory_encoder: False\n16:45:48 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:45:48 | creating task(s): fromfile:parlaiformat\n16:45:48 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:45:48 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-a.txt\n16:45:54 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2450 2.45e-10               .1564                 .1750   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1414            .3167              .2917   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3465  11.7 547.8 490.5       0          0 35.82  200 .2450   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .8415 5.354e-06 239.6 214.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 787.4 705.1        .2374\u001b[0m\n16:45:54 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2450 2.45e-10               .1564                 .1750   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1414            .3167              .2917   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3465  11.7 547.8 490.5       0          0 35.82  200 .2450   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .8415 5.354e-06 239.6 214.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 787.4 705.1        .2374\n","output_type":"stream"}]},{"cell_type":"code","source":"#  evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:45:55.583780Z","iopub.execute_input":"2022-12-03T16:45:55.584149Z","iopub.status.idle":"2022-12-03T16:46:21.537137Z","shell.execute_reply.started":"2022-12-03T16:45:55.584111Z","shell.execute_reply":"2022-12-03T16:46:21.535644Z"},"scrolled":true,"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"16:46:02 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_valid.txt)\u001b[0m\n16:46:02 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:46:02 | Using CUDA\n16:46:02 | loading dictionary from /tmp/model2.dict\n16:46:02 | num words = 54944\n16:46:07 | Loading existing model parameters from /tmp/model2\n16:46:12 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:46:14 | Opt:\n16:46:14 |     activation: gelu\n16:46:14 |     adafactor_eps: '[1e-30, 0.001]'\n16:46:14 |     adam_eps: 1e-08\n16:46:14 |     add_p1_after_newln: False\n16:46:14 |     aggregate_micro: False\n16:46:14 |     allow_missing_init_opts: False\n16:46:14 |     area_under_curve_class: None\n16:46:14 |     area_under_curve_digits: -1\n16:46:14 |     attention_dropout: 0.1\n16:46:14 |     batchsize: 40\n16:46:14 |     betas: '[0.9, 0.999]'\n16:46:14 |     bpe_add_prefix_space: None\n16:46:14 |     bpe_debug: False\n16:46:14 |     bpe_dropout: None\n16:46:14 |     bpe_merge: None\n16:46:14 |     bpe_vocab: None\n16:46:14 |     candidates: inline\n16:46:14 |     cap_num_predictions: 100\n16:46:14 |     checkpoint_activations: False\n16:46:14 |     class_weights: None\n16:46:14 |     classes: \"['__notok__', '__ok__']\"\n16:46:14 |     classes_from_file: None\n16:46:14 |     data_parallel: True\n16:46:14 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:46:14 |     datatype: train\n16:46:14 |     delimiter: '\\n'\n16:46:14 |     dict_class: parlai.core.dict:DictionaryAgent\n16:46:14 |     dict_endtoken: __start__\n16:46:14 |     dict_file: /tmp/model2.dict\n16:46:14 |     dict_include_test: False\n16:46:14 |     dict_include_valid: False\n16:46:14 |     dict_initpath: None\n16:46:14 |     dict_language: english\n16:46:14 |     dict_loaded: True\n16:46:14 |     dict_lower: True\n16:46:14 |     dict_max_ngram_size: -1\n16:46:14 |     dict_maxexs: -1\n16:46:14 |     dict_maxtokens: -1\n16:46:14 |     dict_minfreq: 0\n16:46:14 |     dict_nulltoken: __null__\n16:46:14 |     dict_starttoken: __start__\n16:46:14 |     dict_textfields: text,labels\n16:46:14 |     dict_tokenizer: bpe\n16:46:14 |     dict_unktoken: __unk__\n16:46:14 |     display_examples: False\n16:46:14 |     download_path: None\n16:46:14 |     dropout: 0.1\n16:46:14 |     dynamic_batching: None\n16:46:14 |     embedding_projection: random\n16:46:14 |     embedding_size: 768\n16:46:14 |     embedding_type: random\n16:46:14 |     embeddings_scale: False\n16:46:14 |     encode_candidate_vecs: True\n16:46:14 |     encode_candidate_vecs_batchsize: 256\n16:46:14 |     eval_batchsize: None\n16:46:14 |     eval_candidates: inline\n16:46:14 |     eval_dynamic_batching: None\n16:46:14 |     evaltask: None\n16:46:14 |     ffn_size: 3072\n16:46:14 |     final_extra_opt: \n16:46:14 |     fixed_candidate_vecs: reuse\n16:46:14 |     fixed_candidates_path: None\n16:46:14 |     force_fp16_tokens: True\n16:46:14 |     fp16: True\n16:46:14 |     fp16_impl: safe\n16:46:14 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-b.txt\n16:46:14 |     fromfile_datatype_extension: False\n16:46:14 |     gpu: -1\n16:46:14 |     gradient_clip: 0.1\n16:46:14 |     hide_labels: False\n16:46:14 |     history_add_global_end_token: None\n16:46:14 |     history_reversed: False\n16:46:14 |     history_size: 20\n16:46:14 |     ignore_bad_candidates: False\n16:46:14 |     ignore_labels: None\n16:46:14 |     image_cropsize: 224\n16:46:14 |     image_mode: raw\n16:46:14 |     image_size: 256\n16:46:14 |     inference: max\n16:46:14 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:46:14 |     init_opt: None\n16:46:14 |     interactive_candidates: fixed\n16:46:14 |     interactive_mode: False\n16:46:14 |     invsqrt_lr_decay_gamma: -1\n16:46:14 |     is_debug: False\n16:46:14 |     label_truncate: 72\n16:46:14 |     learn_embeddings: True\n16:46:14 |     learn_positional_embeddings: True\n16:46:14 |     learningrate: 5e-05\n16:46:14 |     load_from_pretrained_ranker: True\n16:46:14 |     log_every_n_secs: 10.0\n16:46:14 |     log_every_n_steps: 50\n16:46:14 |     log_keep_fields: all\n16:46:14 |     loglevel: info\n16:46:14 |     lr_scheduler: reduceonplateau\n16:46:14 |     lr_scheduler_decay: 0.5\n16:46:14 |     lr_scheduler_patience: 3\n16:46:14 |     max_train_steps: -1\n16:46:14 |     max_train_time: 7200.0\n16:46:14 |     memory_attention: sqrt\n16:46:14 |     metrics: default\n16:46:14 |     model: transformer/classifier\n16:46:14 |     model_file: /tmp/model2\n16:46:14 |     model_parallel: False\n16:46:14 |     momentum: 0\n16:46:14 |     multitask_weights: [1]\n16:46:14 |     mutators: None\n16:46:14 |     n_decoder_layers: -1\n16:46:14 |     n_encoder_layers: -1\n16:46:14 |     n_heads: 12\n16:46:14 |     n_layers: 12\n16:46:14 |     n_positions: 1024\n16:46:14 |     n_segments: 2\n16:46:14 |     nesterov: True\n16:46:14 |     no_cuda: False\n16:46:14 |     normalize_sent_emb: False\n16:46:14 |     num_epochs: -1\n16:46:14 |     num_examples: -1\n16:46:14 |     num_workers: 0\n16:46:14 |     nus: [0.7]\n16:46:14 |     optimizer: adamax\n16:46:14 |     output_scaling: 0.06\n16:46:14 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n16:46:14 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:46:14 |     person_tokens: False\n16:46:14 |     print_scores: False\n16:46:14 |     rank_candidates: False\n16:46:14 |     rank_top_k: -1\n16:46:14 |     reduction_type: mean\n16:46:14 |     ref_class: None\n16:46:14 |     relu_dropout: 0.0\n16:46:14 |     repeat_blocking_heuristic: True\n16:46:14 |     report_filename: \n16:46:14 |     return_cand_scores: False\n16:46:14 |     save_after_valid: True\n16:46:14 |     save_every_n_secs: -1\n16:46:14 |     save_format: conversations\n16:46:14 |     share_encoders: False\n16:46:14 |     share_word_embeddings: False\n16:46:14 |     short_final_eval: False\n16:46:14 |     special_tok_lst: None\n16:46:14 |     split_lines: False\n16:46:14 |     starttime: Dec03_16-44\n16:46:14 |     task: fromfile:parlaiformat\n16:46:14 |     tensorboard_log: False\n16:46:14 |     tensorboard_logdir: None\n16:46:14 |     text_truncate: 360\n16:46:14 |     threshold: 0.5\n16:46:14 |     topk: 5\n16:46:14 |     train_predict: False\n16:46:14 |     truncate: 1024\n16:46:14 |     update_classifier_head_only: False\n16:46:14 |     update_freq: 1\n16:46:14 |     use_memories: False\n16:46:14 |     use_reply: none\n16:46:14 |     validation_cutoff: 1.0\n16:46:14 |     validation_every_n_epochs: -1\n16:46:14 |     validation_every_n_secs: 20.0\n16:46:14 |     validation_every_n_steps: -1\n16:46:14 |     validation_max_exs: -1\n16:46:14 |     validation_metric: accuracy\n16:46:14 |     validation_metric_mode: max\n16:46:14 |     validation_patience: 30\n16:46:14 |     validation_share_agent: False\n16:46:14 |     variant: xlm\n16:46:14 |     verbose: False\n16:46:14 |     wandb_entity: None\n16:46:14 |     wandb_log: False\n16:46:14 |     wandb_name: None\n16:46:14 |     wandb_project: None\n16:46:14 |     warmup_rate: 0.0001\n16:46:14 |     warmup_updates: 1000\n16:46:14 |     weight_decay: None\n16:46:14 |     world_logs: \n16:46:14 |     wrap_memory_encoder: False\n16:46:14 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:46:14 | creating task(s): fromfile:parlaiformat\n16:46:14 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:46:14 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run2/data_train-b.txt\n16:46:19 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7550 7.55e-10               .7293                 .8250   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6535            .7763              .7083   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8586  11.7 547.8 525.8       0          0 38.39  200 .7550   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5999 5.354e-06 240.4 230.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 788.2 756.6        .7525\u001b[0m\n16:46:19 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7550 7.55e-10               .7293                 .8250   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6535            .7763              .7083   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8586  11.7 547.8 525.8       0          0 38.39  200 .7550   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5999 5.354e-06 240.4 230.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 788.2 756.6        .7525\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:46:21.540831Z","iopub.execute_input":"2022-12-03T16:46:21.541145Z","iopub.status.idle":"2022-12-03T16:46:23.327133Z","shell.execute_reply.started":"2022-12-03T16:46:21.541115Z","shell.execute_reply":"2022-12-03T16:46:23.325772Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:46:23.331053Z","iopub.execute_input":"2022-12-03T16:46:23.331390Z","iopub.status.idle":"2022-12-03T16:47:44.714201Z","shell.execute_reply.started":"2022-12-03T16:46:23.331353Z","shell.execute_reply":"2022-12-03T16:47:44.713030Z"},"scrolled":true,"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"16:46:30 | building dictionary first...\n16:46:30 | No model with opt yet at: /tmp/model3(.opt)\n16:46:30 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:46:30 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:46:30 | Using CUDA\n16:46:30 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:46:30 | num words = 54944\n16:46:35 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:46:40 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:46:40 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:46:40 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:46:40 | Opt:\n16:46:40 |     activation: gelu\n16:46:40 |     adafactor_eps: '(1e-30, 0.001)'\n16:46:40 |     adam_eps: 1e-08\n16:46:40 |     add_p1_after_newln: False\n16:46:40 |     aggregate_micro: False\n16:46:40 |     allow_missing_init_opts: False\n16:46:40 |     attention_dropout: 0.1\n16:46:40 |     batchsize: 20\n16:46:40 |     betas: '(0.9, 0.999)'\n16:46:40 |     bpe_add_prefix_space: None\n16:46:40 |     bpe_debug: False\n16:46:40 |     bpe_dropout: None\n16:46:40 |     bpe_merge: None\n16:46:40 |     bpe_vocab: None\n16:46:40 |     candidates: inline\n16:46:40 |     cap_num_predictions: 100\n16:46:40 |     checkpoint_activations: False\n16:46:40 |     class_weights: None\n16:46:40 |     classes: \"['__notok__', '__ok__']\"\n16:46:40 |     classes_from_file: None\n16:46:40 |     data_parallel: True\n16:46:40 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:46:40 |     datatype: train\n16:46:40 |     delimiter: '\\n'\n16:46:40 |     dict_class: parlai.core.dict:DictionaryAgent\n16:46:40 |     dict_endtoken: __start__\n16:46:40 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:46:40 |     dict_include_test: False\n16:46:40 |     dict_include_valid: False\n16:46:40 |     dict_initpath: None\n16:46:40 |     dict_language: english\n16:46:40 |     dict_loaded: True\n16:46:40 |     dict_lower: True\n16:46:40 |     dict_max_ngram_size: -1\n16:46:40 |     dict_maxexs: -1\n16:46:40 |     dict_maxtokens: -1\n16:46:40 |     dict_minfreq: 0\n16:46:40 |     dict_nulltoken: __null__\n16:46:40 |     dict_starttoken: __start__\n16:46:40 |     dict_textfields: text,labels\n16:46:40 |     dict_tokenizer: bpe\n16:46:40 |     dict_unktoken: __unk__\n16:46:40 |     display_examples: False\n16:46:40 |     download_path: None\n16:46:40 |     dropout: 0.1\n16:46:40 |     dynamic_batching: None\n16:46:40 |     embedding_projection: random\n16:46:40 |     embedding_size: 768\n16:46:40 |     embedding_type: random\n16:46:40 |     embeddings_scale: False\n16:46:40 |     encode_candidate_vecs: True\n16:46:40 |     encode_candidate_vecs_batchsize: 256\n16:46:40 |     eval_batchsize: None\n16:46:40 |     eval_candidates: inline\n16:46:40 |     eval_dynamic_batching: None\n16:46:40 |     evaltask: None\n16:46:40 |     ffn_size: 3072\n16:46:40 |     final_extra_opt: \n16:46:40 |     fixed_candidate_vecs: reuse\n16:46:40 |     fixed_candidates_path: None\n16:46:40 |     force_fp16_tokens: False\n16:46:40 |     fp16: True\n16:46:40 |     fp16_impl: safe\n16:46:40 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt\n16:46:40 |     fromfile_datatype_extension: False\n16:46:40 |     gpu: -1\n16:46:40 |     gradient_clip: 0.1\n16:46:40 |     hide_labels: False\n16:46:40 |     history_add_global_end_token: None\n16:46:40 |     history_reversed: False\n16:46:40 |     history_size: 20\n16:46:40 |     ignore_bad_candidates: False\n16:46:40 |     ignore_labels: None\n16:46:40 |     image_cropsize: 224\n16:46:40 |     image_mode: raw\n16:46:40 |     image_size: 256\n16:46:40 |     inference: max\n16:46:40 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:46:40 |     init_opt: None\n16:46:40 |     interactive_candidates: fixed\n16:46:40 |     interactive_mode: False\n16:46:40 |     invsqrt_lr_decay_gamma: -1\n16:46:40 |     is_debug: False\n16:46:40 |     label_truncate: 72\n16:46:40 |     learn_embeddings: True\n16:46:40 |     learn_positional_embeddings: True\n16:46:40 |     learningrate: 5e-05\n16:46:40 |     load_from_checkpoint: False\n16:46:40 |     load_from_pretrained_ranker: True\n16:46:40 |     log_every_n_secs: 10.0\n16:46:40 |     log_every_n_steps: 50\n16:46:40 |     log_keep_fields: all\n16:46:40 |     loglevel: info\n16:46:40 |     lr_scheduler: reduceonplateau\n16:46:40 |     lr_scheduler_decay: 0.5\n16:46:40 |     lr_scheduler_patience: 3\n16:46:40 |     max_train_steps: -1\n16:46:40 |     max_train_time: 7200.0\n16:46:40 |     memory_attention: sqrt\n16:46:40 |     metrics: default\n16:46:40 |     model: transformer/classifier\n16:46:40 |     model_file: /tmp/model3\n16:46:40 |     model_parallel: False\n16:46:40 |     momentum: 0\n16:46:40 |     multitask_weights: [1]\n16:46:40 |     mutators: None\n16:46:40 |     n_decoder_layers: -1\n16:46:40 |     n_encoder_layers: -1\n16:46:40 |     n_heads: 12\n16:46:40 |     n_layers: 12\n16:46:40 |     n_positions: 1024\n16:46:40 |     n_segments: 2\n16:46:40 |     nesterov: True\n16:46:40 |     no_cuda: False\n16:46:40 |     normalize_sent_emb: False\n16:46:40 |     num_epochs: -1\n16:46:40 |     num_workers: 0\n16:46:40 |     nus: (0.7,)\n16:46:40 |     optimizer: adamax\n16:46:40 |     output_scaling: 0.06\n16:46:40 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n16:46:40 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:46:40 |     person_tokens: False\n16:46:40 |     print_scores: False\n16:46:40 |     rank_candidates: False\n16:46:40 |     rank_top_k: -1\n16:46:40 |     reduction_type: mean\n16:46:40 |     ref_class: None\n16:46:40 |     relu_dropout: 0.0\n16:46:40 |     repeat_blocking_heuristic: True\n16:46:40 |     return_cand_scores: False\n16:46:40 |     save_after_valid: True\n16:46:40 |     save_every_n_secs: -1\n16:46:40 |     save_format: conversations\n16:46:40 |     share_encoders: False\n16:46:40 |     share_word_embeddings: False\n16:46:40 |     short_final_eval: False\n16:46:40 |     special_tok_lst: None\n16:46:40 |     split_lines: False\n16:46:40 |     starttime: Dec03_16-46\n16:46:40 |     task: fromfile:parlaiformat\n16:46:40 |     tensorboard_log: False\n16:46:40 |     tensorboard_logdir: None\n16:46:40 |     text_truncate: 360\n16:46:40 |     threshold: 0.5\n16:46:40 |     topk: 5\n16:46:40 |     train_predict: False\n16:46:40 |     truncate: 1024\n16:46:40 |     update_classifier_head_only: False\n16:46:40 |     update_freq: 1\n16:46:40 |     use_memories: False\n16:46:40 |     use_reply: none\n16:46:40 |     validation_cutoff: 1.0\n16:46:40 |     validation_every_n_epochs: -1\n16:46:40 |     validation_every_n_secs: 20.0\n16:46:40 |     validation_every_n_steps: -1\n16:46:40 |     validation_max_exs: -1\n16:46:40 |     validation_metric: accuracy\n16:46:40 |     validation_metric_mode: max\n16:46:40 |     validation_patience: 30\n16:46:40 |     validation_share_agent: False\n16:46:40 |     variant: xlm\n16:46:40 |     verbose: False\n16:46:40 |     wandb_entity: None\n16:46:40 |     wandb_log: False\n16:46:40 |     wandb_name: None\n16:46:40 |     wandb_project: None\n16:46:40 |     warmup_rate: 0.0001\n16:46:40 |     warmup_updates: 1000\n16:46:40 |     weight_decay: None\n16:46:40 |     world_logs: \n16:46:40 |     wrap_memory_encoder: False\n16:46:41 | creating task(s): fromfile:parlaiformat\n16:46:41 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt\n16:46:41 | training...\n16:46:51 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5925 5.925e-10               .5766                 .6000   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5550            .6072              .5860   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6300  11.5     1   270 540.5       0          0 40.03  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5925             32768  2.809    .1206     6 .6838 1.005e-06   120 240.2   \n    ltrunc  ltrunclen  total_train_updates  tpb   tps   ups  weighted_f1  \n         0          0                   20  390 780.7 2.006        .5919\n\n16:47:01 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8118 8.118e-10               .8060                 .8159   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7962            .8174              .8081   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8269  11.8     1   276  1048       0          0 75.92  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8118             32768  2.835    .1207 5.982 .6279 2.905e-06 119.6 454.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 395.6 1502 3.805        .8118\n\n16:47:01 | creating task(s): fromfile:parlaiformat\n16:47:01 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:47:01 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt\n16:47:01 | running eval: valid\n16:47:01 | eval completed in 0.19s\n16:47:01 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9600                 .9231   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9565                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 11.71 164.5  1905       0          0 138.9   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5517 2.905e-06    72 833.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2739        .9583\n\u001b[0m\n16:47:01 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n16:47:01 | saving best valid model: /tmp/model3\n16:47:01 | Saving dictionary to /tmp/model3.dict\n16:47:04 | saving model checkpoint: /tmp/model3.checkpoint\n16:47:05 | Saving dictionary to /tmp/model3.checkpoint.dict\n16:47:22 | time:41s total_exs:1900 total_steps:95 epochs:79.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9986 9.986e-10               .9986                 .9973   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9987                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9973 11.48     1 269.6 992.6       0          0 73.63  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9986             32768  2.951    .1207 5.992 .4565 4.755e-06 119.8 441.2   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                   95 389.5 1434 3.69        .9986\n\n16:47:25 | time:44s total_exs:2100 total_steps:105 epochs:87.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.61     1 272.2  1086       0          0 79.76  200   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.831    .1207   6.1 .2842 5.254e-06   122 486.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  105 394.2 1572 4.024            1\n\n16:47:25 | running eval: valid\n16:47:25 | eval completed in 0.25s\n16:47:25 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1547       0          0 112.8   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .2145 5.254e-06    72 677.1       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    105 236.5 2224            1\n\u001b[0m\n16:47:25 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n16:47:25 | saving best valid model: /tmp/model3\n16:47:35 | task solved! stopping.\n16:47:35 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:47:35 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:47:35 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:47:35 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:47:35 | Using CUDA\n16:47:35 | loading dictionary from /tmp/model3.dict\n16:47:35 | num words = 54944\n16:47:39 | Loading existing model parameters from /tmp/model3\n16:47:41 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:47:42 | creating task(s): fromfile:parlaiformat\n16:47:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:47:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt\n16:47:42 | running eval: valid\n16:47:42 | eval completed in 0.20s\n16:47:42 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1855       0          0 135.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2145 5.254e-06    72 811.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    105 236.5 2667            1\n\u001b[0m\n16:47:42 | creating task(s): fromfile:parlaiformat\n16:47:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:47:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt\n16:47:42 | running eval: test\n16:47:43 | eval completed in 0.19s\n16:47:43 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1892       0          0 137.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2145 5.254e-06    72 827.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    105 236.5 2720            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:47:44.716725Z","iopub.execute_input":"2022-12-03T16:47:44.717525Z","iopub.status.idle":"2022-12-03T16:48:12.744617Z","shell.execute_reply.started":"2022-12-03T16:47:44.717470Z","shell.execute_reply":"2022-12-03T16:48:12.743438Z"},"scrolled":true,"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"16:47:52 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt)\u001b[0m\n16:47:52 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:47:52 | Using CUDA\n16:47:52 | loading dictionary from /tmp/model3.dict\n16:47:52 | num words = 54944\n16:47:56 | Loading existing model parameters from /tmp/model3\n16:48:03 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:48:04 | Opt:\n16:48:04 |     activation: gelu\n16:48:04 |     adafactor_eps: '[1e-30, 0.001]'\n16:48:04 |     adam_eps: 1e-08\n16:48:04 |     add_p1_after_newln: False\n16:48:04 |     aggregate_micro: False\n16:48:04 |     allow_missing_init_opts: False\n16:48:04 |     area_under_curve_class: None\n16:48:04 |     area_under_curve_digits: -1\n16:48:04 |     attention_dropout: 0.1\n16:48:04 |     batchsize: 40\n16:48:04 |     betas: '[0.9, 0.999]'\n16:48:04 |     bpe_add_prefix_space: None\n16:48:04 |     bpe_debug: False\n16:48:04 |     bpe_dropout: None\n16:48:04 |     bpe_merge: None\n16:48:04 |     bpe_vocab: None\n16:48:04 |     candidates: inline\n16:48:04 |     cap_num_predictions: 100\n16:48:04 |     checkpoint_activations: False\n16:48:04 |     class_weights: None\n16:48:04 |     classes: \"['__notok__', '__ok__']\"\n16:48:04 |     classes_from_file: None\n16:48:04 |     data_parallel: True\n16:48:04 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:48:04 |     datatype: train\n16:48:04 |     delimiter: '\\n'\n16:48:04 |     dict_class: parlai.core.dict:DictionaryAgent\n16:48:04 |     dict_endtoken: __start__\n16:48:04 |     dict_file: /tmp/model3.dict\n16:48:04 |     dict_include_test: False\n16:48:04 |     dict_include_valid: False\n16:48:04 |     dict_initpath: None\n16:48:04 |     dict_language: english\n16:48:04 |     dict_loaded: True\n16:48:04 |     dict_lower: True\n16:48:04 |     dict_max_ngram_size: -1\n16:48:04 |     dict_maxexs: -1\n16:48:04 |     dict_maxtokens: -1\n16:48:04 |     dict_minfreq: 0\n16:48:04 |     dict_nulltoken: __null__\n16:48:04 |     dict_starttoken: __start__\n16:48:04 |     dict_textfields: text,labels\n16:48:04 |     dict_tokenizer: bpe\n16:48:04 |     dict_unktoken: __unk__\n16:48:04 |     display_examples: False\n16:48:04 |     download_path: None\n16:48:04 |     dropout: 0.1\n16:48:04 |     dynamic_batching: None\n16:48:04 |     embedding_projection: random\n16:48:04 |     embedding_size: 768\n16:48:04 |     embedding_type: random\n16:48:04 |     embeddings_scale: False\n16:48:04 |     encode_candidate_vecs: True\n16:48:04 |     encode_candidate_vecs_batchsize: 256\n16:48:04 |     eval_batchsize: None\n16:48:04 |     eval_candidates: inline\n16:48:04 |     eval_dynamic_batching: None\n16:48:04 |     evaltask: None\n16:48:04 |     ffn_size: 3072\n16:48:04 |     final_extra_opt: \n16:48:04 |     fixed_candidate_vecs: reuse\n16:48:04 |     fixed_candidates_path: None\n16:48:04 |     force_fp16_tokens: True\n16:48:04 |     fp16: True\n16:48:04 |     fp16_impl: safe\n16:48:04 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-a.txt\n16:48:04 |     fromfile_datatype_extension: False\n16:48:04 |     gpu: -1\n16:48:04 |     gradient_clip: 0.1\n16:48:04 |     hide_labels: False\n16:48:04 |     history_add_global_end_token: None\n16:48:04 |     history_reversed: False\n16:48:04 |     history_size: 20\n16:48:04 |     ignore_bad_candidates: False\n16:48:04 |     ignore_labels: None\n16:48:04 |     image_cropsize: 224\n16:48:04 |     image_mode: raw\n16:48:04 |     image_size: 256\n16:48:04 |     inference: max\n16:48:04 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:48:04 |     init_opt: None\n16:48:04 |     interactive_candidates: fixed\n16:48:04 |     interactive_mode: False\n16:48:04 |     invsqrt_lr_decay_gamma: -1\n16:48:04 |     is_debug: False\n16:48:04 |     label_truncate: 72\n16:48:04 |     learn_embeddings: True\n16:48:04 |     learn_positional_embeddings: True\n16:48:04 |     learningrate: 5e-05\n16:48:04 |     load_from_pretrained_ranker: True\n16:48:04 |     log_every_n_secs: 10.0\n16:48:04 |     log_every_n_steps: 50\n16:48:04 |     log_keep_fields: all\n16:48:04 |     loglevel: info\n16:48:04 |     lr_scheduler: reduceonplateau\n16:48:04 |     lr_scheduler_decay: 0.5\n16:48:04 |     lr_scheduler_patience: 3\n16:48:04 |     max_train_steps: -1\n16:48:04 |     max_train_time: 7200.0\n16:48:04 |     memory_attention: sqrt\n16:48:04 |     metrics: default\n16:48:04 |     model: transformer/classifier\n16:48:04 |     model_file: /tmp/model3\n16:48:04 |     model_parallel: False\n16:48:04 |     momentum: 0\n16:48:04 |     multitask_weights: [1]\n16:48:04 |     mutators: None\n16:48:04 |     n_decoder_layers: -1\n16:48:04 |     n_encoder_layers: -1\n16:48:04 |     n_heads: 12\n16:48:04 |     n_layers: 12\n16:48:04 |     n_positions: 1024\n16:48:04 |     n_segments: 2\n16:48:04 |     nesterov: True\n16:48:04 |     no_cuda: False\n16:48:04 |     normalize_sent_emb: False\n16:48:04 |     num_epochs: -1\n16:48:04 |     num_examples: -1\n16:48:04 |     num_workers: 0\n16:48:04 |     nus: [0.7]\n16:48:04 |     optimizer: adamax\n16:48:04 |     output_scaling: 0.06\n16:48:04 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n16:48:04 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:48:04 |     person_tokens: False\n16:48:04 |     print_scores: False\n16:48:04 |     rank_candidates: False\n16:48:04 |     rank_top_k: -1\n16:48:04 |     reduction_type: mean\n16:48:04 |     ref_class: None\n16:48:04 |     relu_dropout: 0.0\n16:48:04 |     repeat_blocking_heuristic: True\n16:48:04 |     report_filename: \n16:48:04 |     return_cand_scores: False\n16:48:04 |     save_after_valid: True\n16:48:04 |     save_every_n_secs: -1\n16:48:04 |     save_format: conversations\n16:48:04 |     share_encoders: False\n16:48:04 |     share_word_embeddings: False\n16:48:04 |     short_final_eval: False\n16:48:04 |     special_tok_lst: None\n16:48:04 |     split_lines: False\n16:48:04 |     starttime: Dec03_16-46\n16:48:04 |     task: fromfile:parlaiformat\n16:48:04 |     tensorboard_log: False\n16:48:04 |     tensorboard_logdir: None\n16:48:04 |     text_truncate: 360\n16:48:04 |     threshold: 0.5\n16:48:04 |     topk: 5\n16:48:04 |     train_predict: False\n16:48:04 |     truncate: 1024\n16:48:04 |     update_classifier_head_only: False\n16:48:04 |     update_freq: 1\n16:48:04 |     use_memories: False\n16:48:04 |     use_reply: none\n16:48:04 |     validation_cutoff: 1.0\n16:48:04 |     validation_every_n_epochs: -1\n16:48:04 |     validation_every_n_secs: 20.0\n16:48:04 |     validation_every_n_steps: -1\n16:48:04 |     validation_max_exs: -1\n16:48:04 |     validation_metric: accuracy\n16:48:04 |     validation_metric_mode: max\n16:48:04 |     validation_patience: 30\n16:48:04 |     validation_share_agent: False\n16:48:04 |     variant: xlm\n16:48:04 |     verbose: False\n16:48:04 |     wandb_entity: None\n16:48:04 |     wandb_log: False\n16:48:04 |     wandb_name: None\n16:48:04 |     wandb_project: None\n16:48:04 |     warmup_rate: 0.0001\n16:48:04 |     warmup_updates: 1000\n16:48:04 |     weight_decay: None\n16:48:04 |     world_logs: \n16:48:04 |     wrap_memory_encoder: False\n16:48:05 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:48:05 | creating task(s): fromfile:parlaiformat\n16:48:05 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:48:05 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-a.txt\n16:48:11 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1900 1.9e-10               .1980                 .1961   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2000            .1818              .1837   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1800 11.47   539 484.4       0          0 35.95  200 .1900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 1.055 5.254e-06   240 215.7       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    105  779 700.1        .1899\u001b[0m\n16:48:11 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1900 1.9e-10               .1980                 .1961   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2000            .1818              .1837   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1800 11.47   539 484.4       0          0 35.95  200 .1900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 1.055 5.254e-06   240 215.7       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    105  779 700.1        .1899\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:48:12.748311Z","iopub.execute_input":"2022-12-03T16:48:12.748661Z","iopub.status.idle":"2022-12-03T16:48:38.914969Z","shell.execute_reply.started":"2022-12-03T16:48:12.748631Z","shell.execute_reply":"2022-12-03T16:48:38.913597Z"},"scrolled":true,"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"16:48:19 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_valid.txt)\u001b[0m\n16:48:19 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:48:19 | Using CUDA\n16:48:19 | loading dictionary from /tmp/model3.dict\n16:48:19 | num words = 54944\n16:48:24 | Loading existing model parameters from /tmp/model3\n16:48:30 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:48:31 | Opt:\n16:48:31 |     activation: gelu\n16:48:31 |     adafactor_eps: '[1e-30, 0.001]'\n16:48:31 |     adam_eps: 1e-08\n16:48:31 |     add_p1_after_newln: False\n16:48:31 |     aggregate_micro: False\n16:48:31 |     allow_missing_init_opts: False\n16:48:31 |     area_under_curve_class: None\n16:48:31 |     area_under_curve_digits: -1\n16:48:31 |     attention_dropout: 0.1\n16:48:31 |     batchsize: 40\n16:48:31 |     betas: '[0.9, 0.999]'\n16:48:31 |     bpe_add_prefix_space: None\n16:48:31 |     bpe_debug: False\n16:48:31 |     bpe_dropout: None\n16:48:31 |     bpe_merge: None\n16:48:31 |     bpe_vocab: None\n16:48:31 |     candidates: inline\n16:48:31 |     cap_num_predictions: 100\n16:48:31 |     checkpoint_activations: False\n16:48:31 |     class_weights: None\n16:48:31 |     classes: \"['__notok__', '__ok__']\"\n16:48:31 |     classes_from_file: None\n16:48:31 |     data_parallel: True\n16:48:31 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:48:31 |     datatype: train\n16:48:31 |     delimiter: '\\n'\n16:48:31 |     dict_class: parlai.core.dict:DictionaryAgent\n16:48:31 |     dict_endtoken: __start__\n16:48:31 |     dict_file: /tmp/model3.dict\n16:48:31 |     dict_include_test: False\n16:48:31 |     dict_include_valid: False\n16:48:31 |     dict_initpath: None\n16:48:31 |     dict_language: english\n16:48:31 |     dict_loaded: True\n16:48:31 |     dict_lower: True\n16:48:31 |     dict_max_ngram_size: -1\n16:48:31 |     dict_maxexs: -1\n16:48:31 |     dict_maxtokens: -1\n16:48:31 |     dict_minfreq: 0\n16:48:31 |     dict_nulltoken: __null__\n16:48:31 |     dict_starttoken: __start__\n16:48:31 |     dict_textfields: text,labels\n16:48:31 |     dict_tokenizer: bpe\n16:48:31 |     dict_unktoken: __unk__\n16:48:31 |     display_examples: False\n16:48:31 |     download_path: None\n16:48:31 |     dropout: 0.1\n16:48:31 |     dynamic_batching: None\n16:48:31 |     embedding_projection: random\n16:48:31 |     embedding_size: 768\n16:48:31 |     embedding_type: random\n16:48:31 |     embeddings_scale: False\n16:48:31 |     encode_candidate_vecs: True\n16:48:31 |     encode_candidate_vecs_batchsize: 256\n16:48:31 |     eval_batchsize: None\n16:48:31 |     eval_candidates: inline\n16:48:31 |     eval_dynamic_batching: None\n16:48:31 |     evaltask: None\n16:48:31 |     ffn_size: 3072\n16:48:31 |     final_extra_opt: \n16:48:31 |     fixed_candidate_vecs: reuse\n16:48:31 |     fixed_candidates_path: None\n16:48:31 |     force_fp16_tokens: True\n16:48:31 |     fp16: True\n16:48:31 |     fp16_impl: safe\n16:48:31 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-b.txt\n16:48:31 |     fromfile_datatype_extension: False\n16:48:31 |     gpu: -1\n16:48:31 |     gradient_clip: 0.1\n16:48:31 |     hide_labels: False\n16:48:31 |     history_add_global_end_token: None\n16:48:31 |     history_reversed: False\n16:48:31 |     history_size: 20\n16:48:31 |     ignore_bad_candidates: False\n16:48:31 |     ignore_labels: None\n16:48:31 |     image_cropsize: 224\n16:48:31 |     image_mode: raw\n16:48:31 |     image_size: 256\n16:48:31 |     inference: max\n16:48:31 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:48:31 |     init_opt: None\n16:48:31 |     interactive_candidates: fixed\n16:48:31 |     interactive_mode: False\n16:48:31 |     invsqrt_lr_decay_gamma: -1\n16:48:31 |     is_debug: False\n16:48:31 |     label_truncate: 72\n16:48:31 |     learn_embeddings: True\n16:48:31 |     learn_positional_embeddings: True\n16:48:31 |     learningrate: 5e-05\n16:48:31 |     load_from_pretrained_ranker: True\n16:48:31 |     log_every_n_secs: 10.0\n16:48:31 |     log_every_n_steps: 50\n16:48:31 |     log_keep_fields: all\n16:48:31 |     loglevel: info\n16:48:31 |     lr_scheduler: reduceonplateau\n16:48:31 |     lr_scheduler_decay: 0.5\n16:48:31 |     lr_scheduler_patience: 3\n16:48:31 |     max_train_steps: -1\n16:48:31 |     max_train_time: 7200.0\n16:48:31 |     memory_attention: sqrt\n16:48:31 |     metrics: default\n16:48:31 |     model: transformer/classifier\n16:48:31 |     model_file: /tmp/model3\n16:48:31 |     model_parallel: False\n16:48:31 |     momentum: 0\n16:48:31 |     multitask_weights: [1]\n16:48:31 |     mutators: None\n16:48:31 |     n_decoder_layers: -1\n16:48:31 |     n_encoder_layers: -1\n16:48:31 |     n_heads: 12\n16:48:31 |     n_layers: 12\n16:48:31 |     n_positions: 1024\n16:48:31 |     n_segments: 2\n16:48:31 |     nesterov: True\n16:48:31 |     no_cuda: False\n16:48:31 |     normalize_sent_emb: False\n16:48:31 |     num_epochs: -1\n16:48:31 |     num_examples: -1\n16:48:31 |     num_workers: 0\n16:48:31 |     nus: [0.7]\n16:48:31 |     optimizer: adamax\n16:48:31 |     output_scaling: 0.06\n16:48:31 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n16:48:31 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:48:31 |     person_tokens: False\n16:48:31 |     print_scores: False\n16:48:31 |     rank_candidates: False\n16:48:31 |     rank_top_k: -1\n16:48:31 |     reduction_type: mean\n16:48:31 |     ref_class: None\n16:48:31 |     relu_dropout: 0.0\n16:48:31 |     repeat_blocking_heuristic: True\n16:48:31 |     report_filename: \n16:48:31 |     return_cand_scores: False\n16:48:31 |     save_after_valid: True\n16:48:31 |     save_every_n_secs: -1\n16:48:31 |     save_format: conversations\n16:48:31 |     share_encoders: False\n16:48:31 |     share_word_embeddings: False\n16:48:31 |     short_final_eval: False\n16:48:31 |     special_tok_lst: None\n16:48:31 |     split_lines: False\n16:48:31 |     starttime: Dec03_16-46\n16:48:31 |     task: fromfile:parlaiformat\n16:48:31 |     tensorboard_log: False\n16:48:31 |     tensorboard_logdir: None\n16:48:31 |     text_truncate: 360\n16:48:31 |     threshold: 0.5\n16:48:31 |     topk: 5\n16:48:31 |     train_predict: False\n16:48:31 |     truncate: 1024\n16:48:31 |     update_classifier_head_only: False\n16:48:31 |     update_freq: 1\n16:48:31 |     use_memories: False\n16:48:31 |     use_reply: none\n16:48:31 |     validation_cutoff: 1.0\n16:48:31 |     validation_every_n_epochs: -1\n16:48:31 |     validation_every_n_secs: 20.0\n16:48:31 |     validation_every_n_steps: -1\n16:48:31 |     validation_max_exs: -1\n16:48:31 |     validation_metric: accuracy\n16:48:31 |     validation_metric_mode: max\n16:48:31 |     validation_patience: 30\n16:48:31 |     validation_share_agent: False\n16:48:31 |     variant: xlm\n16:48:31 |     verbose: False\n16:48:31 |     wandb_entity: None\n16:48:31 |     wandb_log: False\n16:48:31 |     wandb_name: None\n16:48:31 |     wandb_project: None\n16:48:31 |     warmup_rate: 0.0001\n16:48:31 |     warmup_updates: 1000\n16:48:31 |     weight_decay: None\n16:48:31 |     world_logs: \n16:48:31 |     wrap_memory_encoder: False\n16:48:31 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:48:31 | creating task(s): fromfile:parlaiformat\n16:48:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:48:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run3/data_train-b.txt\n16:48:37 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8100 8.1e-10               .8119                 .8039   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8200            .8081              .8163   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8000 11.47   539 505.4       0          0  37.5  200 .8100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .5003 5.254e-06   240   225       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    105  779 730.4        .8100\u001b[0m\n16:48:37 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8100 8.1e-10               .8119                 .8039   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8200            .8081              .8163   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8000 11.47   539 505.4       0          0  37.5  200 .8100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .5003 5.254e-06   240   225       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    105  779 730.4        .8100\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:48:38.916875Z","iopub.execute_input":"2022-12-03T16:48:38.917331Z","iopub.status.idle":"2022-12-03T16:48:40.035223Z","shell.execute_reply.started":"2022-12-03T16:48:38.917285Z","shell.execute_reply":"2022-12-03T16:48:40.033982Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:48:40.039127Z","iopub.execute_input":"2022-12-03T16:48:40.039453Z","iopub.status.idle":"2022-12-03T16:49:34.864743Z","shell.execute_reply.started":"2022-12-03T16:48:40.039412Z","shell.execute_reply":"2022-12-03T16:49:34.863543Z"},"scrolled":true,"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"16:48:47 | building dictionary first...\n16:48:47 | No model with opt yet at: /tmp/model4(.opt)\n16:48:47 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:48:47 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:48:47 | Using CUDA\n16:48:47 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:48:47 | num words = 54944\n16:48:51 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:48:57 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:48:57 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:48:57 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:48:57 | Opt:\n16:48:57 |     activation: gelu\n16:48:57 |     adafactor_eps: '(1e-30, 0.001)'\n16:48:57 |     adam_eps: 1e-08\n16:48:57 |     add_p1_after_newln: False\n16:48:57 |     aggregate_micro: False\n16:48:57 |     allow_missing_init_opts: False\n16:48:57 |     attention_dropout: 0.1\n16:48:57 |     batchsize: 20\n16:48:57 |     betas: '(0.9, 0.999)'\n16:48:57 |     bpe_add_prefix_space: None\n16:48:57 |     bpe_debug: False\n16:48:57 |     bpe_dropout: None\n16:48:57 |     bpe_merge: None\n16:48:57 |     bpe_vocab: None\n16:48:57 |     candidates: inline\n16:48:57 |     cap_num_predictions: 100\n16:48:57 |     checkpoint_activations: False\n16:48:57 |     class_weights: None\n16:48:57 |     classes: \"['__notok__', '__ok__']\"\n16:48:57 |     classes_from_file: None\n16:48:57 |     data_parallel: True\n16:48:57 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:48:57 |     datatype: train\n16:48:57 |     delimiter: '\\n'\n16:48:57 |     dict_class: parlai.core.dict:DictionaryAgent\n16:48:57 |     dict_endtoken: __start__\n16:48:57 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:48:57 |     dict_include_test: False\n16:48:57 |     dict_include_valid: False\n16:48:57 |     dict_initpath: None\n16:48:57 |     dict_language: english\n16:48:57 |     dict_loaded: True\n16:48:57 |     dict_lower: True\n16:48:57 |     dict_max_ngram_size: -1\n16:48:57 |     dict_maxexs: -1\n16:48:57 |     dict_maxtokens: -1\n16:48:57 |     dict_minfreq: 0\n16:48:57 |     dict_nulltoken: __null__\n16:48:57 |     dict_starttoken: __start__\n16:48:57 |     dict_textfields: text,labels\n16:48:57 |     dict_tokenizer: bpe\n16:48:57 |     dict_unktoken: __unk__\n16:48:57 |     display_examples: False\n16:48:57 |     download_path: None\n16:48:57 |     dropout: 0.1\n16:48:57 |     dynamic_batching: None\n16:48:57 |     embedding_projection: random\n16:48:57 |     embedding_size: 768\n16:48:57 |     embedding_type: random\n16:48:57 |     embeddings_scale: False\n16:48:57 |     encode_candidate_vecs: True\n16:48:57 |     encode_candidate_vecs_batchsize: 256\n16:48:57 |     eval_batchsize: None\n16:48:57 |     eval_candidates: inline\n16:48:57 |     eval_dynamic_batching: None\n16:48:57 |     evaltask: None\n16:48:57 |     ffn_size: 3072\n16:48:57 |     final_extra_opt: \n16:48:57 |     fixed_candidate_vecs: reuse\n16:48:57 |     fixed_candidates_path: None\n16:48:57 |     force_fp16_tokens: False\n16:48:57 |     fp16: True\n16:48:57 |     fp16_impl: safe\n16:48:57 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt\n16:48:57 |     fromfile_datatype_extension: False\n16:48:57 |     gpu: -1\n16:48:57 |     gradient_clip: 0.1\n16:48:57 |     hide_labels: False\n16:48:57 |     history_add_global_end_token: None\n16:48:57 |     history_reversed: False\n16:48:57 |     history_size: 20\n16:48:57 |     ignore_bad_candidates: False\n16:48:57 |     ignore_labels: None\n16:48:57 |     image_cropsize: 224\n16:48:57 |     image_mode: raw\n16:48:57 |     image_size: 256\n16:48:57 |     inference: max\n16:48:57 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:48:57 |     init_opt: None\n16:48:57 |     interactive_candidates: fixed\n16:48:57 |     interactive_mode: False\n16:48:57 |     invsqrt_lr_decay_gamma: -1\n16:48:57 |     is_debug: False\n16:48:57 |     label_truncate: 72\n16:48:57 |     learn_embeddings: True\n16:48:57 |     learn_positional_embeddings: True\n16:48:57 |     learningrate: 5e-05\n16:48:57 |     load_from_checkpoint: False\n16:48:57 |     load_from_pretrained_ranker: True\n16:48:57 |     log_every_n_secs: 10.0\n16:48:57 |     log_every_n_steps: 50\n16:48:57 |     log_keep_fields: all\n16:48:57 |     loglevel: info\n16:48:57 |     lr_scheduler: reduceonplateau\n16:48:57 |     lr_scheduler_decay: 0.5\n16:48:57 |     lr_scheduler_patience: 3\n16:48:57 |     max_train_steps: -1\n16:48:57 |     max_train_time: 7200.0\n16:48:57 |     memory_attention: sqrt\n16:48:57 |     metrics: default\n16:48:57 |     model: transformer/classifier\n16:48:57 |     model_file: /tmp/model4\n16:48:57 |     model_parallel: False\n16:48:57 |     momentum: 0\n16:48:57 |     multitask_weights: [1]\n16:48:57 |     mutators: None\n16:48:57 |     n_decoder_layers: -1\n16:48:57 |     n_encoder_layers: -1\n16:48:57 |     n_heads: 12\n16:48:57 |     n_layers: 12\n16:48:57 |     n_positions: 1024\n16:48:57 |     n_segments: 2\n16:48:57 |     nesterov: True\n16:48:57 |     no_cuda: False\n16:48:57 |     normalize_sent_emb: False\n16:48:57 |     num_epochs: -1\n16:48:57 |     num_workers: 0\n16:48:57 |     nus: (0.7,)\n16:48:57 |     optimizer: adamax\n16:48:57 |     output_scaling: 0.06\n16:48:57 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n16:48:57 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:48:57 |     person_tokens: False\n16:48:57 |     print_scores: False\n16:48:57 |     rank_candidates: False\n16:48:57 |     rank_top_k: -1\n16:48:57 |     reduction_type: mean\n16:48:57 |     ref_class: None\n16:48:57 |     relu_dropout: 0.0\n16:48:57 |     repeat_blocking_heuristic: True\n16:48:57 |     return_cand_scores: False\n16:48:57 |     save_after_valid: True\n16:48:57 |     save_every_n_secs: -1\n16:48:57 |     save_format: conversations\n16:48:57 |     share_encoders: False\n16:48:57 |     share_word_embeddings: False\n16:48:57 |     short_final_eval: False\n16:48:57 |     special_tok_lst: None\n16:48:57 |     split_lines: False\n16:48:57 |     starttime: Dec03_16-48\n16:48:57 |     task: fromfile:parlaiformat\n16:48:57 |     tensorboard_log: False\n16:48:57 |     tensorboard_logdir: None\n16:48:57 |     text_truncate: 360\n16:48:57 |     threshold: 0.5\n16:48:57 |     topk: 5\n16:48:57 |     train_predict: False\n16:48:57 |     truncate: 1024\n16:48:57 |     update_classifier_head_only: False\n16:48:57 |     update_freq: 1\n16:48:57 |     use_memories: False\n16:48:57 |     use_reply: none\n16:48:57 |     validation_cutoff: 1.0\n16:48:57 |     validation_every_n_epochs: -1\n16:48:57 |     validation_every_n_secs: 20.0\n16:48:57 |     validation_every_n_steps: -1\n16:48:57 |     validation_max_exs: -1\n16:48:57 |     validation_metric: accuracy\n16:48:57 |     validation_metric_mode: max\n16:48:57 |     validation_patience: 30\n16:48:57 |     validation_share_agent: False\n16:48:57 |     variant: xlm\n16:48:57 |     verbose: False\n16:48:57 |     wandb_entity: None\n16:48:57 |     wandb_log: False\n16:48:57 |     wandb_name: None\n16:48:57 |     wandb_project: None\n16:48:57 |     warmup_rate: 0.0001\n16:48:57 |     warmup_updates: 1000\n16:48:57 |     weight_decay: None\n16:48:57 |     world_logs: \n16:48:57 |     wrap_memory_encoder: False\n16:48:57 | creating task(s): fromfile:parlaiformat\n16:48:57 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt\n16:48:57 | training...\n16:49:08 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5425 5.425e-10               .5436                 .5142   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5767            .5414              .5745   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5118 11.85     1   277 542.1       0          0 39.14  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5425             32768  2.754    .1189 5.945 .6739 1.005e-06 118.9 232.7   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 395.9 774.7 1.961        .5424\n\n16:49:17 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8079 8.079e-10               .8059                 .8301   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7829            .8099              .7873   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8338 12.27     1 285.3  1117       0          0 78.27  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8079             32768  2.773    .1189 6.018 .6297 2.905e-06 120.4 471.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 405.7 1588 3.923        .8078\n\n16:49:17 | creating task(s): fromfile:parlaiformat\n16:49:17 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:49:17 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt\n16:49:17 | running eval: valid\n16:49:18 | eval completed in 0.22s\n16:49:18 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1684       0          0 119.8   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5595 2.905e-06    72 719.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 240.5 2403            1\n\u001b[0m\n16:49:18 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:49:18 | saving best valid model: /tmp/model4\n16:49:18 | Saving dictionary to /tmp/model4.dict\n16:49:21 | task solved! stopping.\n16:49:21 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:49:21 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:49:21 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:49:21 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:49:21 | Using CUDA\n16:49:21 | loading dictionary from /tmp/model4.dict\n16:49:21 | num words = 54944\n16:49:26 | Loading existing model parameters from /tmp/model4\n16:49:30 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:49:31 | creating task(s): fromfile:parlaiformat\n16:49:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:49:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt\n16:49:31 | running eval: valid\n16:49:31 | eval completed in 0.31s\n16:49:31 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1349       0          0 95.23   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5595 2.905e-06    72 576.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 240.5 1925            1\n\u001b[0m\n16:49:31 | creating task(s): fromfile:parlaiformat\n16:49:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:49:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt\n16:49:31 | running eval: test\n16:49:32 | eval completed in 0.25s\n16:49:32 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1553       0          0 110.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5595 2.905e-06    72 663.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 240.5 2217            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:49:34.868238Z","iopub.execute_input":"2022-12-03T16:49:34.868702Z","iopub.status.idle":"2022-12-03T16:50:11.182677Z","shell.execute_reply.started":"2022-12-03T16:49:34.868660Z","shell.execute_reply":"2022-12-03T16:50:11.181381Z"},"scrolled":true,"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"16:49:49 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt)\u001b[0m\n16:49:49 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:49:49 | Using CUDA\n16:49:49 | loading dictionary from /tmp/model4.dict\n16:49:49 | num words = 54944\n16:49:53 | Loading existing model parameters from /tmp/model4\n16:50:01 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:50:03 | Opt:\n16:50:03 |     activation: gelu\n16:50:03 |     adafactor_eps: '[1e-30, 0.001]'\n16:50:03 |     adam_eps: 1e-08\n16:50:03 |     add_p1_after_newln: False\n16:50:03 |     aggregate_micro: False\n16:50:03 |     allow_missing_init_opts: False\n16:50:03 |     area_under_curve_class: None\n16:50:03 |     area_under_curve_digits: -1\n16:50:03 |     attention_dropout: 0.1\n16:50:03 |     batchsize: 40\n16:50:03 |     betas: '[0.9, 0.999]'\n16:50:03 |     bpe_add_prefix_space: None\n16:50:03 |     bpe_debug: False\n16:50:03 |     bpe_dropout: None\n16:50:03 |     bpe_merge: None\n16:50:03 |     bpe_vocab: None\n16:50:03 |     candidates: inline\n16:50:03 |     cap_num_predictions: 100\n16:50:03 |     checkpoint_activations: False\n16:50:03 |     class_weights: None\n16:50:03 |     classes: \"['__notok__', '__ok__']\"\n16:50:03 |     classes_from_file: None\n16:50:03 |     data_parallel: True\n16:50:03 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:50:03 |     datatype: train\n16:50:03 |     delimiter: '\\n'\n16:50:03 |     dict_class: parlai.core.dict:DictionaryAgent\n16:50:03 |     dict_endtoken: __start__\n16:50:03 |     dict_file: /tmp/model4.dict\n16:50:03 |     dict_include_test: False\n16:50:03 |     dict_include_valid: False\n16:50:03 |     dict_initpath: None\n16:50:03 |     dict_language: english\n16:50:03 |     dict_loaded: True\n16:50:03 |     dict_lower: True\n16:50:03 |     dict_max_ngram_size: -1\n16:50:03 |     dict_maxexs: -1\n16:50:03 |     dict_maxtokens: -1\n16:50:03 |     dict_minfreq: 0\n16:50:03 |     dict_nulltoken: __null__\n16:50:03 |     dict_starttoken: __start__\n16:50:03 |     dict_textfields: text,labels\n16:50:03 |     dict_tokenizer: bpe\n16:50:03 |     dict_unktoken: __unk__\n16:50:03 |     display_examples: False\n16:50:03 |     download_path: None\n16:50:03 |     dropout: 0.1\n16:50:03 |     dynamic_batching: None\n16:50:03 |     embedding_projection: random\n16:50:03 |     embedding_size: 768\n16:50:03 |     embedding_type: random\n16:50:03 |     embeddings_scale: False\n16:50:03 |     encode_candidate_vecs: True\n16:50:03 |     encode_candidate_vecs_batchsize: 256\n16:50:03 |     eval_batchsize: None\n16:50:03 |     eval_candidates: inline\n16:50:03 |     eval_dynamic_batching: None\n16:50:03 |     evaltask: None\n16:50:03 |     ffn_size: 3072\n16:50:03 |     final_extra_opt: \n16:50:03 |     fixed_candidate_vecs: reuse\n16:50:03 |     fixed_candidates_path: None\n16:50:03 |     force_fp16_tokens: True\n16:50:03 |     fp16: True\n16:50:03 |     fp16_impl: safe\n16:50:03 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-a.txt\n16:50:03 |     fromfile_datatype_extension: False\n16:50:03 |     gpu: -1\n16:50:03 |     gradient_clip: 0.1\n16:50:03 |     hide_labels: False\n16:50:03 |     history_add_global_end_token: None\n16:50:03 |     history_reversed: False\n16:50:03 |     history_size: 20\n16:50:03 |     ignore_bad_candidates: False\n16:50:03 |     ignore_labels: None\n16:50:03 |     image_cropsize: 224\n16:50:03 |     image_mode: raw\n16:50:03 |     image_size: 256\n16:50:03 |     inference: max\n16:50:03 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:50:03 |     init_opt: None\n16:50:03 |     interactive_candidates: fixed\n16:50:03 |     interactive_mode: False\n16:50:03 |     invsqrt_lr_decay_gamma: -1\n16:50:03 |     is_debug: False\n16:50:03 |     label_truncate: 72\n16:50:03 |     learn_embeddings: True\n16:50:03 |     learn_positional_embeddings: True\n16:50:03 |     learningrate: 5e-05\n16:50:03 |     load_from_pretrained_ranker: True\n16:50:03 |     log_every_n_secs: 10.0\n16:50:03 |     log_every_n_steps: 50\n16:50:03 |     log_keep_fields: all\n16:50:03 |     loglevel: info\n16:50:03 |     lr_scheduler: reduceonplateau\n16:50:03 |     lr_scheduler_decay: 0.5\n16:50:03 |     lr_scheduler_patience: 3\n16:50:03 |     max_train_steps: -1\n16:50:03 |     max_train_time: 7200.0\n16:50:03 |     memory_attention: sqrt\n16:50:03 |     metrics: default\n16:50:03 |     model: transformer/classifier\n16:50:03 |     model_file: /tmp/model4\n16:50:03 |     model_parallel: False\n16:50:03 |     momentum: 0\n16:50:03 |     multitask_weights: [1]\n16:50:03 |     mutators: None\n16:50:03 |     n_decoder_layers: -1\n16:50:03 |     n_encoder_layers: -1\n16:50:03 |     n_heads: 12\n16:50:03 |     n_layers: 12\n16:50:03 |     n_positions: 1024\n16:50:03 |     n_segments: 2\n16:50:03 |     nesterov: True\n16:50:03 |     no_cuda: False\n16:50:03 |     normalize_sent_emb: False\n16:50:03 |     num_epochs: -1\n16:50:03 |     num_examples: -1\n16:50:03 |     num_workers: 0\n16:50:03 |     nus: [0.7]\n16:50:03 |     optimizer: adamax\n16:50:03 |     output_scaling: 0.06\n16:50:03 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n16:50:03 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:50:03 |     person_tokens: False\n16:50:03 |     print_scores: False\n16:50:03 |     rank_candidates: False\n16:50:03 |     rank_top_k: -1\n16:50:03 |     reduction_type: mean\n16:50:03 |     ref_class: None\n16:50:03 |     relu_dropout: 0.0\n16:50:03 |     repeat_blocking_heuristic: True\n16:50:03 |     report_filename: \n16:50:03 |     return_cand_scores: False\n16:50:03 |     save_after_valid: True\n16:50:03 |     save_every_n_secs: -1\n16:50:03 |     save_format: conversations\n16:50:03 |     share_encoders: False\n16:50:03 |     share_word_embeddings: False\n16:50:03 |     short_final_eval: False\n16:50:03 |     special_tok_lst: None\n16:50:03 |     split_lines: False\n16:50:03 |     starttime: Dec03_16-48\n16:50:03 |     task: fromfile:parlaiformat\n16:50:03 |     tensorboard_log: False\n16:50:03 |     tensorboard_logdir: None\n16:50:03 |     text_truncate: 360\n16:50:03 |     threshold: 0.5\n16:50:03 |     topk: 5\n16:50:03 |     train_predict: False\n16:50:03 |     truncate: 1024\n16:50:03 |     update_classifier_head_only: False\n16:50:03 |     update_freq: 1\n16:50:03 |     use_memories: False\n16:50:03 |     use_reply: none\n16:50:03 |     validation_cutoff: 1.0\n16:50:03 |     validation_every_n_epochs: -1\n16:50:03 |     validation_every_n_secs: 20.0\n16:50:03 |     validation_every_n_steps: -1\n16:50:03 |     validation_max_exs: -1\n16:50:03 |     validation_metric: accuracy\n16:50:03 |     validation_metric_mode: max\n16:50:03 |     validation_patience: 30\n16:50:03 |     validation_share_agent: False\n16:50:03 |     variant: xlm\n16:50:03 |     verbose: False\n16:50:03 |     wandb_entity: None\n16:50:03 |     wandb_log: False\n16:50:03 |     wandb_name: None\n16:50:03 |     wandb_project: None\n16:50:03 |     warmup_rate: 0.0001\n16:50:03 |     warmup_updates: 1000\n16:50:03 |     weight_decay: None\n16:50:03 |     world_logs: \n16:50:03 |     wrap_memory_encoder: False\n16:50:03 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:50:03 | creating task(s): fromfile:parlaiformat\n16:50:03 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:50:03 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-a.txt\n16:50:09 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2900 2.9e-10               .1839                 .2133   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1616            .3717              .3360   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4158 11.46 538.2 505.2       0          0 37.55  200 .2900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .7377 2.905e-06 239.6 224.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 777.8 730.1        .2787\u001b[0m\n16:50:09 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2900 2.9e-10               .1839                 .2133   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1616            .3717              .3360   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4158 11.46 538.2 505.2       0          0 37.55  200 .2900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .7377 2.905e-06 239.6 224.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 777.8 730.1        .2787\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:50:11.184528Z","iopub.execute_input":"2022-12-03T16:50:11.184952Z","iopub.status.idle":"2022-12-03T16:50:38.098158Z","shell.execute_reply.started":"2022-12-03T16:50:11.184907Z","shell.execute_reply":"2022-12-03T16:50:38.096962Z"},"scrolled":true,"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"16:50:18 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_valid.txt)\u001b[0m\n16:50:18 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:50:18 | Using CUDA\n16:50:18 | loading dictionary from /tmp/model4.dict\n16:50:18 | num words = 54944\n16:50:22 | Loading existing model parameters from /tmp/model4\n16:50:28 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:50:29 | Opt:\n16:50:29 |     activation: gelu\n16:50:29 |     adafactor_eps: '[1e-30, 0.001]'\n16:50:29 |     adam_eps: 1e-08\n16:50:29 |     add_p1_after_newln: False\n16:50:29 |     aggregate_micro: False\n16:50:29 |     allow_missing_init_opts: False\n16:50:29 |     area_under_curve_class: None\n16:50:29 |     area_under_curve_digits: -1\n16:50:29 |     attention_dropout: 0.1\n16:50:29 |     batchsize: 40\n16:50:29 |     betas: '[0.9, 0.999]'\n16:50:29 |     bpe_add_prefix_space: None\n16:50:29 |     bpe_debug: False\n16:50:29 |     bpe_dropout: None\n16:50:29 |     bpe_merge: None\n16:50:29 |     bpe_vocab: None\n16:50:29 |     candidates: inline\n16:50:29 |     cap_num_predictions: 100\n16:50:29 |     checkpoint_activations: False\n16:50:29 |     class_weights: None\n16:50:29 |     classes: \"['__notok__', '__ok__']\"\n16:50:29 |     classes_from_file: None\n16:50:29 |     data_parallel: True\n16:50:29 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:50:29 |     datatype: train\n16:50:29 |     delimiter: '\\n'\n16:50:29 |     dict_class: parlai.core.dict:DictionaryAgent\n16:50:29 |     dict_endtoken: __start__\n16:50:29 |     dict_file: /tmp/model4.dict\n16:50:29 |     dict_include_test: False\n16:50:29 |     dict_include_valid: False\n16:50:29 |     dict_initpath: None\n16:50:29 |     dict_language: english\n16:50:29 |     dict_loaded: True\n16:50:29 |     dict_lower: True\n16:50:29 |     dict_max_ngram_size: -1\n16:50:29 |     dict_maxexs: -1\n16:50:29 |     dict_maxtokens: -1\n16:50:29 |     dict_minfreq: 0\n16:50:29 |     dict_nulltoken: __null__\n16:50:29 |     dict_starttoken: __start__\n16:50:29 |     dict_textfields: text,labels\n16:50:29 |     dict_tokenizer: bpe\n16:50:29 |     dict_unktoken: __unk__\n16:50:29 |     display_examples: False\n16:50:29 |     download_path: None\n16:50:29 |     dropout: 0.1\n16:50:29 |     dynamic_batching: None\n16:50:29 |     embedding_projection: random\n16:50:29 |     embedding_size: 768\n16:50:29 |     embedding_type: random\n16:50:29 |     embeddings_scale: False\n16:50:29 |     encode_candidate_vecs: True\n16:50:29 |     encode_candidate_vecs_batchsize: 256\n16:50:29 |     eval_batchsize: None\n16:50:29 |     eval_candidates: inline\n16:50:29 |     eval_dynamic_batching: None\n16:50:29 |     evaltask: None\n16:50:29 |     ffn_size: 3072\n16:50:29 |     final_extra_opt: \n16:50:29 |     fixed_candidate_vecs: reuse\n16:50:29 |     fixed_candidates_path: None\n16:50:29 |     force_fp16_tokens: True\n16:50:29 |     fp16: True\n16:50:29 |     fp16_impl: safe\n16:50:29 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-b.txt\n16:50:29 |     fromfile_datatype_extension: False\n16:50:29 |     gpu: -1\n16:50:29 |     gradient_clip: 0.1\n16:50:29 |     hide_labels: False\n16:50:29 |     history_add_global_end_token: None\n16:50:29 |     history_reversed: False\n16:50:29 |     history_size: 20\n16:50:29 |     ignore_bad_candidates: False\n16:50:29 |     ignore_labels: None\n16:50:29 |     image_cropsize: 224\n16:50:29 |     image_mode: raw\n16:50:29 |     image_size: 256\n16:50:29 |     inference: max\n16:50:29 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:50:29 |     init_opt: None\n16:50:29 |     interactive_candidates: fixed\n16:50:29 |     interactive_mode: False\n16:50:29 |     invsqrt_lr_decay_gamma: -1\n16:50:29 |     is_debug: False\n16:50:29 |     label_truncate: 72\n16:50:29 |     learn_embeddings: True\n16:50:29 |     learn_positional_embeddings: True\n16:50:29 |     learningrate: 5e-05\n16:50:29 |     load_from_pretrained_ranker: True\n16:50:29 |     log_every_n_secs: 10.0\n16:50:29 |     log_every_n_steps: 50\n16:50:29 |     log_keep_fields: all\n16:50:29 |     loglevel: info\n16:50:29 |     lr_scheduler: reduceonplateau\n16:50:29 |     lr_scheduler_decay: 0.5\n16:50:29 |     lr_scheduler_patience: 3\n16:50:29 |     max_train_steps: -1\n16:50:29 |     max_train_time: 7200.0\n16:50:29 |     memory_attention: sqrt\n16:50:29 |     metrics: default\n16:50:29 |     model: transformer/classifier\n16:50:29 |     model_file: /tmp/model4\n16:50:29 |     model_parallel: False\n16:50:29 |     momentum: 0\n16:50:29 |     multitask_weights: [1]\n16:50:29 |     mutators: None\n16:50:29 |     n_decoder_layers: -1\n16:50:29 |     n_encoder_layers: -1\n16:50:29 |     n_heads: 12\n16:50:29 |     n_layers: 12\n16:50:29 |     n_positions: 1024\n16:50:29 |     n_segments: 2\n16:50:29 |     nesterov: True\n16:50:29 |     no_cuda: False\n16:50:29 |     normalize_sent_emb: False\n16:50:29 |     num_epochs: -1\n16:50:29 |     num_examples: -1\n16:50:29 |     num_workers: 0\n16:50:29 |     nus: [0.7]\n16:50:29 |     optimizer: adamax\n16:50:29 |     output_scaling: 0.06\n16:50:29 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n16:50:29 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:50:29 |     person_tokens: False\n16:50:29 |     print_scores: False\n16:50:29 |     rank_candidates: False\n16:50:29 |     rank_top_k: -1\n16:50:29 |     reduction_type: mean\n16:50:29 |     ref_class: None\n16:50:29 |     relu_dropout: 0.0\n16:50:29 |     repeat_blocking_heuristic: True\n16:50:29 |     report_filename: \n16:50:29 |     return_cand_scores: False\n16:50:29 |     save_after_valid: True\n16:50:29 |     save_every_n_secs: -1\n16:50:29 |     save_format: conversations\n16:50:29 |     share_encoders: False\n16:50:29 |     share_word_embeddings: False\n16:50:29 |     short_final_eval: False\n16:50:29 |     special_tok_lst: None\n16:50:29 |     split_lines: False\n16:50:29 |     starttime: Dec03_16-48\n16:50:29 |     task: fromfile:parlaiformat\n16:50:29 |     tensorboard_log: False\n16:50:29 |     tensorboard_logdir: None\n16:50:29 |     text_truncate: 360\n16:50:29 |     threshold: 0.5\n16:50:29 |     topk: 5\n16:50:29 |     train_predict: False\n16:50:29 |     truncate: 1024\n16:50:29 |     update_classifier_head_only: False\n16:50:29 |     update_freq: 1\n16:50:29 |     use_memories: False\n16:50:29 |     use_reply: none\n16:50:29 |     validation_cutoff: 1.0\n16:50:29 |     validation_every_n_epochs: -1\n16:50:29 |     validation_every_n_secs: 20.0\n16:50:29 |     validation_every_n_steps: -1\n16:50:29 |     validation_max_exs: -1\n16:50:29 |     validation_metric: accuracy\n16:50:29 |     validation_metric_mode: max\n16:50:29 |     validation_patience: 30\n16:50:29 |     validation_share_agent: False\n16:50:29 |     variant: xlm\n16:50:29 |     verbose: False\n16:50:29 |     wandb_entity: None\n16:50:29 |     wandb_log: False\n16:50:29 |     wandb_name: None\n16:50:29 |     wandb_project: None\n16:50:29 |     warmup_rate: 0.0001\n16:50:29 |     warmup_updates: 1000\n16:50:29 |     weight_decay: None\n16:50:29 |     world_logs: \n16:50:29 |     wrap_memory_encoder: False\n16:50:30 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:50:30 | creating task(s): fromfile:parlaiformat\n16:50:30 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:50:30 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run4/data_train-b.txt\n16:50:36 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7100 7.1e-10               .6705                 .7867   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5842            .7411              .6640   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8384 11.46 538.2 436.4       0          0 32.44  200 .7100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .6585 2.905e-06 240.4 194.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 778.6 631.4        .7054\u001b[0m\n16:50:36 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7100 7.1e-10               .6705                 .7867   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5842            .7411              .6640   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8384 11.46 538.2 436.4       0          0 32.44  200 .7100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .6585 2.905e-06 240.4 194.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 778.6 631.4        .7054\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:50:38.099905Z","iopub.execute_input":"2022-12-03T16:50:38.100295Z","iopub.status.idle":"2022-12-03T16:50:39.208275Z","shell.execute_reply.started":"2022-12-03T16:50:38.100252Z","shell.execute_reply":"2022-12-03T16:50:39.206963Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:50:39.211078Z","iopub.execute_input":"2022-12-03T16:50:39.211525Z","iopub.status.idle":"2022-12-03T16:51:56.595429Z","shell.execute_reply.started":"2022-12-03T16:50:39.211463Z","shell.execute_reply":"2022-12-03T16:51:56.594194Z"},"scrolled":true,"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"16:50:46 | building dictionary first...\n16:50:46 | No model with opt yet at: /tmp/model5(.opt)\n16:50:46 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:50:46 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:50:46 | Using CUDA\n16:50:46 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:50:46 | num words = 54944\n16:50:50 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:50:57 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:50:57 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:50:57 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:50:57 | Opt:\n16:50:57 |     activation: gelu\n16:50:57 |     adafactor_eps: '(1e-30, 0.001)'\n16:50:57 |     adam_eps: 1e-08\n16:50:57 |     add_p1_after_newln: False\n16:50:57 |     aggregate_micro: False\n16:50:57 |     allow_missing_init_opts: False\n16:50:57 |     attention_dropout: 0.1\n16:50:57 |     batchsize: 20\n16:50:57 |     betas: '(0.9, 0.999)'\n16:50:57 |     bpe_add_prefix_space: None\n16:50:57 |     bpe_debug: False\n16:50:57 |     bpe_dropout: None\n16:50:57 |     bpe_merge: None\n16:50:57 |     bpe_vocab: None\n16:50:57 |     candidates: inline\n16:50:57 |     cap_num_predictions: 100\n16:50:57 |     checkpoint_activations: False\n16:50:57 |     class_weights: None\n16:50:57 |     classes: \"['__notok__', '__ok__']\"\n16:50:57 |     classes_from_file: None\n16:50:57 |     data_parallel: True\n16:50:57 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:50:57 |     datatype: train\n16:50:57 |     delimiter: '\\n'\n16:50:57 |     dict_class: parlai.core.dict:DictionaryAgent\n16:50:57 |     dict_endtoken: __start__\n16:50:57 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:50:57 |     dict_include_test: False\n16:50:57 |     dict_include_valid: False\n16:50:57 |     dict_initpath: None\n16:50:57 |     dict_language: english\n16:50:57 |     dict_loaded: True\n16:50:57 |     dict_lower: True\n16:50:57 |     dict_max_ngram_size: -1\n16:50:57 |     dict_maxexs: -1\n16:50:57 |     dict_maxtokens: -1\n16:50:57 |     dict_minfreq: 0\n16:50:57 |     dict_nulltoken: __null__\n16:50:57 |     dict_starttoken: __start__\n16:50:57 |     dict_textfields: text,labels\n16:50:57 |     dict_tokenizer: bpe\n16:50:57 |     dict_unktoken: __unk__\n16:50:57 |     display_examples: False\n16:50:57 |     download_path: None\n16:50:57 |     dropout: 0.1\n16:50:57 |     dynamic_batching: None\n16:50:57 |     embedding_projection: random\n16:50:57 |     embedding_size: 768\n16:50:57 |     embedding_type: random\n16:50:57 |     embeddings_scale: False\n16:50:57 |     encode_candidate_vecs: True\n16:50:57 |     encode_candidate_vecs_batchsize: 256\n16:50:57 |     eval_batchsize: None\n16:50:57 |     eval_candidates: inline\n16:50:57 |     eval_dynamic_batching: None\n16:50:57 |     evaltask: None\n16:50:57 |     ffn_size: 3072\n16:50:57 |     final_extra_opt: \n16:50:57 |     fixed_candidate_vecs: reuse\n16:50:57 |     fixed_candidates_path: None\n16:50:57 |     force_fp16_tokens: False\n16:50:57 |     fp16: True\n16:50:57 |     fp16_impl: safe\n16:50:57 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt\n16:50:57 |     fromfile_datatype_extension: False\n16:50:57 |     gpu: -1\n16:50:57 |     gradient_clip: 0.1\n16:50:57 |     hide_labels: False\n16:50:57 |     history_add_global_end_token: None\n16:50:57 |     history_reversed: False\n16:50:57 |     history_size: 20\n16:50:57 |     ignore_bad_candidates: False\n16:50:57 |     ignore_labels: None\n16:50:57 |     image_cropsize: 224\n16:50:57 |     image_mode: raw\n16:50:57 |     image_size: 256\n16:50:57 |     inference: max\n16:50:57 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:50:57 |     init_opt: None\n16:50:57 |     interactive_candidates: fixed\n16:50:57 |     interactive_mode: False\n16:50:57 |     invsqrt_lr_decay_gamma: -1\n16:50:57 |     is_debug: False\n16:50:57 |     label_truncate: 72\n16:50:57 |     learn_embeddings: True\n16:50:57 |     learn_positional_embeddings: True\n16:50:57 |     learningrate: 5e-05\n16:50:57 |     load_from_checkpoint: False\n16:50:57 |     load_from_pretrained_ranker: True\n16:50:57 |     log_every_n_secs: 10.0\n16:50:57 |     log_every_n_steps: 50\n16:50:57 |     log_keep_fields: all\n16:50:57 |     loglevel: info\n16:50:57 |     lr_scheduler: reduceonplateau\n16:50:57 |     lr_scheduler_decay: 0.5\n16:50:57 |     lr_scheduler_patience: 3\n16:50:57 |     max_train_steps: -1\n16:50:57 |     max_train_time: 7200.0\n16:50:57 |     memory_attention: sqrt\n16:50:57 |     metrics: default\n16:50:57 |     model: transformer/classifier\n16:50:57 |     model_file: /tmp/model5\n16:50:57 |     model_parallel: False\n16:50:57 |     momentum: 0\n16:50:57 |     multitask_weights: [1]\n16:50:57 |     mutators: None\n16:50:57 |     n_decoder_layers: -1\n16:50:57 |     n_encoder_layers: -1\n16:50:57 |     n_heads: 12\n16:50:57 |     n_layers: 12\n16:50:57 |     n_positions: 1024\n16:50:57 |     n_segments: 2\n16:50:57 |     nesterov: True\n16:50:57 |     no_cuda: False\n16:50:57 |     normalize_sent_emb: False\n16:50:57 |     num_epochs: -1\n16:50:57 |     num_workers: 0\n16:50:57 |     nus: (0.7,)\n16:50:57 |     optimizer: adamax\n16:50:57 |     output_scaling: 0.06\n16:50:57 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n16:50:57 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:50:57 |     person_tokens: False\n16:50:57 |     print_scores: False\n16:50:57 |     rank_candidates: False\n16:50:57 |     rank_top_k: -1\n16:50:57 |     reduction_type: mean\n16:50:57 |     ref_class: None\n16:50:57 |     relu_dropout: 0.0\n16:50:57 |     repeat_blocking_heuristic: True\n16:50:57 |     return_cand_scores: False\n16:50:57 |     save_after_valid: True\n16:50:57 |     save_every_n_secs: -1\n16:50:57 |     save_format: conversations\n16:50:57 |     share_encoders: False\n16:50:57 |     share_word_embeddings: False\n16:50:57 |     short_final_eval: False\n16:50:57 |     special_tok_lst: None\n16:50:57 |     split_lines: False\n16:50:57 |     starttime: Dec03_16-50\n16:50:57 |     task: fromfile:parlaiformat\n16:50:57 |     tensorboard_log: False\n16:50:57 |     tensorboard_logdir: None\n16:50:57 |     text_truncate: 360\n16:50:57 |     threshold: 0.5\n16:50:57 |     topk: 5\n16:50:57 |     train_predict: False\n16:50:57 |     truncate: 1024\n16:50:57 |     update_classifier_head_only: False\n16:50:57 |     update_freq: 1\n16:50:57 |     use_memories: False\n16:50:57 |     use_reply: none\n16:50:57 |     validation_cutoff: 1.0\n16:50:57 |     validation_every_n_epochs: -1\n16:50:57 |     validation_every_n_secs: 20.0\n16:50:57 |     validation_every_n_steps: -1\n16:50:57 |     validation_max_exs: -1\n16:50:57 |     validation_metric: accuracy\n16:50:57 |     validation_metric_mode: max\n16:50:57 |     validation_patience: 30\n16:50:57 |     validation_share_agent: False\n16:50:57 |     variant: xlm\n16:50:57 |     verbose: False\n16:50:57 |     wandb_entity: None\n16:50:57 |     wandb_log: False\n16:50:57 |     wandb_name: None\n16:50:57 |     wandb_project: None\n16:50:57 |     warmup_rate: 0.0001\n16:50:57 |     warmup_updates: 1000\n16:50:57 |     weight_decay: None\n16:50:57 |     world_logs: \n16:50:57 |     wrap_memory_encoder: False\n16:50:57 | creating task(s): fromfile:parlaiformat\n16:50:57 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt\n16:50:57 | training...\n16:51:07 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6625 6.625e-10               .6868                 .6491   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7291            .6341              .6802   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5939 10.78     1 255.6 510.9       0          0 39.98  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6625             32768  2.786    .1206 6.015 .6773 1.005e-06 120.3 240.5   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 375.9 751.4 2.004        .6609\n\n16:51:17 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8667 8.667e-10               .8729                 .8460   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9015            .8598              .8911   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8307 10.51     1 250.2 984.5       0          0  78.7  780   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8667             32768  2.456    .1207 6.015 .6279 2.955e-06 120.3 473.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   59 370.5 1458 3.944        .8664\n\n16:51:17 | creating task(s): fromfile:parlaiformat\n16:51:17 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:51:17 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt\n16:51:17 | running eval: valid\n16:51:18 | eval completed in 0.20s\n16:51:18 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9600                 .9231   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9565                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 10.67   152  1795       0          0 141.7   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5599 2.955e-06    72 850.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  224 2645        .9583\n\u001b[0m\n16:51:18 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n16:51:18 | saving best valid model: /tmp/model5\n16:51:18 | Saving dictionary to /tmp/model5.dict\n16:51:21 | saving model checkpoint: /tmp/model5.checkpoint\n16:51:21 | Saving dictionary to /tmp/model5.checkpoint.dict\n16:51:39 | time:42s total_exs:1880 total_steps:94 epochs:78.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9929 9.929e-10               .9931                 .9863   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9926                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9853 10.45     1 248.9 854.6       0          0 68.66  700   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9929             32768   2.55    .1207 6.029 .4998 4.705e-06 120.6 413.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   94 369.5 1269 3.441        .9929\n\n16:51:42 | time:44s total_exs:2060 total_steps:103 epochs:85.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 10.76     1 255.2 944.8       0          0 74.03  180   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  3.118    .1207 6.022 .3841 5.154e-06 120.4 445.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  103 375.7 1391 3.736            1\n\n16:51:42 | running eval: valid\n16:51:42 | eval completed in 0.19s\n16:51:42 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1765       0          0 139.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3247 5.154e-06    72   836       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    103  224 2601            1\n\u001b[0m\n16:51:42 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n16:51:42 | saving best valid model: /tmp/model5\n16:51:46 | task solved! stopping.\n16:51:46 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:51:46 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:51:46 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:51:46 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:51:46 | Using CUDA\n16:51:46 | loading dictionary from /tmp/model5.dict\n16:51:47 | num words = 54944\n16:51:51 | Loading existing model parameters from /tmp/model5\n16:51:53 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:51:54 | creating task(s): fromfile:parlaiformat\n16:51:54 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:51:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt\n16:51:54 | running eval: valid\n16:51:54 | eval completed in 0.20s\n16:51:54 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1711       0          0   135   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3247 5.154e-06    72 810.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    103  224 2521            1\n\u001b[0m\n16:51:54 | creating task(s): fromfile:parlaiformat\n16:51:54 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:51:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt\n16:51:54 | running eval: test\n16:51:55 | eval completed in 0.20s\n16:51:55 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1720       0          0 135.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3247 5.154e-06    72 814.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    103  224 2534            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:51:56.597565Z","iopub.execute_input":"2022-12-03T16:51:56.597946Z","iopub.status.idle":"2022-12-03T16:52:24.880994Z","shell.execute_reply.started":"2022-12-03T16:51:56.597906Z","shell.execute_reply":"2022-12-03T16:52:24.879781Z"},"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"16:52:03 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt)\u001b[0m\n16:52:03 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:52:03 | Using CUDA\n16:52:03 | loading dictionary from /tmp/model5.dict\n16:52:03 | num words = 54944\n16:52:08 | Loading existing model parameters from /tmp/model5\n16:52:16 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:52:17 | Opt:\n16:52:17 |     activation: gelu\n16:52:17 |     adafactor_eps: '[1e-30, 0.001]'\n16:52:17 |     adam_eps: 1e-08\n16:52:17 |     add_p1_after_newln: False\n16:52:17 |     aggregate_micro: False\n16:52:17 |     allow_missing_init_opts: False\n16:52:17 |     area_under_curve_class: None\n16:52:17 |     area_under_curve_digits: -1\n16:52:17 |     attention_dropout: 0.1\n16:52:17 |     batchsize: 40\n16:52:17 |     betas: '[0.9, 0.999]'\n16:52:17 |     bpe_add_prefix_space: None\n16:52:17 |     bpe_debug: False\n16:52:17 |     bpe_dropout: None\n16:52:17 |     bpe_merge: None\n16:52:17 |     bpe_vocab: None\n16:52:17 |     candidates: inline\n16:52:17 |     cap_num_predictions: 100\n16:52:17 |     checkpoint_activations: False\n16:52:17 |     class_weights: None\n16:52:17 |     classes: \"['__notok__', '__ok__']\"\n16:52:17 |     classes_from_file: None\n16:52:17 |     data_parallel: True\n16:52:17 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:52:17 |     datatype: train\n16:52:17 |     delimiter: '\\n'\n16:52:17 |     dict_class: parlai.core.dict:DictionaryAgent\n16:52:17 |     dict_endtoken: __start__\n16:52:17 |     dict_file: /tmp/model5.dict\n16:52:17 |     dict_include_test: False\n16:52:17 |     dict_include_valid: False\n16:52:17 |     dict_initpath: None\n16:52:17 |     dict_language: english\n16:52:17 |     dict_loaded: True\n16:52:17 |     dict_lower: True\n16:52:17 |     dict_max_ngram_size: -1\n16:52:17 |     dict_maxexs: -1\n16:52:17 |     dict_maxtokens: -1\n16:52:17 |     dict_minfreq: 0\n16:52:17 |     dict_nulltoken: __null__\n16:52:17 |     dict_starttoken: __start__\n16:52:17 |     dict_textfields: text,labels\n16:52:17 |     dict_tokenizer: bpe\n16:52:17 |     dict_unktoken: __unk__\n16:52:17 |     display_examples: False\n16:52:17 |     download_path: None\n16:52:17 |     dropout: 0.1\n16:52:17 |     dynamic_batching: None\n16:52:17 |     embedding_projection: random\n16:52:17 |     embedding_size: 768\n16:52:17 |     embedding_type: random\n16:52:17 |     embeddings_scale: False\n16:52:17 |     encode_candidate_vecs: True\n16:52:17 |     encode_candidate_vecs_batchsize: 256\n16:52:17 |     eval_batchsize: None\n16:52:17 |     eval_candidates: inline\n16:52:17 |     eval_dynamic_batching: None\n16:52:17 |     evaltask: None\n16:52:17 |     ffn_size: 3072\n16:52:17 |     final_extra_opt: \n16:52:17 |     fixed_candidate_vecs: reuse\n16:52:17 |     fixed_candidates_path: None\n16:52:17 |     force_fp16_tokens: True\n16:52:17 |     fp16: True\n16:52:17 |     fp16_impl: safe\n16:52:17 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-a.txt\n16:52:17 |     fromfile_datatype_extension: False\n16:52:17 |     gpu: -1\n16:52:17 |     gradient_clip: 0.1\n16:52:17 |     hide_labels: False\n16:52:17 |     history_add_global_end_token: None\n16:52:17 |     history_reversed: False\n16:52:17 |     history_size: 20\n16:52:17 |     ignore_bad_candidates: False\n16:52:17 |     ignore_labels: None\n16:52:17 |     image_cropsize: 224\n16:52:17 |     image_mode: raw\n16:52:17 |     image_size: 256\n16:52:17 |     inference: max\n16:52:17 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:52:17 |     init_opt: None\n16:52:17 |     interactive_candidates: fixed\n16:52:17 |     interactive_mode: False\n16:52:17 |     invsqrt_lr_decay_gamma: -1\n16:52:17 |     is_debug: False\n16:52:17 |     label_truncate: 72\n16:52:17 |     learn_embeddings: True\n16:52:17 |     learn_positional_embeddings: True\n16:52:17 |     learningrate: 5e-05\n16:52:17 |     load_from_pretrained_ranker: True\n16:52:17 |     log_every_n_secs: 10.0\n16:52:17 |     log_every_n_steps: 50\n16:52:17 |     log_keep_fields: all\n16:52:17 |     loglevel: info\n16:52:17 |     lr_scheduler: reduceonplateau\n16:52:17 |     lr_scheduler_decay: 0.5\n16:52:17 |     lr_scheduler_patience: 3\n16:52:17 |     max_train_steps: -1\n16:52:17 |     max_train_time: 7200.0\n16:52:17 |     memory_attention: sqrt\n16:52:17 |     metrics: default\n16:52:17 |     model: transformer/classifier\n16:52:17 |     model_file: /tmp/model5\n16:52:17 |     model_parallel: False\n16:52:17 |     momentum: 0\n16:52:17 |     multitask_weights: [1]\n16:52:17 |     mutators: None\n16:52:17 |     n_decoder_layers: -1\n16:52:17 |     n_encoder_layers: -1\n16:52:17 |     n_heads: 12\n16:52:17 |     n_layers: 12\n16:52:17 |     n_positions: 1024\n16:52:17 |     n_segments: 2\n16:52:17 |     nesterov: True\n16:52:17 |     no_cuda: False\n16:52:17 |     normalize_sent_emb: False\n16:52:17 |     num_epochs: -1\n16:52:17 |     num_examples: -1\n16:52:17 |     num_workers: 0\n16:52:17 |     nus: [0.7]\n16:52:17 |     optimizer: adamax\n16:52:17 |     output_scaling: 0.06\n16:52:17 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n16:52:17 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:52:17 |     person_tokens: False\n16:52:17 |     print_scores: False\n16:52:17 |     rank_candidates: False\n16:52:17 |     rank_top_k: -1\n16:52:17 |     reduction_type: mean\n16:52:17 |     ref_class: None\n16:52:17 |     relu_dropout: 0.0\n16:52:17 |     repeat_blocking_heuristic: True\n16:52:17 |     report_filename: \n16:52:17 |     return_cand_scores: False\n16:52:17 |     save_after_valid: True\n16:52:17 |     save_every_n_secs: -1\n16:52:17 |     save_format: conversations\n16:52:17 |     share_encoders: False\n16:52:17 |     share_word_embeddings: False\n16:52:17 |     short_final_eval: False\n16:52:17 |     special_tok_lst: None\n16:52:17 |     split_lines: False\n16:52:17 |     starttime: Dec03_16-50\n16:52:17 |     task: fromfile:parlaiformat\n16:52:17 |     tensorboard_log: False\n16:52:17 |     tensorboard_logdir: None\n16:52:17 |     text_truncate: 360\n16:52:17 |     threshold: 0.5\n16:52:17 |     topk: 5\n16:52:17 |     train_predict: False\n16:52:17 |     truncate: 1024\n16:52:17 |     update_classifier_head_only: False\n16:52:17 |     update_freq: 1\n16:52:17 |     use_memories: False\n16:52:17 |     use_reply: none\n16:52:17 |     validation_cutoff: 1.0\n16:52:17 |     validation_every_n_epochs: -1\n16:52:17 |     validation_every_n_secs: 20.0\n16:52:17 |     validation_every_n_steps: -1\n16:52:17 |     validation_max_exs: -1\n16:52:17 |     validation_metric: accuracy\n16:52:17 |     validation_metric_mode: max\n16:52:17 |     validation_patience: 30\n16:52:17 |     validation_share_agent: False\n16:52:17 |     variant: xlm\n16:52:17 |     verbose: False\n16:52:17 |     wandb_entity: None\n16:52:17 |     wandb_log: False\n16:52:17 |     wandb_name: None\n16:52:17 |     wandb_project: None\n16:52:17 |     warmup_rate: 0.0001\n16:52:17 |     warmup_updates: 1000\n16:52:17 |     weight_decay: None\n16:52:17 |     world_logs: \n16:52:17 |     wrap_memory_encoder: False\n16:52:17 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:52:17 | creating task(s): fromfile:parlaiformat\n16:52:17 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:52:17 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-a.txt\n16:52:23 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2000   2e-10               .2157                 .2095   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2222            .1837              .1895   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1782 11.79 551.8 527.7       0          0 38.25  200 .2000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .8739 5.154e-06 239.6 229.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 791.4 756.8        .1995\u001b[0m\n16:52:23 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2000   2e-10               .2157                 .2095   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2222            .1837              .1895   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1782 11.79 551.8 527.7       0          0 38.25  200 .2000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .8739 5.154e-06 239.6 229.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 791.4 756.8        .1995\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:52:24.886224Z","iopub.execute_input":"2022-12-03T16:52:24.888687Z","iopub.status.idle":"2022-12-03T16:52:51.316655Z","shell.execute_reply.started":"2022-12-03T16:52:24.888645Z","shell.execute_reply":"2022-12-03T16:52:51.315436Z"},"scrolled":true,"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"16:52:32 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_valid.txt)\u001b[0m\n16:52:32 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:52:32 | Using CUDA\n16:52:32 | loading dictionary from /tmp/model5.dict\n16:52:32 | num words = 54944\n16:52:36 | Loading existing model parameters from /tmp/model5\n16:52:42 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:52:43 | Opt:\n16:52:43 |     activation: gelu\n16:52:43 |     adafactor_eps: '[1e-30, 0.001]'\n16:52:43 |     adam_eps: 1e-08\n16:52:43 |     add_p1_after_newln: False\n16:52:43 |     aggregate_micro: False\n16:52:43 |     allow_missing_init_opts: False\n16:52:43 |     area_under_curve_class: None\n16:52:43 |     area_under_curve_digits: -1\n16:52:43 |     attention_dropout: 0.1\n16:52:43 |     batchsize: 40\n16:52:43 |     betas: '[0.9, 0.999]'\n16:52:43 |     bpe_add_prefix_space: None\n16:52:43 |     bpe_debug: False\n16:52:43 |     bpe_dropout: None\n16:52:43 |     bpe_merge: None\n16:52:43 |     bpe_vocab: None\n16:52:43 |     candidates: inline\n16:52:43 |     cap_num_predictions: 100\n16:52:43 |     checkpoint_activations: False\n16:52:43 |     class_weights: None\n16:52:43 |     classes: \"['__notok__', '__ok__']\"\n16:52:43 |     classes_from_file: None\n16:52:43 |     data_parallel: True\n16:52:43 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:52:43 |     datatype: train\n16:52:43 |     delimiter: '\\n'\n16:52:43 |     dict_class: parlai.core.dict:DictionaryAgent\n16:52:43 |     dict_endtoken: __start__\n16:52:43 |     dict_file: /tmp/model5.dict\n16:52:43 |     dict_include_test: False\n16:52:43 |     dict_include_valid: False\n16:52:43 |     dict_initpath: None\n16:52:43 |     dict_language: english\n16:52:43 |     dict_loaded: True\n16:52:43 |     dict_lower: True\n16:52:43 |     dict_max_ngram_size: -1\n16:52:43 |     dict_maxexs: -1\n16:52:43 |     dict_maxtokens: -1\n16:52:43 |     dict_minfreq: 0\n16:52:43 |     dict_nulltoken: __null__\n16:52:43 |     dict_starttoken: __start__\n16:52:43 |     dict_textfields: text,labels\n16:52:43 |     dict_tokenizer: bpe\n16:52:43 |     dict_unktoken: __unk__\n16:52:43 |     display_examples: False\n16:52:43 |     download_path: None\n16:52:43 |     dropout: 0.1\n16:52:43 |     dynamic_batching: None\n16:52:43 |     embedding_projection: random\n16:52:43 |     embedding_size: 768\n16:52:43 |     embedding_type: random\n16:52:43 |     embeddings_scale: False\n16:52:43 |     encode_candidate_vecs: True\n16:52:43 |     encode_candidate_vecs_batchsize: 256\n16:52:43 |     eval_batchsize: None\n16:52:43 |     eval_candidates: inline\n16:52:43 |     eval_dynamic_batching: None\n16:52:43 |     evaltask: None\n16:52:43 |     ffn_size: 3072\n16:52:43 |     final_extra_opt: \n16:52:43 |     fixed_candidate_vecs: reuse\n16:52:43 |     fixed_candidates_path: None\n16:52:43 |     force_fp16_tokens: True\n16:52:43 |     fp16: True\n16:52:43 |     fp16_impl: safe\n16:52:43 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-b.txt\n16:52:43 |     fromfile_datatype_extension: False\n16:52:43 |     gpu: -1\n16:52:43 |     gradient_clip: 0.1\n16:52:43 |     hide_labels: False\n16:52:43 |     history_add_global_end_token: None\n16:52:43 |     history_reversed: False\n16:52:43 |     history_size: 20\n16:52:43 |     ignore_bad_candidates: False\n16:52:43 |     ignore_labels: None\n16:52:43 |     image_cropsize: 224\n16:52:43 |     image_mode: raw\n16:52:43 |     image_size: 256\n16:52:43 |     inference: max\n16:52:43 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:52:43 |     init_opt: None\n16:52:43 |     interactive_candidates: fixed\n16:52:43 |     interactive_mode: False\n16:52:43 |     invsqrt_lr_decay_gamma: -1\n16:52:43 |     is_debug: False\n16:52:43 |     label_truncate: 72\n16:52:43 |     learn_embeddings: True\n16:52:43 |     learn_positional_embeddings: True\n16:52:43 |     learningrate: 5e-05\n16:52:43 |     load_from_pretrained_ranker: True\n16:52:43 |     log_every_n_secs: 10.0\n16:52:43 |     log_every_n_steps: 50\n16:52:43 |     log_keep_fields: all\n16:52:43 |     loglevel: info\n16:52:43 |     lr_scheduler: reduceonplateau\n16:52:43 |     lr_scheduler_decay: 0.5\n16:52:43 |     lr_scheduler_patience: 3\n16:52:43 |     max_train_steps: -1\n16:52:43 |     max_train_time: 7200.0\n16:52:43 |     memory_attention: sqrt\n16:52:43 |     metrics: default\n16:52:43 |     model: transformer/classifier\n16:52:43 |     model_file: /tmp/model5\n16:52:43 |     model_parallel: False\n16:52:43 |     momentum: 0\n16:52:43 |     multitask_weights: [1]\n16:52:43 |     mutators: None\n16:52:43 |     n_decoder_layers: -1\n16:52:43 |     n_encoder_layers: -1\n16:52:43 |     n_heads: 12\n16:52:43 |     n_layers: 12\n16:52:43 |     n_positions: 1024\n16:52:43 |     n_segments: 2\n16:52:43 |     nesterov: True\n16:52:43 |     no_cuda: False\n16:52:43 |     normalize_sent_emb: False\n16:52:43 |     num_epochs: -1\n16:52:43 |     num_examples: -1\n16:52:43 |     num_workers: 0\n16:52:43 |     nus: [0.7]\n16:52:43 |     optimizer: adamax\n16:52:43 |     output_scaling: 0.06\n16:52:43 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n16:52:43 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:52:43 |     person_tokens: False\n16:52:43 |     print_scores: False\n16:52:43 |     rank_candidates: False\n16:52:43 |     rank_top_k: -1\n16:52:43 |     reduction_type: mean\n16:52:43 |     ref_class: None\n16:52:43 |     relu_dropout: 0.0\n16:52:43 |     repeat_blocking_heuristic: True\n16:52:43 |     report_filename: \n16:52:43 |     return_cand_scores: False\n16:52:43 |     save_after_valid: True\n16:52:43 |     save_every_n_secs: -1\n16:52:43 |     save_format: conversations\n16:52:43 |     share_encoders: False\n16:52:43 |     share_word_embeddings: False\n16:52:43 |     short_final_eval: False\n16:52:43 |     special_tok_lst: None\n16:52:43 |     split_lines: False\n16:52:43 |     starttime: Dec03_16-50\n16:52:43 |     task: fromfile:parlaiformat\n16:52:43 |     tensorboard_log: False\n16:52:43 |     tensorboard_logdir: None\n16:52:43 |     text_truncate: 360\n16:52:43 |     threshold: 0.5\n16:52:43 |     topk: 5\n16:52:43 |     train_predict: False\n16:52:43 |     truncate: 1024\n16:52:43 |     update_classifier_head_only: False\n16:52:43 |     update_freq: 1\n16:52:43 |     use_memories: False\n16:52:43 |     use_reply: none\n16:52:43 |     validation_cutoff: 1.0\n16:52:43 |     validation_every_n_epochs: -1\n16:52:43 |     validation_every_n_secs: 20.0\n16:52:43 |     validation_every_n_steps: -1\n16:52:43 |     validation_max_exs: -1\n16:52:43 |     validation_metric: accuracy\n16:52:43 |     validation_metric_mode: max\n16:52:43 |     validation_patience: 30\n16:52:43 |     validation_share_agent: False\n16:52:43 |     variant: xlm\n16:52:43 |     verbose: False\n16:52:43 |     wandb_entity: None\n16:52:43 |     wandb_log: False\n16:52:43 |     wandb_name: None\n16:52:43 |     wandb_project: None\n16:52:43 |     warmup_rate: 0.0001\n16:52:43 |     warmup_updates: 1000\n16:52:43 |     weight_decay: None\n16:52:43 |     world_logs: \n16:52:43 |     wrap_memory_encoder: False\n16:52:44 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:52:44 | creating task(s): fromfile:parlaiformat\n16:52:44 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:52:44 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr1type2/run5/data_train-b.txt\n16:52:49 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8000   8e-10               .8058                 .7905   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8218            .7938              .8105   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7778 11.79 551.8 519.2       0          0 37.64  200 .8000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5680 5.154e-06 240.4 226.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 792.2 745.4        .7999\u001b[0m\n16:52:49 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8000   8e-10               .8058                 .7905   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8218            .7938              .8105   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7778 11.79 551.8 519.2       0          0 37.64  200 .8000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5680 5.154e-06 240.4 226.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 792.2 745.4        .7999\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:52:51.318686Z","iopub.execute_input":"2022-12-03T16:52:51.319079Z","iopub.status.idle":"2022-12-03T16:52:52.461811Z","shell.execute_reply.started":"2022-12-03T16:52:51.319040Z","shell.execute_reply":"2022-12-03T16:52:52.460501Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Choose completion prev1corr2type1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:52:52.470086Z","iopub.execute_input":"2022-12-03T16:52:52.470734Z","iopub.status.idle":"2022-12-03T16:53:48.110840Z","shell.execute_reply.started":"2022-12-03T16:52:52.470689Z","shell.execute_reply":"2022-12-03T16:53:48.109628Z"},"scrolled":true,"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"16:52:59 | building dictionary first...\n16:52:59 | No model with opt yet at: /tmp/model1(.opt)\n16:52:59 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:52:59 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:52:59 | Using CUDA\n16:52:59 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:52:59 | num words = 54944\n16:53:03 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:53:11 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:53:11 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:53:11 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:53:11 | Opt:\n16:53:11 |     activation: gelu\n16:53:11 |     adafactor_eps: '(1e-30, 0.001)'\n16:53:11 |     adam_eps: 1e-08\n16:53:11 |     add_p1_after_newln: False\n16:53:11 |     aggregate_micro: False\n16:53:11 |     allow_missing_init_opts: False\n16:53:11 |     attention_dropout: 0.1\n16:53:11 |     batchsize: 20\n16:53:11 |     betas: '(0.9, 0.999)'\n16:53:11 |     bpe_add_prefix_space: None\n16:53:11 |     bpe_debug: False\n16:53:11 |     bpe_dropout: None\n16:53:11 |     bpe_merge: None\n16:53:11 |     bpe_vocab: None\n16:53:11 |     candidates: inline\n16:53:11 |     cap_num_predictions: 100\n16:53:11 |     checkpoint_activations: False\n16:53:11 |     class_weights: None\n16:53:11 |     classes: \"['__notok__', '__ok__']\"\n16:53:11 |     classes_from_file: None\n16:53:11 |     data_parallel: True\n16:53:11 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:53:11 |     datatype: train\n16:53:11 |     delimiter: '\\n'\n16:53:11 |     dict_class: parlai.core.dict:DictionaryAgent\n16:53:11 |     dict_endtoken: __start__\n16:53:11 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:53:11 |     dict_include_test: False\n16:53:11 |     dict_include_valid: False\n16:53:11 |     dict_initpath: None\n16:53:11 |     dict_language: english\n16:53:11 |     dict_loaded: True\n16:53:11 |     dict_lower: True\n16:53:11 |     dict_max_ngram_size: -1\n16:53:11 |     dict_maxexs: -1\n16:53:11 |     dict_maxtokens: -1\n16:53:11 |     dict_minfreq: 0\n16:53:11 |     dict_nulltoken: __null__\n16:53:11 |     dict_starttoken: __start__\n16:53:11 |     dict_textfields: text,labels\n16:53:11 |     dict_tokenizer: bpe\n16:53:11 |     dict_unktoken: __unk__\n16:53:11 |     display_examples: False\n16:53:11 |     download_path: None\n16:53:11 |     dropout: 0.1\n16:53:11 |     dynamic_batching: None\n16:53:11 |     embedding_projection: random\n16:53:11 |     embedding_size: 768\n16:53:11 |     embedding_type: random\n16:53:11 |     embeddings_scale: False\n16:53:11 |     encode_candidate_vecs: True\n16:53:11 |     encode_candidate_vecs_batchsize: 256\n16:53:11 |     eval_batchsize: None\n16:53:11 |     eval_candidates: inline\n16:53:11 |     eval_dynamic_batching: None\n16:53:11 |     evaltask: None\n16:53:11 |     ffn_size: 3072\n16:53:11 |     final_extra_opt: \n16:53:11 |     fixed_candidate_vecs: reuse\n16:53:11 |     fixed_candidates_path: None\n16:53:11 |     force_fp16_tokens: False\n16:53:11 |     fp16: True\n16:53:11 |     fp16_impl: safe\n16:53:11 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt\n16:53:11 |     fromfile_datatype_extension: False\n16:53:11 |     gpu: -1\n16:53:11 |     gradient_clip: 0.1\n16:53:11 |     hide_labels: False\n16:53:11 |     history_add_global_end_token: None\n16:53:11 |     history_reversed: False\n16:53:11 |     history_size: 20\n16:53:11 |     ignore_bad_candidates: False\n16:53:11 |     ignore_labels: None\n16:53:11 |     image_cropsize: 224\n16:53:11 |     image_mode: raw\n16:53:11 |     image_size: 256\n16:53:11 |     inference: max\n16:53:11 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:53:11 |     init_opt: None\n16:53:11 |     interactive_candidates: fixed\n16:53:11 |     interactive_mode: False\n16:53:11 |     invsqrt_lr_decay_gamma: -1\n16:53:11 |     is_debug: False\n16:53:11 |     label_truncate: 72\n16:53:11 |     learn_embeddings: True\n16:53:11 |     learn_positional_embeddings: True\n16:53:11 |     learningrate: 5e-05\n16:53:11 |     load_from_checkpoint: False\n16:53:11 |     load_from_pretrained_ranker: True\n16:53:11 |     log_every_n_secs: 10.0\n16:53:11 |     log_every_n_steps: 50\n16:53:11 |     log_keep_fields: all\n16:53:11 |     loglevel: info\n16:53:11 |     lr_scheduler: reduceonplateau\n16:53:11 |     lr_scheduler_decay: 0.5\n16:53:11 |     lr_scheduler_patience: 3\n16:53:11 |     max_train_steps: -1\n16:53:11 |     max_train_time: 7200.0\n16:53:11 |     memory_attention: sqrt\n16:53:11 |     metrics: default\n16:53:11 |     model: transformer/classifier\n16:53:11 |     model_file: /tmp/model1\n16:53:11 |     model_parallel: False\n16:53:11 |     momentum: 0\n16:53:11 |     multitask_weights: [1]\n16:53:11 |     mutators: None\n16:53:11 |     n_decoder_layers: -1\n16:53:11 |     n_encoder_layers: -1\n16:53:11 |     n_heads: 12\n16:53:11 |     n_layers: 12\n16:53:11 |     n_positions: 1024\n16:53:11 |     n_segments: 2\n16:53:11 |     nesterov: True\n16:53:11 |     no_cuda: False\n16:53:11 |     normalize_sent_emb: False\n16:53:11 |     num_epochs: -1\n16:53:11 |     num_workers: 0\n16:53:11 |     nus: (0.7,)\n16:53:11 |     optimizer: adamax\n16:53:11 |     output_scaling: 0.06\n16:53:11 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n16:53:11 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:53:11 |     person_tokens: False\n16:53:11 |     print_scores: False\n16:53:11 |     rank_candidates: False\n16:53:11 |     rank_top_k: -1\n16:53:11 |     reduction_type: mean\n16:53:11 |     ref_class: None\n16:53:11 |     relu_dropout: 0.0\n16:53:11 |     repeat_blocking_heuristic: True\n16:53:11 |     return_cand_scores: False\n16:53:11 |     save_after_valid: True\n16:53:11 |     save_every_n_secs: -1\n16:53:11 |     save_format: conversations\n16:53:11 |     share_encoders: False\n16:53:11 |     share_word_embeddings: False\n16:53:11 |     short_final_eval: False\n16:53:11 |     special_tok_lst: None\n16:53:11 |     split_lines: False\n16:53:11 |     starttime: Dec03_16-52\n16:53:11 |     task: fromfile:parlaiformat\n16:53:11 |     tensorboard_log: False\n16:53:11 |     tensorboard_logdir: None\n16:53:11 |     text_truncate: 360\n16:53:11 |     threshold: 0.5\n16:53:11 |     topk: 5\n16:53:11 |     train_predict: False\n16:53:11 |     truncate: 1024\n16:53:11 |     update_classifier_head_only: False\n16:53:11 |     update_freq: 1\n16:53:11 |     use_memories: False\n16:53:11 |     use_reply: none\n16:53:11 |     validation_cutoff: 1.0\n16:53:11 |     validation_every_n_epochs: -1\n16:53:11 |     validation_every_n_secs: 20.0\n16:53:11 |     validation_every_n_steps: -1\n16:53:11 |     validation_max_exs: -1\n16:53:11 |     validation_metric: accuracy\n16:53:11 |     validation_metric_mode: max\n16:53:11 |     validation_patience: 30\n16:53:11 |     validation_share_agent: False\n16:53:11 |     variant: xlm\n16:53:11 |     verbose: False\n16:53:11 |     wandb_entity: None\n16:53:11 |     wandb_log: False\n16:53:11 |     wandb_name: None\n16:53:11 |     wandb_project: None\n16:53:11 |     warmup_rate: 0.0001\n16:53:11 |     warmup_updates: 1000\n16:53:11 |     weight_decay: None\n16:53:11 |     world_logs: \n16:53:11 |     wrap_memory_encoder: False\n16:53:11 | creating task(s): fromfile:parlaiformat\n16:53:11 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt\n16:53:11 | training...\n16:53:22 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5325 5.325e-10               .1974                 .6053   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1179            .6702              .5249   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9268 11.25     1 265.1   518       0          0 39.08  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5325             32768  2.773    .1189 5.975 .6878 1.005e-06 119.5 233.5   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 384.6 751.5 1.959        .4397\n\n16:53:31 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7716 7.716e-10               .7131                 .9722   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5630            .8103              .6889   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9837 11.53     1 270.6  1044       0          0 77.19  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7716             32768  2.909    .1189 6.008 .6396 2.855e-06 120.2 463.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 390.8 1508 3.869        .7613\n\n16:53:31 | creating task(s): fromfile:parlaiformat\n16:53:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:53:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt\n16:53:31 | running eval: valid\n16:53:31 | eval completed in 0.19s\n16:53:31 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1857       0          0 137.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5808 2.855e-06    72 827.7       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 233.5 2685            1\n\u001b[0m\n16:53:31 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:53:31 | saving best valid model: /tmp/model1\n16:53:31 | Saving dictionary to /tmp/model1.dict\n16:53:35 | task solved! stopping.\n16:53:35 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:53:35 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:53:35 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:53:35 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:53:35 | Using CUDA\n16:53:35 | loading dictionary from /tmp/model1.dict\n16:53:35 | num words = 54944\n16:53:40 | Loading existing model parameters from /tmp/model1\n16:53:44 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:53:46 | creating task(s): fromfile:parlaiformat\n16:53:46 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:53:46 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt\n16:53:46 | running eval: valid\n16:53:46 | eval completed in 0.20s\n16:53:46 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1779       0          0 132.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5808 2.855e-06    72   793       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 233.5 2572            1\n\u001b[0m\n16:53:46 | creating task(s): fromfile:parlaiformat\n16:53:46 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:53:46 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt\n16:53:46 | running eval: test\n16:53:46 | eval completed in 0.19s\n16:53:46 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1830       0          0 135.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5808 2.855e-06    72 815.7       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 233.5 2646            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:53:48.112669Z","iopub.execute_input":"2022-12-03T16:53:48.113111Z","iopub.status.idle":"2022-12-03T16:54:15.261132Z","shell.execute_reply.started":"2022-12-03T16:53:48.113065Z","shell.execute_reply":"2022-12-03T16:54:15.259277Z"},"scrolled":true,"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"16:53:55 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt)\u001b[0m\n16:53:55 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:53:55 | Using CUDA\n16:53:55 | loading dictionary from /tmp/model1.dict\n16:53:55 | num words = 54944\n16:53:59 | Loading existing model parameters from /tmp/model1\n16:54:06 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:54:07 | Opt:\n16:54:07 |     activation: gelu\n16:54:07 |     adafactor_eps: '[1e-30, 0.001]'\n16:54:07 |     adam_eps: 1e-08\n16:54:07 |     add_p1_after_newln: False\n16:54:07 |     aggregate_micro: False\n16:54:07 |     allow_missing_init_opts: False\n16:54:07 |     area_under_curve_class: None\n16:54:07 |     area_under_curve_digits: -1\n16:54:07 |     attention_dropout: 0.1\n16:54:07 |     batchsize: 40\n16:54:07 |     betas: '[0.9, 0.999]'\n16:54:07 |     bpe_add_prefix_space: None\n16:54:07 |     bpe_debug: False\n16:54:07 |     bpe_dropout: None\n16:54:07 |     bpe_merge: None\n16:54:07 |     bpe_vocab: None\n16:54:07 |     candidates: inline\n16:54:07 |     cap_num_predictions: 100\n16:54:07 |     checkpoint_activations: False\n16:54:07 |     class_weights: None\n16:54:07 |     classes: \"['__notok__', '__ok__']\"\n16:54:07 |     classes_from_file: None\n16:54:07 |     data_parallel: True\n16:54:07 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:54:07 |     datatype: train\n16:54:07 |     delimiter: '\\n'\n16:54:07 |     dict_class: parlai.core.dict:DictionaryAgent\n16:54:07 |     dict_endtoken: __start__\n16:54:07 |     dict_file: /tmp/model1.dict\n16:54:07 |     dict_include_test: False\n16:54:07 |     dict_include_valid: False\n16:54:07 |     dict_initpath: None\n16:54:07 |     dict_language: english\n16:54:07 |     dict_loaded: True\n16:54:07 |     dict_lower: True\n16:54:07 |     dict_max_ngram_size: -1\n16:54:07 |     dict_maxexs: -1\n16:54:07 |     dict_maxtokens: -1\n16:54:07 |     dict_minfreq: 0\n16:54:07 |     dict_nulltoken: __null__\n16:54:07 |     dict_starttoken: __start__\n16:54:07 |     dict_textfields: text,labels\n16:54:07 |     dict_tokenizer: bpe\n16:54:07 |     dict_unktoken: __unk__\n16:54:07 |     display_examples: False\n16:54:07 |     download_path: None\n16:54:07 |     dropout: 0.1\n16:54:07 |     dynamic_batching: None\n16:54:07 |     embedding_projection: random\n16:54:07 |     embedding_size: 768\n16:54:07 |     embedding_type: random\n16:54:07 |     embeddings_scale: False\n16:54:07 |     encode_candidate_vecs: True\n16:54:07 |     encode_candidate_vecs_batchsize: 256\n16:54:07 |     eval_batchsize: None\n16:54:07 |     eval_candidates: inline\n16:54:07 |     eval_dynamic_batching: None\n16:54:07 |     evaltask: None\n16:54:07 |     ffn_size: 3072\n16:54:07 |     final_extra_opt: \n16:54:07 |     fixed_candidate_vecs: reuse\n16:54:07 |     fixed_candidates_path: None\n16:54:07 |     force_fp16_tokens: True\n16:54:07 |     fp16: True\n16:54:07 |     fp16_impl: safe\n16:54:07 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-a.txt\n16:54:07 |     fromfile_datatype_extension: False\n16:54:07 |     gpu: -1\n16:54:07 |     gradient_clip: 0.1\n16:54:07 |     hide_labels: False\n16:54:07 |     history_add_global_end_token: None\n16:54:07 |     history_reversed: False\n16:54:07 |     history_size: 20\n16:54:07 |     ignore_bad_candidates: False\n16:54:07 |     ignore_labels: None\n16:54:07 |     image_cropsize: 224\n16:54:07 |     image_mode: raw\n16:54:07 |     image_size: 256\n16:54:07 |     inference: max\n16:54:07 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:54:07 |     init_opt: None\n16:54:07 |     interactive_candidates: fixed\n16:54:07 |     interactive_mode: False\n16:54:07 |     invsqrt_lr_decay_gamma: -1\n16:54:07 |     is_debug: False\n16:54:07 |     label_truncate: 72\n16:54:07 |     learn_embeddings: True\n16:54:07 |     learn_positional_embeddings: True\n16:54:07 |     learningrate: 5e-05\n16:54:07 |     load_from_pretrained_ranker: True\n16:54:07 |     log_every_n_secs: 10.0\n16:54:07 |     log_every_n_steps: 50\n16:54:07 |     log_keep_fields: all\n16:54:07 |     loglevel: info\n16:54:07 |     lr_scheduler: reduceonplateau\n16:54:07 |     lr_scheduler_decay: 0.5\n16:54:07 |     lr_scheduler_patience: 3\n16:54:07 |     max_train_steps: -1\n16:54:07 |     max_train_time: 7200.0\n16:54:07 |     memory_attention: sqrt\n16:54:07 |     metrics: default\n16:54:07 |     model: transformer/classifier\n16:54:07 |     model_file: /tmp/model1\n16:54:07 |     model_parallel: False\n16:54:07 |     momentum: 0\n16:54:07 |     multitask_weights: [1]\n16:54:07 |     mutators: None\n16:54:07 |     n_decoder_layers: -1\n16:54:07 |     n_encoder_layers: -1\n16:54:07 |     n_heads: 12\n16:54:07 |     n_layers: 12\n16:54:07 |     n_positions: 1024\n16:54:07 |     n_segments: 2\n16:54:07 |     nesterov: True\n16:54:07 |     no_cuda: False\n16:54:07 |     normalize_sent_emb: False\n16:54:07 |     num_epochs: -1\n16:54:07 |     num_examples: -1\n16:54:07 |     num_workers: 0\n16:54:07 |     nus: [0.7]\n16:54:07 |     optimizer: adamax\n16:54:07 |     output_scaling: 0.06\n16:54:07 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n16:54:07 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:54:07 |     person_tokens: False\n16:54:07 |     print_scores: False\n16:54:07 |     rank_candidates: False\n16:54:07 |     rank_top_k: -1\n16:54:07 |     reduction_type: mean\n16:54:07 |     ref_class: None\n16:54:07 |     relu_dropout: 0.0\n16:54:07 |     repeat_blocking_heuristic: True\n16:54:07 |     report_filename: \n16:54:07 |     return_cand_scores: False\n16:54:07 |     save_after_valid: True\n16:54:07 |     save_every_n_secs: -1\n16:54:07 |     save_format: conversations\n16:54:07 |     share_encoders: False\n16:54:07 |     share_word_embeddings: False\n16:54:07 |     short_final_eval: False\n16:54:07 |     special_tok_lst: None\n16:54:07 |     split_lines: False\n16:54:07 |     starttime: Dec03_16-52\n16:54:07 |     task: fromfile:parlaiformat\n16:54:07 |     tensorboard_log: False\n16:54:07 |     tensorboard_logdir: None\n16:54:07 |     text_truncate: 360\n16:54:07 |     threshold: 0.5\n16:54:07 |     topk: 5\n16:54:07 |     train_predict: False\n16:54:07 |     truncate: 1024\n16:54:07 |     update_classifier_head_only: False\n16:54:07 |     update_freq: 1\n16:54:07 |     use_memories: False\n16:54:07 |     use_reply: none\n16:54:07 |     validation_cutoff: 1.0\n16:54:07 |     validation_every_n_epochs: -1\n16:54:07 |     validation_every_n_secs: 20.0\n16:54:07 |     validation_every_n_steps: -1\n16:54:07 |     validation_max_exs: -1\n16:54:07 |     validation_metric: accuracy\n16:54:07 |     validation_metric_mode: max\n16:54:07 |     validation_patience: 30\n16:54:07 |     validation_share_agent: False\n16:54:07 |     variant: xlm\n16:54:07 |     verbose: False\n16:54:07 |     wandb_entity: None\n16:54:07 |     wandb_log: False\n16:54:07 |     wandb_name: None\n16:54:07 |     wandb_project: None\n16:54:07 |     warmup_rate: 0.0001\n16:54:07 |     warmup_updates: 1000\n16:54:07 |     weight_decay: None\n16:54:07 |     world_logs: \n16:54:07 |     wrap_memory_encoder: False\n16:54:08 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:54:08 | creating task(s): fromfile:parlaiformat\n16:54:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:54:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-a.txt\n16:54:13 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6900 6.9e-10               .6395                 .7639   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5500            .7281              .6484   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8300 11.13 525.2 517.4       0          0 39.41  200 .6900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .6595 2.855e-06   240 236.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 753.9        .6838\u001b[0m\n16:54:13 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6900 6.9e-10               .6395                 .7639   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5500            .7281              .6484   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8300 11.13 525.2 517.4       0          0 39.41  200 .6900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .6595 2.855e-06   240 236.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 753.9        .6838\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:54:15.264727Z","iopub.execute_input":"2022-12-03T16:54:15.265636Z","iopub.status.idle":"2022-12-03T16:54:41.512791Z","shell.execute_reply.started":"2022-12-03T16:54:15.265572Z","shell.execute_reply":"2022-12-03T16:54:41.511565Z"},"scrolled":true,"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"16:54:22 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_valid.txt)\u001b[0m\n16:54:22 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:54:22 | Using CUDA\n16:54:22 | loading dictionary from /tmp/model1.dict\n16:54:22 | num words = 54944\n16:54:27 | Loading existing model parameters from /tmp/model1\n16:54:32 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:54:33 | Opt:\n16:54:33 |     activation: gelu\n16:54:33 |     adafactor_eps: '[1e-30, 0.001]'\n16:54:33 |     adam_eps: 1e-08\n16:54:33 |     add_p1_after_newln: False\n16:54:33 |     aggregate_micro: False\n16:54:33 |     allow_missing_init_opts: False\n16:54:33 |     area_under_curve_class: None\n16:54:33 |     area_under_curve_digits: -1\n16:54:33 |     attention_dropout: 0.1\n16:54:33 |     batchsize: 40\n16:54:33 |     betas: '[0.9, 0.999]'\n16:54:33 |     bpe_add_prefix_space: None\n16:54:33 |     bpe_debug: False\n16:54:33 |     bpe_dropout: None\n16:54:33 |     bpe_merge: None\n16:54:33 |     bpe_vocab: None\n16:54:33 |     candidates: inline\n16:54:33 |     cap_num_predictions: 100\n16:54:33 |     checkpoint_activations: False\n16:54:33 |     class_weights: None\n16:54:33 |     classes: \"['__notok__', '__ok__']\"\n16:54:33 |     classes_from_file: None\n16:54:33 |     data_parallel: True\n16:54:33 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:54:33 |     datatype: train\n16:54:33 |     delimiter: '\\n'\n16:54:33 |     dict_class: parlai.core.dict:DictionaryAgent\n16:54:33 |     dict_endtoken: __start__\n16:54:33 |     dict_file: /tmp/model1.dict\n16:54:33 |     dict_include_test: False\n16:54:33 |     dict_include_valid: False\n16:54:33 |     dict_initpath: None\n16:54:33 |     dict_language: english\n16:54:33 |     dict_loaded: True\n16:54:33 |     dict_lower: True\n16:54:33 |     dict_max_ngram_size: -1\n16:54:33 |     dict_maxexs: -1\n16:54:33 |     dict_maxtokens: -1\n16:54:33 |     dict_minfreq: 0\n16:54:33 |     dict_nulltoken: __null__\n16:54:33 |     dict_starttoken: __start__\n16:54:33 |     dict_textfields: text,labels\n16:54:33 |     dict_tokenizer: bpe\n16:54:33 |     dict_unktoken: __unk__\n16:54:33 |     display_examples: False\n16:54:33 |     download_path: None\n16:54:33 |     dropout: 0.1\n16:54:33 |     dynamic_batching: None\n16:54:33 |     embedding_projection: random\n16:54:33 |     embedding_size: 768\n16:54:33 |     embedding_type: random\n16:54:33 |     embeddings_scale: False\n16:54:33 |     encode_candidate_vecs: True\n16:54:33 |     encode_candidate_vecs_batchsize: 256\n16:54:33 |     eval_batchsize: None\n16:54:33 |     eval_candidates: inline\n16:54:33 |     eval_dynamic_batching: None\n16:54:33 |     evaltask: None\n16:54:33 |     ffn_size: 3072\n16:54:33 |     final_extra_opt: \n16:54:33 |     fixed_candidate_vecs: reuse\n16:54:33 |     fixed_candidates_path: None\n16:54:33 |     force_fp16_tokens: True\n16:54:33 |     fp16: True\n16:54:33 |     fp16_impl: safe\n16:54:33 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-b.txt\n16:54:33 |     fromfile_datatype_extension: False\n16:54:33 |     gpu: -1\n16:54:33 |     gradient_clip: 0.1\n16:54:33 |     hide_labels: False\n16:54:33 |     history_add_global_end_token: None\n16:54:33 |     history_reversed: False\n16:54:33 |     history_size: 20\n16:54:33 |     ignore_bad_candidates: False\n16:54:33 |     ignore_labels: None\n16:54:33 |     image_cropsize: 224\n16:54:33 |     image_mode: raw\n16:54:33 |     image_size: 256\n16:54:33 |     inference: max\n16:54:33 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:54:33 |     init_opt: None\n16:54:33 |     interactive_candidates: fixed\n16:54:33 |     interactive_mode: False\n16:54:33 |     invsqrt_lr_decay_gamma: -1\n16:54:33 |     is_debug: False\n16:54:33 |     label_truncate: 72\n16:54:33 |     learn_embeddings: True\n16:54:33 |     learn_positional_embeddings: True\n16:54:33 |     learningrate: 5e-05\n16:54:33 |     load_from_pretrained_ranker: True\n16:54:33 |     log_every_n_secs: 10.0\n16:54:33 |     log_every_n_steps: 50\n16:54:33 |     log_keep_fields: all\n16:54:33 |     loglevel: info\n16:54:33 |     lr_scheduler: reduceonplateau\n16:54:33 |     lr_scheduler_decay: 0.5\n16:54:33 |     lr_scheduler_patience: 3\n16:54:33 |     max_train_steps: -1\n16:54:33 |     max_train_time: 7200.0\n16:54:33 |     memory_attention: sqrt\n16:54:33 |     metrics: default\n16:54:33 |     model: transformer/classifier\n16:54:33 |     model_file: /tmp/model1\n16:54:33 |     model_parallel: False\n16:54:33 |     momentum: 0\n16:54:33 |     multitask_weights: [1]\n16:54:33 |     mutators: None\n16:54:33 |     n_decoder_layers: -1\n16:54:33 |     n_encoder_layers: -1\n16:54:33 |     n_heads: 12\n16:54:33 |     n_layers: 12\n16:54:33 |     n_positions: 1024\n16:54:33 |     n_segments: 2\n16:54:33 |     nesterov: True\n16:54:33 |     no_cuda: False\n16:54:33 |     normalize_sent_emb: False\n16:54:33 |     num_epochs: -1\n16:54:33 |     num_examples: -1\n16:54:33 |     num_workers: 0\n16:54:33 |     nus: [0.7]\n16:54:33 |     optimizer: adamax\n16:54:33 |     output_scaling: 0.06\n16:54:33 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n16:54:33 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:54:33 |     person_tokens: False\n16:54:33 |     print_scores: False\n16:54:33 |     rank_candidates: False\n16:54:33 |     rank_top_k: -1\n16:54:33 |     reduction_type: mean\n16:54:33 |     ref_class: None\n16:54:33 |     relu_dropout: 0.0\n16:54:33 |     repeat_blocking_heuristic: True\n16:54:33 |     report_filename: \n16:54:33 |     return_cand_scores: False\n16:54:33 |     save_after_valid: True\n16:54:33 |     save_every_n_secs: -1\n16:54:33 |     save_format: conversations\n16:54:33 |     share_encoders: False\n16:54:33 |     share_word_embeddings: False\n16:54:33 |     short_final_eval: False\n16:54:33 |     special_tok_lst: None\n16:54:33 |     split_lines: False\n16:54:33 |     starttime: Dec03_16-52\n16:54:33 |     task: fromfile:parlaiformat\n16:54:33 |     tensorboard_log: False\n16:54:33 |     tensorboard_logdir: None\n16:54:33 |     text_truncate: 360\n16:54:33 |     threshold: 0.5\n16:54:33 |     topk: 5\n16:54:33 |     train_predict: False\n16:54:33 |     truncate: 1024\n16:54:33 |     update_classifier_head_only: False\n16:54:33 |     update_freq: 1\n16:54:33 |     use_memories: False\n16:54:33 |     use_reply: none\n16:54:33 |     validation_cutoff: 1.0\n16:54:33 |     validation_every_n_epochs: -1\n16:54:33 |     validation_every_n_secs: 20.0\n16:54:33 |     validation_every_n_steps: -1\n16:54:33 |     validation_max_exs: -1\n16:54:33 |     validation_metric: accuracy\n16:54:33 |     validation_metric_mode: max\n16:54:33 |     validation_patience: 30\n16:54:33 |     validation_share_agent: False\n16:54:33 |     variant: xlm\n16:54:33 |     verbose: False\n16:54:33 |     wandb_entity: None\n16:54:33 |     wandb_log: False\n16:54:33 |     wandb_name: None\n16:54:33 |     wandb_project: None\n16:54:33 |     warmup_rate: 0.0001\n16:54:33 |     warmup_updates: 1000\n16:54:33 |     weight_decay: None\n16:54:33 |     world_logs: \n16:54:33 |     wrap_memory_encoder: False\n16:54:34 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:54:34 | creating task(s): fromfile:parlaiformat\n16:54:34 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:54:34 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run1/data_train-b.txt\n16:54:39 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3100 3.1e-10               .1977                 .2361   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1700            .3947              .3516   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4500 11.13 525.2   492       0          0 37.47  200 .3100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .7330 2.855e-06   240 224.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 716.9        .2962\u001b[0m\n16:54:39 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3100 3.1e-10               .1977                 .2361   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1700            .3947              .3516   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4500 11.13 525.2   492       0          0 37.47  200 .3100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .7330 2.855e-06   240 224.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 716.9        .2962\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:54:41.514900Z","iopub.execute_input":"2022-12-03T16:54:41.515283Z","iopub.status.idle":"2022-12-03T16:54:42.622588Z","shell.execute_reply.started":"2022-12-03T16:54:41.515246Z","shell.execute_reply":"2022-12-03T16:54:42.621209Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:54:42.624838Z","iopub.execute_input":"2022-12-03T16:54:42.625251Z","iopub.status.idle":"2022-12-03T16:55:43.905474Z","shell.execute_reply.started":"2022-12-03T16:54:42.625210Z","shell.execute_reply":"2022-12-03T16:55:43.904023Z"},"scrolled":true,"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"16:54:50 | building dictionary first...\n16:54:50 | No model with opt yet at: /tmp/model2(.opt)\n16:54:50 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:54:50 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:54:50 | Using CUDA\n16:54:50 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:54:50 | num words = 54944\n16:54:55 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:55:09 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:55:09 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:55:09 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:55:09 | Opt:\n16:55:09 |     activation: gelu\n16:55:09 |     adafactor_eps: '(1e-30, 0.001)'\n16:55:09 |     adam_eps: 1e-08\n16:55:09 |     add_p1_after_newln: False\n16:55:09 |     aggregate_micro: False\n16:55:09 |     allow_missing_init_opts: False\n16:55:09 |     attention_dropout: 0.1\n16:55:09 |     batchsize: 20\n16:55:09 |     betas: '(0.9, 0.999)'\n16:55:09 |     bpe_add_prefix_space: None\n16:55:09 |     bpe_debug: False\n16:55:09 |     bpe_dropout: None\n16:55:09 |     bpe_merge: None\n16:55:09 |     bpe_vocab: None\n16:55:09 |     candidates: inline\n16:55:09 |     cap_num_predictions: 100\n16:55:09 |     checkpoint_activations: False\n16:55:09 |     class_weights: None\n16:55:09 |     classes: \"['__notok__', '__ok__']\"\n16:55:09 |     classes_from_file: None\n16:55:09 |     data_parallel: True\n16:55:09 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:55:09 |     datatype: train\n16:55:09 |     delimiter: '\\n'\n16:55:09 |     dict_class: parlai.core.dict:DictionaryAgent\n16:55:09 |     dict_endtoken: __start__\n16:55:09 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:55:09 |     dict_include_test: False\n16:55:09 |     dict_include_valid: False\n16:55:09 |     dict_initpath: None\n16:55:09 |     dict_language: english\n16:55:09 |     dict_loaded: True\n16:55:09 |     dict_lower: True\n16:55:09 |     dict_max_ngram_size: -1\n16:55:09 |     dict_maxexs: -1\n16:55:09 |     dict_maxtokens: -1\n16:55:09 |     dict_minfreq: 0\n16:55:09 |     dict_nulltoken: __null__\n16:55:09 |     dict_starttoken: __start__\n16:55:09 |     dict_textfields: text,labels\n16:55:09 |     dict_tokenizer: bpe\n16:55:09 |     dict_unktoken: __unk__\n16:55:09 |     display_examples: False\n16:55:09 |     download_path: None\n16:55:09 |     dropout: 0.1\n16:55:09 |     dynamic_batching: None\n16:55:09 |     embedding_projection: random\n16:55:09 |     embedding_size: 768\n16:55:09 |     embedding_type: random\n16:55:09 |     embeddings_scale: False\n16:55:09 |     encode_candidate_vecs: True\n16:55:09 |     encode_candidate_vecs_batchsize: 256\n16:55:09 |     eval_batchsize: None\n16:55:09 |     eval_candidates: inline\n16:55:09 |     eval_dynamic_batching: None\n16:55:09 |     evaltask: None\n16:55:09 |     ffn_size: 3072\n16:55:09 |     final_extra_opt: \n16:55:09 |     fixed_candidate_vecs: reuse\n16:55:09 |     fixed_candidates_path: None\n16:55:09 |     force_fp16_tokens: False\n16:55:09 |     fp16: True\n16:55:09 |     fp16_impl: safe\n16:55:09 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt\n16:55:09 |     fromfile_datatype_extension: False\n16:55:09 |     gpu: -1\n16:55:09 |     gradient_clip: 0.1\n16:55:09 |     hide_labels: False\n16:55:09 |     history_add_global_end_token: None\n16:55:09 |     history_reversed: False\n16:55:09 |     history_size: 20\n16:55:09 |     ignore_bad_candidates: False\n16:55:09 |     ignore_labels: None\n16:55:09 |     image_cropsize: 224\n16:55:09 |     image_mode: raw\n16:55:09 |     image_size: 256\n16:55:09 |     inference: max\n16:55:09 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:55:09 |     init_opt: None\n16:55:09 |     interactive_candidates: fixed\n16:55:09 |     interactive_mode: False\n16:55:09 |     invsqrt_lr_decay_gamma: -1\n16:55:09 |     is_debug: False\n16:55:09 |     label_truncate: 72\n16:55:09 |     learn_embeddings: True\n16:55:09 |     learn_positional_embeddings: True\n16:55:09 |     learningrate: 5e-05\n16:55:09 |     load_from_checkpoint: False\n16:55:09 |     load_from_pretrained_ranker: True\n16:55:09 |     log_every_n_secs: 10.0\n16:55:09 |     log_every_n_steps: 50\n16:55:09 |     log_keep_fields: all\n16:55:09 |     loglevel: info\n16:55:09 |     lr_scheduler: reduceonplateau\n16:55:09 |     lr_scheduler_decay: 0.5\n16:55:09 |     lr_scheduler_patience: 3\n16:55:09 |     max_train_steps: -1\n16:55:09 |     max_train_time: 7200.0\n16:55:09 |     memory_attention: sqrt\n16:55:09 |     metrics: default\n16:55:09 |     model: transformer/classifier\n16:55:09 |     model_file: /tmp/model2\n16:55:09 |     model_parallel: False\n16:55:09 |     momentum: 0\n16:55:09 |     multitask_weights: [1]\n16:55:09 |     mutators: None\n16:55:09 |     n_decoder_layers: -1\n16:55:09 |     n_encoder_layers: -1\n16:55:09 |     n_heads: 12\n16:55:09 |     n_layers: 12\n16:55:09 |     n_positions: 1024\n16:55:09 |     n_segments: 2\n16:55:09 |     nesterov: True\n16:55:09 |     no_cuda: False\n16:55:09 |     normalize_sent_emb: False\n16:55:09 |     num_epochs: -1\n16:55:09 |     num_workers: 0\n16:55:09 |     nus: (0.7,)\n16:55:09 |     optimizer: adamax\n16:55:09 |     output_scaling: 0.06\n16:55:09 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n16:55:09 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:55:09 |     person_tokens: False\n16:55:09 |     print_scores: False\n16:55:09 |     rank_candidates: False\n16:55:09 |     rank_top_k: -1\n16:55:09 |     reduction_type: mean\n16:55:09 |     ref_class: None\n16:55:09 |     relu_dropout: 0.0\n16:55:09 |     repeat_blocking_heuristic: True\n16:55:09 |     return_cand_scores: False\n16:55:09 |     save_after_valid: True\n16:55:09 |     save_every_n_secs: -1\n16:55:09 |     save_format: conversations\n16:55:09 |     share_encoders: False\n16:55:09 |     share_word_embeddings: False\n16:55:09 |     short_final_eval: False\n16:55:09 |     special_tok_lst: None\n16:55:09 |     split_lines: False\n16:55:09 |     starttime: Dec03_16-54\n16:55:09 |     task: fromfile:parlaiformat\n16:55:09 |     tensorboard_log: False\n16:55:09 |     tensorboard_logdir: None\n16:55:09 |     text_truncate: 360\n16:55:09 |     threshold: 0.5\n16:55:09 |     topk: 5\n16:55:09 |     train_predict: False\n16:55:09 |     truncate: 1024\n16:55:09 |     update_classifier_head_only: False\n16:55:09 |     update_freq: 1\n16:55:09 |     use_memories: False\n16:55:09 |     use_reply: none\n16:55:09 |     validation_cutoff: 1.0\n16:55:09 |     validation_every_n_epochs: -1\n16:55:09 |     validation_every_n_secs: 20.0\n16:55:09 |     validation_every_n_steps: -1\n16:55:09 |     validation_max_exs: -1\n16:55:09 |     validation_metric: accuracy\n16:55:09 |     validation_metric_mode: max\n16:55:09 |     validation_patience: 30\n16:55:09 |     validation_share_agent: False\n16:55:09 |     variant: xlm\n16:55:09 |     verbose: False\n16:55:09 |     wandb_entity: None\n16:55:09 |     wandb_log: False\n16:55:09 |     wandb_name: None\n16:55:09 |     wandb_project: None\n16:55:09 |     warmup_rate: 0.0001\n16:55:09 |     warmup_updates: 1000\n16:55:09 |     weight_decay: None\n16:55:09 |     world_logs: \n16:55:09 |     wrap_memory_encoder: False\n16:55:09 | creating task(s): fromfile:parlaiformat\n16:55:09 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt\n16:55:09 | training...\n16:55:19 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5571 5.571e-10               .6737                 .5079   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .3111                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .1842 11.16     1 263.2   541       0          0 41.11  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5571             32768  2.652    .1206 5.914 .6601 1.055e-06 118.3 243.2   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps  ups  weighted_f1  \n         0          0                   21 381.5 784.2 2.06        .4769\n\n16:55:29 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8932 8.932e-10               .9047                 .8260   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .8786                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7836 11.07     1 261.4  1012       0          0 77.42  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8932             32768  2.672    .1207 6.014 .5999 2.905e-06 120.3 465.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                   58 381.7 1478 3.88        .8919\n\n16:55:29 | creating task(s): fromfile:parlaiformat\n16:55:29 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:55:29 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt\n16:55:29 | running eval: valid\n16:55:29 | eval completed in 0.20s\n16:55:29 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1722       0          0 132.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5347 2.905e-06    72 794.7       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2517            1\n\u001b[0m\n16:55:29 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:55:29 | saving best valid model: /tmp/model2\n16:55:29 | Saving dictionary to /tmp/model2.dict\n16:55:33 | task solved! stopping.\n16:55:33 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:55:33 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:55:33 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:55:33 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:55:33 | Using CUDA\n16:55:33 | loading dictionary from /tmp/model2.dict\n16:55:34 | num words = 54944\n16:55:38 | Loading existing model parameters from /tmp/model2\n16:55:40 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:55:41 | creating task(s): fromfile:parlaiformat\n16:55:41 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:55:41 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt\n16:55:41 | running eval: valid\n16:55:41 | eval completed in 0.22s\n16:55:41 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1535       0          0   118   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5347 2.905e-06    72 708.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2244            1\n\u001b[0m\n16:55:41 | creating task(s): fromfile:parlaiformat\n16:55:41 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:55:41 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt\n16:55:41 | running eval: test\n16:55:42 | eval completed in 0.20s\n16:55:42 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1755       0          0 134.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5347 2.905e-06    72 809.9       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2565            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:55:43.907460Z","iopub.execute_input":"2022-12-03T16:55:43.907874Z","iopub.status.idle":"2022-12-03T16:56:11.764134Z","shell.execute_reply.started":"2022-12-03T16:55:43.907831Z","shell.execute_reply":"2022-12-03T16:56:11.762936Z"},"scrolled":true,"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"16:55:50 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt)\u001b[0m\n16:55:50 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:55:50 | Using CUDA\n16:55:50 | loading dictionary from /tmp/model2.dict\n16:55:51 | num words = 54944\n16:55:55 | Loading existing model parameters from /tmp/model2\n16:56:02 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:56:04 | Opt:\n16:56:04 |     activation: gelu\n16:56:04 |     adafactor_eps: '[1e-30, 0.001]'\n16:56:04 |     adam_eps: 1e-08\n16:56:04 |     add_p1_after_newln: False\n16:56:04 |     aggregate_micro: False\n16:56:04 |     allow_missing_init_opts: False\n16:56:04 |     area_under_curve_class: None\n16:56:04 |     area_under_curve_digits: -1\n16:56:04 |     attention_dropout: 0.1\n16:56:04 |     batchsize: 40\n16:56:04 |     betas: '[0.9, 0.999]'\n16:56:04 |     bpe_add_prefix_space: None\n16:56:04 |     bpe_debug: False\n16:56:04 |     bpe_dropout: None\n16:56:04 |     bpe_merge: None\n16:56:04 |     bpe_vocab: None\n16:56:04 |     candidates: inline\n16:56:04 |     cap_num_predictions: 100\n16:56:04 |     checkpoint_activations: False\n16:56:04 |     class_weights: None\n16:56:04 |     classes: \"['__notok__', '__ok__']\"\n16:56:04 |     classes_from_file: None\n16:56:04 |     data_parallel: True\n16:56:04 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:56:04 |     datatype: train\n16:56:04 |     delimiter: '\\n'\n16:56:04 |     dict_class: parlai.core.dict:DictionaryAgent\n16:56:04 |     dict_endtoken: __start__\n16:56:04 |     dict_file: /tmp/model2.dict\n16:56:04 |     dict_include_test: False\n16:56:04 |     dict_include_valid: False\n16:56:04 |     dict_initpath: None\n16:56:04 |     dict_language: english\n16:56:04 |     dict_loaded: True\n16:56:04 |     dict_lower: True\n16:56:04 |     dict_max_ngram_size: -1\n16:56:04 |     dict_maxexs: -1\n16:56:04 |     dict_maxtokens: -1\n16:56:04 |     dict_minfreq: 0\n16:56:04 |     dict_nulltoken: __null__\n16:56:04 |     dict_starttoken: __start__\n16:56:04 |     dict_textfields: text,labels\n16:56:04 |     dict_tokenizer: bpe\n16:56:04 |     dict_unktoken: __unk__\n16:56:04 |     display_examples: False\n16:56:04 |     download_path: None\n16:56:04 |     dropout: 0.1\n16:56:04 |     dynamic_batching: None\n16:56:04 |     embedding_projection: random\n16:56:04 |     embedding_size: 768\n16:56:04 |     embedding_type: random\n16:56:04 |     embeddings_scale: False\n16:56:04 |     encode_candidate_vecs: True\n16:56:04 |     encode_candidate_vecs_batchsize: 256\n16:56:04 |     eval_batchsize: None\n16:56:04 |     eval_candidates: inline\n16:56:04 |     eval_dynamic_batching: None\n16:56:04 |     evaltask: None\n16:56:04 |     ffn_size: 3072\n16:56:04 |     final_extra_opt: \n16:56:04 |     fixed_candidate_vecs: reuse\n16:56:04 |     fixed_candidates_path: None\n16:56:04 |     force_fp16_tokens: True\n16:56:04 |     fp16: True\n16:56:04 |     fp16_impl: safe\n16:56:04 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-a.txt\n16:56:04 |     fromfile_datatype_extension: False\n16:56:04 |     gpu: -1\n16:56:04 |     gradient_clip: 0.1\n16:56:04 |     hide_labels: False\n16:56:04 |     history_add_global_end_token: None\n16:56:04 |     history_reversed: False\n16:56:04 |     history_size: 20\n16:56:04 |     ignore_bad_candidates: False\n16:56:04 |     ignore_labels: None\n16:56:04 |     image_cropsize: 224\n16:56:04 |     image_mode: raw\n16:56:04 |     image_size: 256\n16:56:04 |     inference: max\n16:56:04 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:56:04 |     init_opt: None\n16:56:04 |     interactive_candidates: fixed\n16:56:04 |     interactive_mode: False\n16:56:04 |     invsqrt_lr_decay_gamma: -1\n16:56:04 |     is_debug: False\n16:56:04 |     label_truncate: 72\n16:56:04 |     learn_embeddings: True\n16:56:04 |     learn_positional_embeddings: True\n16:56:04 |     learningrate: 5e-05\n16:56:04 |     load_from_pretrained_ranker: True\n16:56:04 |     log_every_n_secs: 10.0\n16:56:04 |     log_every_n_steps: 50\n16:56:04 |     log_keep_fields: all\n16:56:04 |     loglevel: info\n16:56:04 |     lr_scheduler: reduceonplateau\n16:56:04 |     lr_scheduler_decay: 0.5\n16:56:04 |     lr_scheduler_patience: 3\n16:56:04 |     max_train_steps: -1\n16:56:04 |     max_train_time: 7200.0\n16:56:04 |     memory_attention: sqrt\n16:56:04 |     metrics: default\n16:56:04 |     model: transformer/classifier\n16:56:04 |     model_file: /tmp/model2\n16:56:04 |     model_parallel: False\n16:56:04 |     momentum: 0\n16:56:04 |     multitask_weights: [1]\n16:56:04 |     mutators: None\n16:56:04 |     n_decoder_layers: -1\n16:56:04 |     n_encoder_layers: -1\n16:56:04 |     n_heads: 12\n16:56:04 |     n_layers: 12\n16:56:04 |     n_positions: 1024\n16:56:04 |     n_segments: 2\n16:56:04 |     nesterov: True\n16:56:04 |     no_cuda: False\n16:56:04 |     normalize_sent_emb: False\n16:56:04 |     num_epochs: -1\n16:56:04 |     num_examples: -1\n16:56:04 |     num_workers: 0\n16:56:04 |     nus: [0.7]\n16:56:04 |     optimizer: adamax\n16:56:04 |     output_scaling: 0.06\n16:56:04 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n16:56:04 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:56:04 |     person_tokens: False\n16:56:04 |     print_scores: False\n16:56:04 |     rank_candidates: False\n16:56:04 |     rank_top_k: -1\n16:56:04 |     reduction_type: mean\n16:56:04 |     ref_class: None\n16:56:04 |     relu_dropout: 0.0\n16:56:04 |     repeat_blocking_heuristic: True\n16:56:04 |     report_filename: \n16:56:04 |     return_cand_scores: False\n16:56:04 |     save_after_valid: True\n16:56:04 |     save_every_n_secs: -1\n16:56:04 |     save_format: conversations\n16:56:04 |     share_encoders: False\n16:56:04 |     share_word_embeddings: False\n16:56:04 |     short_final_eval: False\n16:56:04 |     special_tok_lst: None\n16:56:04 |     split_lines: False\n16:56:04 |     starttime: Dec03_16-54\n16:56:04 |     task: fromfile:parlaiformat\n16:56:04 |     tensorboard_log: False\n16:56:04 |     tensorboard_logdir: None\n16:56:04 |     text_truncate: 360\n16:56:04 |     threshold: 0.5\n16:56:04 |     topk: 5\n16:56:04 |     train_predict: False\n16:56:04 |     truncate: 1024\n16:56:04 |     update_classifier_head_only: False\n16:56:04 |     update_freq: 1\n16:56:04 |     use_memories: False\n16:56:04 |     use_reply: none\n16:56:04 |     validation_cutoff: 1.0\n16:56:04 |     validation_every_n_epochs: -1\n16:56:04 |     validation_every_n_secs: 20.0\n16:56:04 |     validation_every_n_steps: -1\n16:56:04 |     validation_max_exs: -1\n16:56:04 |     validation_metric: accuracy\n16:56:04 |     validation_metric_mode: max\n16:56:04 |     validation_patience: 30\n16:56:04 |     validation_share_agent: False\n16:56:04 |     variant: xlm\n16:56:04 |     verbose: False\n16:56:04 |     wandb_entity: None\n16:56:04 |     wandb_log: False\n16:56:04 |     wandb_name: None\n16:56:04 |     wandb_project: None\n16:56:04 |     warmup_rate: 0.0001\n16:56:04 |     warmup_updates: 1000\n16:56:04 |     weight_decay: None\n16:56:04 |     world_logs: \n16:56:04 |     wrap_memory_encoder: False\n16:56:04 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:56:04 | creating task(s): fromfile:parlaiformat\n16:56:04 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:56:04 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-a.txt\n16:56:10 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7850 7.85e-10               .7795                 .8000   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7600            .7902              .7714   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8100  11.7 547.8 508.3       0          0 37.12  200 .7850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6408 2.905e-06   240 222.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 787.8 731.1        .7849\u001b[0m\n16:56:10 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7850 7.85e-10               .7795                 .8000   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7600            .7902              .7714   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8100  11.7 547.8 508.3       0          0 37.12  200 .7850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6408 2.905e-06   240 222.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 787.8 731.1        .7849\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:56:11.766975Z","iopub.execute_input":"2022-12-03T16:56:11.767373Z","iopub.status.idle":"2022-12-03T16:56:38.299747Z","shell.execute_reply.started":"2022-12-03T16:56:11.767333Z","shell.execute_reply":"2022-12-03T16:56:38.298557Z"},"scrolled":true,"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"16:56:18 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_valid.txt)\u001b[0m\n16:56:18 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:56:18 | Using CUDA\n16:56:18 | loading dictionary from /tmp/model2.dict\n16:56:19 | num words = 54944\n16:56:23 | Loading existing model parameters from /tmp/model2\n16:56:29 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:56:30 | Opt:\n16:56:30 |     activation: gelu\n16:56:30 |     adafactor_eps: '[1e-30, 0.001]'\n16:56:30 |     adam_eps: 1e-08\n16:56:30 |     add_p1_after_newln: False\n16:56:30 |     aggregate_micro: False\n16:56:30 |     allow_missing_init_opts: False\n16:56:30 |     area_under_curve_class: None\n16:56:30 |     area_under_curve_digits: -1\n16:56:30 |     attention_dropout: 0.1\n16:56:30 |     batchsize: 40\n16:56:30 |     betas: '[0.9, 0.999]'\n16:56:30 |     bpe_add_prefix_space: None\n16:56:30 |     bpe_debug: False\n16:56:30 |     bpe_dropout: None\n16:56:30 |     bpe_merge: None\n16:56:30 |     bpe_vocab: None\n16:56:30 |     candidates: inline\n16:56:30 |     cap_num_predictions: 100\n16:56:30 |     checkpoint_activations: False\n16:56:30 |     class_weights: None\n16:56:30 |     classes: \"['__notok__', '__ok__']\"\n16:56:30 |     classes_from_file: None\n16:56:30 |     data_parallel: True\n16:56:30 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:56:30 |     datatype: train\n16:56:30 |     delimiter: '\\n'\n16:56:30 |     dict_class: parlai.core.dict:DictionaryAgent\n16:56:30 |     dict_endtoken: __start__\n16:56:30 |     dict_file: /tmp/model2.dict\n16:56:30 |     dict_include_test: False\n16:56:30 |     dict_include_valid: False\n16:56:30 |     dict_initpath: None\n16:56:30 |     dict_language: english\n16:56:30 |     dict_loaded: True\n16:56:30 |     dict_lower: True\n16:56:30 |     dict_max_ngram_size: -1\n16:56:30 |     dict_maxexs: -1\n16:56:30 |     dict_maxtokens: -1\n16:56:30 |     dict_minfreq: 0\n16:56:30 |     dict_nulltoken: __null__\n16:56:30 |     dict_starttoken: __start__\n16:56:30 |     dict_textfields: text,labels\n16:56:30 |     dict_tokenizer: bpe\n16:56:30 |     dict_unktoken: __unk__\n16:56:30 |     display_examples: False\n16:56:30 |     download_path: None\n16:56:30 |     dropout: 0.1\n16:56:30 |     dynamic_batching: None\n16:56:30 |     embedding_projection: random\n16:56:30 |     embedding_size: 768\n16:56:30 |     embedding_type: random\n16:56:30 |     embeddings_scale: False\n16:56:30 |     encode_candidate_vecs: True\n16:56:30 |     encode_candidate_vecs_batchsize: 256\n16:56:30 |     eval_batchsize: None\n16:56:30 |     eval_candidates: inline\n16:56:30 |     eval_dynamic_batching: None\n16:56:30 |     evaltask: None\n16:56:30 |     ffn_size: 3072\n16:56:30 |     final_extra_opt: \n16:56:30 |     fixed_candidate_vecs: reuse\n16:56:30 |     fixed_candidates_path: None\n16:56:30 |     force_fp16_tokens: True\n16:56:30 |     fp16: True\n16:56:30 |     fp16_impl: safe\n16:56:30 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-b.txt\n16:56:30 |     fromfile_datatype_extension: False\n16:56:30 |     gpu: -1\n16:56:30 |     gradient_clip: 0.1\n16:56:30 |     hide_labels: False\n16:56:30 |     history_add_global_end_token: None\n16:56:30 |     history_reversed: False\n16:56:30 |     history_size: 20\n16:56:30 |     ignore_bad_candidates: False\n16:56:30 |     ignore_labels: None\n16:56:30 |     image_cropsize: 224\n16:56:30 |     image_mode: raw\n16:56:30 |     image_size: 256\n16:56:30 |     inference: max\n16:56:30 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:56:30 |     init_opt: None\n16:56:30 |     interactive_candidates: fixed\n16:56:30 |     interactive_mode: False\n16:56:30 |     invsqrt_lr_decay_gamma: -1\n16:56:30 |     is_debug: False\n16:56:30 |     label_truncate: 72\n16:56:30 |     learn_embeddings: True\n16:56:30 |     learn_positional_embeddings: True\n16:56:30 |     learningrate: 5e-05\n16:56:30 |     load_from_pretrained_ranker: True\n16:56:30 |     log_every_n_secs: 10.0\n16:56:30 |     log_every_n_steps: 50\n16:56:30 |     log_keep_fields: all\n16:56:30 |     loglevel: info\n16:56:30 |     lr_scheduler: reduceonplateau\n16:56:30 |     lr_scheduler_decay: 0.5\n16:56:30 |     lr_scheduler_patience: 3\n16:56:30 |     max_train_steps: -1\n16:56:30 |     max_train_time: 7200.0\n16:56:30 |     memory_attention: sqrt\n16:56:30 |     metrics: default\n16:56:30 |     model: transformer/classifier\n16:56:30 |     model_file: /tmp/model2\n16:56:30 |     model_parallel: False\n16:56:30 |     momentum: 0\n16:56:30 |     multitask_weights: [1]\n16:56:30 |     mutators: None\n16:56:30 |     n_decoder_layers: -1\n16:56:30 |     n_encoder_layers: -1\n16:56:30 |     n_heads: 12\n16:56:30 |     n_layers: 12\n16:56:30 |     n_positions: 1024\n16:56:30 |     n_segments: 2\n16:56:30 |     nesterov: True\n16:56:30 |     no_cuda: False\n16:56:30 |     normalize_sent_emb: False\n16:56:30 |     num_epochs: -1\n16:56:30 |     num_examples: -1\n16:56:30 |     num_workers: 0\n16:56:30 |     nus: [0.7]\n16:56:30 |     optimizer: adamax\n16:56:30 |     output_scaling: 0.06\n16:56:30 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n16:56:30 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:56:30 |     person_tokens: False\n16:56:30 |     print_scores: False\n16:56:30 |     rank_candidates: False\n16:56:30 |     rank_top_k: -1\n16:56:30 |     reduction_type: mean\n16:56:30 |     ref_class: None\n16:56:30 |     relu_dropout: 0.0\n16:56:30 |     repeat_blocking_heuristic: True\n16:56:30 |     report_filename: \n16:56:30 |     return_cand_scores: False\n16:56:30 |     save_after_valid: True\n16:56:30 |     save_every_n_secs: -1\n16:56:30 |     save_format: conversations\n16:56:30 |     share_encoders: False\n16:56:30 |     share_word_embeddings: False\n16:56:30 |     short_final_eval: False\n16:56:30 |     special_tok_lst: None\n16:56:30 |     split_lines: False\n16:56:30 |     starttime: Dec03_16-54\n16:56:30 |     task: fromfile:parlaiformat\n16:56:30 |     tensorboard_log: False\n16:56:30 |     tensorboard_logdir: None\n16:56:30 |     text_truncate: 360\n16:56:30 |     threshold: 0.5\n16:56:30 |     topk: 5\n16:56:30 |     train_predict: False\n16:56:30 |     truncate: 1024\n16:56:30 |     update_classifier_head_only: False\n16:56:30 |     update_freq: 1\n16:56:30 |     use_memories: False\n16:56:30 |     use_reply: none\n16:56:30 |     validation_cutoff: 1.0\n16:56:30 |     validation_every_n_epochs: -1\n16:56:30 |     validation_every_n_secs: 20.0\n16:56:30 |     validation_every_n_steps: -1\n16:56:30 |     validation_max_exs: -1\n16:56:30 |     validation_metric: accuracy\n16:56:30 |     validation_metric_mode: max\n16:56:30 |     validation_patience: 30\n16:56:30 |     validation_share_agent: False\n16:56:30 |     variant: xlm\n16:56:30 |     verbose: False\n16:56:30 |     wandb_entity: None\n16:56:30 |     wandb_log: False\n16:56:30 |     wandb_name: None\n16:56:30 |     wandb_project: None\n16:56:30 |     warmup_rate: 0.0001\n16:56:30 |     warmup_updates: 1000\n16:56:30 |     weight_decay: None\n16:56:30 |     world_logs: \n16:56:30 |     wrap_memory_encoder: False\n16:56:31 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:56:31 | creating task(s): fromfile:parlaiformat\n16:56:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:56:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run2/data_train-b.txt\n16:56:36 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2150 2.15e-10               .1949                 .2000   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1900            .2341              .2286   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2400  11.7 547.8   537       0          0 39.21  200 .2150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7552 2.905e-06   240 235.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 787.8 772.3        .2145\u001b[0m\n16:56:36 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2150 2.15e-10               .1949                 .2000   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1900            .2341              .2286   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2400  11.7 547.8   537       0          0 39.21  200 .2150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7552 2.905e-06   240 235.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 787.8 772.3        .2145\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:56:38.302147Z","iopub.execute_input":"2022-12-03T16:56:38.302575Z","iopub.status.idle":"2022-12-03T16:56:39.401996Z","shell.execute_reply.started":"2022-12-03T16:56:38.302533Z","shell.execute_reply":"2022-12-03T16:56:39.400546Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:56:39.404966Z","iopub.execute_input":"2022-12-03T16:56:39.406048Z","iopub.status.idle":"2022-12-03T16:57:34.117443Z","shell.execute_reply.started":"2022-12-03T16:56:39.406003Z","shell.execute_reply":"2022-12-03T16:57:34.115851Z"},"scrolled":true,"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"16:56:46 | building dictionary first...\n16:56:46 | No model with opt yet at: /tmp/model3(.opt)\n16:56:46 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:56:46 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:56:46 | Using CUDA\n16:56:46 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:56:46 | num words = 54944\n16:56:51 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:56:58 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:56:58 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:56:58 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:56:58 | Opt:\n16:56:58 |     activation: gelu\n16:56:58 |     adafactor_eps: '(1e-30, 0.001)'\n16:56:58 |     adam_eps: 1e-08\n16:56:58 |     add_p1_after_newln: False\n16:56:58 |     aggregate_micro: False\n16:56:58 |     allow_missing_init_opts: False\n16:56:58 |     attention_dropout: 0.1\n16:56:58 |     batchsize: 20\n16:56:58 |     betas: '(0.9, 0.999)'\n16:56:58 |     bpe_add_prefix_space: None\n16:56:58 |     bpe_debug: False\n16:56:58 |     bpe_dropout: None\n16:56:58 |     bpe_merge: None\n16:56:58 |     bpe_vocab: None\n16:56:58 |     candidates: inline\n16:56:58 |     cap_num_predictions: 100\n16:56:58 |     checkpoint_activations: False\n16:56:58 |     class_weights: None\n16:56:58 |     classes: \"['__notok__', '__ok__']\"\n16:56:58 |     classes_from_file: None\n16:56:58 |     data_parallel: True\n16:56:58 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:56:58 |     datatype: train\n16:56:58 |     delimiter: '\\n'\n16:56:58 |     dict_class: parlai.core.dict:DictionaryAgent\n16:56:58 |     dict_endtoken: __start__\n16:56:58 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:56:58 |     dict_include_test: False\n16:56:58 |     dict_include_valid: False\n16:56:58 |     dict_initpath: None\n16:56:58 |     dict_language: english\n16:56:58 |     dict_loaded: True\n16:56:58 |     dict_lower: True\n16:56:58 |     dict_max_ngram_size: -1\n16:56:58 |     dict_maxexs: -1\n16:56:58 |     dict_maxtokens: -1\n16:56:58 |     dict_minfreq: 0\n16:56:58 |     dict_nulltoken: __null__\n16:56:58 |     dict_starttoken: __start__\n16:56:58 |     dict_textfields: text,labels\n16:56:58 |     dict_tokenizer: bpe\n16:56:58 |     dict_unktoken: __unk__\n16:56:58 |     display_examples: False\n16:56:58 |     download_path: None\n16:56:58 |     dropout: 0.1\n16:56:58 |     dynamic_batching: None\n16:56:58 |     embedding_projection: random\n16:56:58 |     embedding_size: 768\n16:56:58 |     embedding_type: random\n16:56:58 |     embeddings_scale: False\n16:56:58 |     encode_candidate_vecs: True\n16:56:58 |     encode_candidate_vecs_batchsize: 256\n16:56:58 |     eval_batchsize: None\n16:56:58 |     eval_candidates: inline\n16:56:58 |     eval_dynamic_batching: None\n16:56:58 |     evaltask: None\n16:56:58 |     ffn_size: 3072\n16:56:58 |     final_extra_opt: \n16:56:58 |     fixed_candidate_vecs: reuse\n16:56:58 |     fixed_candidates_path: None\n16:56:58 |     force_fp16_tokens: False\n16:56:58 |     fp16: True\n16:56:58 |     fp16_impl: safe\n16:56:58 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt\n16:56:58 |     fromfile_datatype_extension: False\n16:56:58 |     gpu: -1\n16:56:58 |     gradient_clip: 0.1\n16:56:58 |     hide_labels: False\n16:56:58 |     history_add_global_end_token: None\n16:56:58 |     history_reversed: False\n16:56:58 |     history_size: 20\n16:56:58 |     ignore_bad_candidates: False\n16:56:58 |     ignore_labels: None\n16:56:58 |     image_cropsize: 224\n16:56:58 |     image_mode: raw\n16:56:58 |     image_size: 256\n16:56:58 |     inference: max\n16:56:58 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:56:58 |     init_opt: None\n16:56:58 |     interactive_candidates: fixed\n16:56:58 |     interactive_mode: False\n16:56:58 |     invsqrt_lr_decay_gamma: -1\n16:56:58 |     is_debug: False\n16:56:58 |     label_truncate: 72\n16:56:58 |     learn_embeddings: True\n16:56:58 |     learn_positional_embeddings: True\n16:56:58 |     learningrate: 5e-05\n16:56:58 |     load_from_checkpoint: False\n16:56:58 |     load_from_pretrained_ranker: True\n16:56:58 |     log_every_n_secs: 10.0\n16:56:58 |     log_every_n_steps: 50\n16:56:58 |     log_keep_fields: all\n16:56:58 |     loglevel: info\n16:56:58 |     lr_scheduler: reduceonplateau\n16:56:58 |     lr_scheduler_decay: 0.5\n16:56:58 |     lr_scheduler_patience: 3\n16:56:58 |     max_train_steps: -1\n16:56:58 |     max_train_time: 7200.0\n16:56:58 |     memory_attention: sqrt\n16:56:58 |     metrics: default\n16:56:58 |     model: transformer/classifier\n16:56:58 |     model_file: /tmp/model3\n16:56:58 |     model_parallel: False\n16:56:58 |     momentum: 0\n16:56:58 |     multitask_weights: [1]\n16:56:58 |     mutators: None\n16:56:58 |     n_decoder_layers: -1\n16:56:58 |     n_encoder_layers: -1\n16:56:58 |     n_heads: 12\n16:56:58 |     n_layers: 12\n16:56:58 |     n_positions: 1024\n16:56:58 |     n_segments: 2\n16:56:58 |     nesterov: True\n16:56:58 |     no_cuda: False\n16:56:58 |     normalize_sent_emb: False\n16:56:58 |     num_epochs: -1\n16:56:58 |     num_workers: 0\n16:56:58 |     nus: (0.7,)\n16:56:58 |     optimizer: adamax\n16:56:58 |     output_scaling: 0.06\n16:56:58 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n16:56:58 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:56:58 |     person_tokens: False\n16:56:58 |     print_scores: False\n16:56:58 |     rank_candidates: False\n16:56:58 |     rank_top_k: -1\n16:56:58 |     reduction_type: mean\n16:56:58 |     ref_class: None\n16:56:58 |     relu_dropout: 0.0\n16:56:58 |     repeat_blocking_heuristic: True\n16:56:58 |     return_cand_scores: False\n16:56:58 |     save_after_valid: True\n16:56:58 |     save_every_n_secs: -1\n16:56:58 |     save_format: conversations\n16:56:58 |     share_encoders: False\n16:56:58 |     share_word_embeddings: False\n16:56:58 |     short_final_eval: False\n16:56:58 |     special_tok_lst: None\n16:56:58 |     split_lines: False\n16:56:58 |     starttime: Dec03_16-56\n16:56:58 |     task: fromfile:parlaiformat\n16:56:58 |     tensorboard_log: False\n16:56:58 |     tensorboard_logdir: None\n16:56:58 |     text_truncate: 360\n16:56:58 |     threshold: 0.5\n16:56:58 |     topk: 5\n16:56:58 |     train_predict: False\n16:56:58 |     truncate: 1024\n16:56:58 |     update_classifier_head_only: False\n16:56:58 |     update_freq: 1\n16:56:58 |     use_memories: False\n16:56:58 |     use_reply: none\n16:56:58 |     validation_cutoff: 1.0\n16:56:58 |     validation_every_n_epochs: -1\n16:56:58 |     validation_every_n_secs: 20.0\n16:56:58 |     validation_every_n_steps: -1\n16:56:58 |     validation_max_exs: -1\n16:56:58 |     validation_metric: accuracy\n16:56:58 |     validation_metric_mode: max\n16:56:58 |     validation_patience: 30\n16:56:58 |     validation_share_agent: False\n16:56:58 |     variant: xlm\n16:56:58 |     verbose: False\n16:56:58 |     wandb_entity: None\n16:56:58 |     wandb_log: False\n16:56:58 |     wandb_name: None\n16:56:58 |     wandb_project: None\n16:56:58 |     warmup_rate: 0.0001\n16:56:58 |     warmup_updates: 1000\n16:56:58 |     weight_decay: None\n16:56:58 |     world_logs: \n16:56:58 |     wrap_memory_encoder: False\n16:56:59 | creating task(s): fromfile:parlaiformat\n16:56:59 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt\n16:56:59 | training...\n16:57:09 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1850 1.85e-10               .2126                 .1947   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2340            .1554              .1724   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .1415 11.54     1 270.8 541.2       0          0 39.97  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .1850             32768  2.813    .1206  5.94 .7319 1.005e-06 118.8 237.4   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 389.6 778.7 2.003        .1823\n\n16:57:19 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6158 6.158e-10               .5642                 .6176   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5192            .6565              .6145   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7045  11.8     1   276  1052       0          0 76.26  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6158             32768  2.728    .1207 5.958 .6712 2.905e-06 119.2 454.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 395.1 1507 3.822        .6123\n\n16:57:19 | creating task(s): fromfile:parlaiformat\n16:57:19 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:57:19 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt\n16:57:19 | running eval: valid\n16:57:19 | eval completed in 0.20s\n16:57:19 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1815       0          0 132.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .6008 2.905e-06    72 794.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2609            1\n\u001b[0m\n16:57:19 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:57:19 | saving best valid model: /tmp/model3\n16:57:19 | Saving dictionary to /tmp/model3.dict\n16:57:23 | task solved! stopping.\n16:57:23 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:57:23 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:57:23 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:57:23 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:57:23 | Using CUDA\n16:57:23 | loading dictionary from /tmp/model3.dict\n16:57:23 | num words = 54944\n16:57:28 | Loading existing model parameters from /tmp/model3\n16:57:30 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:57:31 | creating task(s): fromfile:parlaiformat\n16:57:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:57:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt\n16:57:31 | running eval: valid\n16:57:32 | eval completed in 0.22s\n16:57:32 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1696       0          0 123.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .6008 2.905e-06    72 742.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2439            1\n\u001b[0m\n16:57:32 | creating task(s): fromfile:parlaiformat\n16:57:32 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:57:32 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt\n16:57:32 | running eval: test\n16:57:32 | eval completed in 0.22s\n16:57:32 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1596       0          0 116.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .6008 2.905e-06    72 698.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2294            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:57:34.121432Z","iopub.execute_input":"2022-12-03T16:57:34.122185Z","iopub.status.idle":"2022-12-03T16:58:01.748627Z","shell.execute_reply.started":"2022-12-03T16:57:34.122150Z","shell.execute_reply":"2022-12-03T16:58:01.747381Z"},"scrolled":true,"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"16:57:41 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt)\u001b[0m\n16:57:41 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:57:41 | Using CUDA\n16:57:41 | loading dictionary from /tmp/model3.dict\n16:57:41 | num words = 54944\n16:57:45 | Loading existing model parameters from /tmp/model3\n16:57:52 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:57:53 | Opt:\n16:57:53 |     activation: gelu\n16:57:53 |     adafactor_eps: '[1e-30, 0.001]'\n16:57:53 |     adam_eps: 1e-08\n16:57:53 |     add_p1_after_newln: False\n16:57:53 |     aggregate_micro: False\n16:57:53 |     allow_missing_init_opts: False\n16:57:53 |     area_under_curve_class: None\n16:57:53 |     area_under_curve_digits: -1\n16:57:53 |     attention_dropout: 0.1\n16:57:53 |     batchsize: 40\n16:57:53 |     betas: '[0.9, 0.999]'\n16:57:53 |     bpe_add_prefix_space: None\n16:57:53 |     bpe_debug: False\n16:57:53 |     bpe_dropout: None\n16:57:53 |     bpe_merge: None\n16:57:53 |     bpe_vocab: None\n16:57:53 |     candidates: inline\n16:57:53 |     cap_num_predictions: 100\n16:57:53 |     checkpoint_activations: False\n16:57:53 |     class_weights: None\n16:57:53 |     classes: \"['__notok__', '__ok__']\"\n16:57:53 |     classes_from_file: None\n16:57:53 |     data_parallel: True\n16:57:53 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:57:53 |     datatype: train\n16:57:53 |     delimiter: '\\n'\n16:57:53 |     dict_class: parlai.core.dict:DictionaryAgent\n16:57:53 |     dict_endtoken: __start__\n16:57:53 |     dict_file: /tmp/model3.dict\n16:57:53 |     dict_include_test: False\n16:57:53 |     dict_include_valid: False\n16:57:53 |     dict_initpath: None\n16:57:53 |     dict_language: english\n16:57:53 |     dict_loaded: True\n16:57:53 |     dict_lower: True\n16:57:53 |     dict_max_ngram_size: -1\n16:57:53 |     dict_maxexs: -1\n16:57:53 |     dict_maxtokens: -1\n16:57:53 |     dict_minfreq: 0\n16:57:53 |     dict_nulltoken: __null__\n16:57:53 |     dict_starttoken: __start__\n16:57:53 |     dict_textfields: text,labels\n16:57:53 |     dict_tokenizer: bpe\n16:57:53 |     dict_unktoken: __unk__\n16:57:53 |     display_examples: False\n16:57:53 |     download_path: None\n16:57:53 |     dropout: 0.1\n16:57:53 |     dynamic_batching: None\n16:57:53 |     embedding_projection: random\n16:57:53 |     embedding_size: 768\n16:57:53 |     embedding_type: random\n16:57:53 |     embeddings_scale: False\n16:57:53 |     encode_candidate_vecs: True\n16:57:53 |     encode_candidate_vecs_batchsize: 256\n16:57:53 |     eval_batchsize: None\n16:57:53 |     eval_candidates: inline\n16:57:53 |     eval_dynamic_batching: None\n16:57:53 |     evaltask: None\n16:57:53 |     ffn_size: 3072\n16:57:53 |     final_extra_opt: \n16:57:53 |     fixed_candidate_vecs: reuse\n16:57:53 |     fixed_candidates_path: None\n16:57:53 |     force_fp16_tokens: True\n16:57:53 |     fp16: True\n16:57:53 |     fp16_impl: safe\n16:57:53 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-a.txt\n16:57:53 |     fromfile_datatype_extension: False\n16:57:53 |     gpu: -1\n16:57:53 |     gradient_clip: 0.1\n16:57:53 |     hide_labels: False\n16:57:53 |     history_add_global_end_token: None\n16:57:53 |     history_reversed: False\n16:57:53 |     history_size: 20\n16:57:53 |     ignore_bad_candidates: False\n16:57:53 |     ignore_labels: None\n16:57:53 |     image_cropsize: 224\n16:57:53 |     image_mode: raw\n16:57:53 |     image_size: 256\n16:57:53 |     inference: max\n16:57:53 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:57:53 |     init_opt: None\n16:57:53 |     interactive_candidates: fixed\n16:57:53 |     interactive_mode: False\n16:57:53 |     invsqrt_lr_decay_gamma: -1\n16:57:53 |     is_debug: False\n16:57:53 |     label_truncate: 72\n16:57:53 |     learn_embeddings: True\n16:57:53 |     learn_positional_embeddings: True\n16:57:53 |     learningrate: 5e-05\n16:57:53 |     load_from_pretrained_ranker: True\n16:57:53 |     log_every_n_secs: 10.0\n16:57:53 |     log_every_n_steps: 50\n16:57:53 |     log_keep_fields: all\n16:57:53 |     loglevel: info\n16:57:53 |     lr_scheduler: reduceonplateau\n16:57:53 |     lr_scheduler_decay: 0.5\n16:57:53 |     lr_scheduler_patience: 3\n16:57:53 |     max_train_steps: -1\n16:57:53 |     max_train_time: 7200.0\n16:57:53 |     memory_attention: sqrt\n16:57:53 |     metrics: default\n16:57:53 |     model: transformer/classifier\n16:57:53 |     model_file: /tmp/model3\n16:57:53 |     model_parallel: False\n16:57:53 |     momentum: 0\n16:57:53 |     multitask_weights: [1]\n16:57:53 |     mutators: None\n16:57:53 |     n_decoder_layers: -1\n16:57:53 |     n_encoder_layers: -1\n16:57:53 |     n_heads: 12\n16:57:53 |     n_layers: 12\n16:57:53 |     n_positions: 1024\n16:57:53 |     n_segments: 2\n16:57:53 |     nesterov: True\n16:57:53 |     no_cuda: False\n16:57:53 |     normalize_sent_emb: False\n16:57:53 |     num_epochs: -1\n16:57:53 |     num_examples: -1\n16:57:53 |     num_workers: 0\n16:57:53 |     nus: [0.7]\n16:57:53 |     optimizer: adamax\n16:57:53 |     output_scaling: 0.06\n16:57:53 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n16:57:53 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:57:53 |     person_tokens: False\n16:57:53 |     print_scores: False\n16:57:53 |     rank_candidates: False\n16:57:53 |     rank_top_k: -1\n16:57:53 |     reduction_type: mean\n16:57:53 |     ref_class: None\n16:57:53 |     relu_dropout: 0.0\n16:57:53 |     repeat_blocking_heuristic: True\n16:57:53 |     report_filename: \n16:57:53 |     return_cand_scores: False\n16:57:53 |     save_after_valid: True\n16:57:53 |     save_every_n_secs: -1\n16:57:53 |     save_format: conversations\n16:57:53 |     share_encoders: False\n16:57:53 |     share_word_embeddings: False\n16:57:53 |     short_final_eval: False\n16:57:53 |     special_tok_lst: None\n16:57:53 |     split_lines: False\n16:57:53 |     starttime: Dec03_16-56\n16:57:53 |     task: fromfile:parlaiformat\n16:57:53 |     tensorboard_log: False\n16:57:53 |     tensorboard_logdir: None\n16:57:53 |     text_truncate: 360\n16:57:53 |     threshold: 0.5\n16:57:53 |     topk: 5\n16:57:53 |     train_predict: False\n16:57:53 |     truncate: 1024\n16:57:53 |     update_classifier_head_only: False\n16:57:53 |     update_freq: 1\n16:57:53 |     use_memories: False\n16:57:53 |     use_reply: none\n16:57:53 |     validation_cutoff: 1.0\n16:57:53 |     validation_every_n_epochs: -1\n16:57:53 |     validation_every_n_secs: 20.0\n16:57:53 |     validation_every_n_steps: -1\n16:57:53 |     validation_max_exs: -1\n16:57:53 |     validation_metric: accuracy\n16:57:53 |     validation_metric_mode: max\n16:57:53 |     validation_patience: 30\n16:57:53 |     validation_share_agent: False\n16:57:53 |     variant: xlm\n16:57:53 |     verbose: False\n16:57:53 |     wandb_entity: None\n16:57:53 |     wandb_log: False\n16:57:53 |     wandb_name: None\n16:57:53 |     wandb_project: None\n16:57:53 |     warmup_rate: 0.0001\n16:57:53 |     warmup_updates: 1000\n16:57:53 |     weight_decay: None\n16:57:53 |     world_logs: \n16:57:53 |     wrap_memory_encoder: False\n16:57:54 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:57:54 | creating task(s): fromfile:parlaiformat\n16:57:54 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:57:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-a.txt\n16:58:00 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .5800 5.8e-10               .4940                 .6212   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4100            .6410              .5597   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7500 11.47   539 474.5       0          0 35.22  200 .5800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6771 2.905e-06   240 211.3       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 685.8        .5675\u001b[0m\n16:58:00 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .5800 5.8e-10               .4940                 .6212   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4100            .6410              .5597   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7500 11.47   539 474.5       0          0 35.22  200 .5800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6771 2.905e-06   240 211.3       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 685.8        .5675\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:58:01.750811Z","iopub.execute_input":"2022-12-03T16:58:01.751216Z","iopub.status.idle":"2022-12-03T16:58:27.578261Z","shell.execute_reply.started":"2022-12-03T16:58:01.751175Z","shell.execute_reply":"2022-12-03T16:58:27.577084Z"},"scrolled":true,"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"16:58:08 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_valid.txt)\u001b[0m\n16:58:08 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:58:08 | Using CUDA\n16:58:08 | loading dictionary from /tmp/model3.dict\n16:58:08 | num words = 54944\n16:58:13 | Loading existing model parameters from /tmp/model3\n16:58:18 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:58:20 | Opt:\n16:58:20 |     activation: gelu\n16:58:20 |     adafactor_eps: '[1e-30, 0.001]'\n16:58:20 |     adam_eps: 1e-08\n16:58:20 |     add_p1_after_newln: False\n16:58:20 |     aggregate_micro: False\n16:58:20 |     allow_missing_init_opts: False\n16:58:20 |     area_under_curve_class: None\n16:58:20 |     area_under_curve_digits: -1\n16:58:20 |     attention_dropout: 0.1\n16:58:20 |     batchsize: 40\n16:58:20 |     betas: '[0.9, 0.999]'\n16:58:20 |     bpe_add_prefix_space: None\n16:58:20 |     bpe_debug: False\n16:58:20 |     bpe_dropout: None\n16:58:20 |     bpe_merge: None\n16:58:20 |     bpe_vocab: None\n16:58:20 |     candidates: inline\n16:58:20 |     cap_num_predictions: 100\n16:58:20 |     checkpoint_activations: False\n16:58:20 |     class_weights: None\n16:58:20 |     classes: \"['__notok__', '__ok__']\"\n16:58:20 |     classes_from_file: None\n16:58:20 |     data_parallel: True\n16:58:20 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:58:20 |     datatype: train\n16:58:20 |     delimiter: '\\n'\n16:58:20 |     dict_class: parlai.core.dict:DictionaryAgent\n16:58:20 |     dict_endtoken: __start__\n16:58:20 |     dict_file: /tmp/model3.dict\n16:58:20 |     dict_include_test: False\n16:58:20 |     dict_include_valid: False\n16:58:20 |     dict_initpath: None\n16:58:20 |     dict_language: english\n16:58:20 |     dict_loaded: True\n16:58:20 |     dict_lower: True\n16:58:20 |     dict_max_ngram_size: -1\n16:58:20 |     dict_maxexs: -1\n16:58:20 |     dict_maxtokens: -1\n16:58:20 |     dict_minfreq: 0\n16:58:20 |     dict_nulltoken: __null__\n16:58:20 |     dict_starttoken: __start__\n16:58:20 |     dict_textfields: text,labels\n16:58:20 |     dict_tokenizer: bpe\n16:58:20 |     dict_unktoken: __unk__\n16:58:20 |     display_examples: False\n16:58:20 |     download_path: None\n16:58:20 |     dropout: 0.1\n16:58:20 |     dynamic_batching: None\n16:58:20 |     embedding_projection: random\n16:58:20 |     embedding_size: 768\n16:58:20 |     embedding_type: random\n16:58:20 |     embeddings_scale: False\n16:58:20 |     encode_candidate_vecs: True\n16:58:20 |     encode_candidate_vecs_batchsize: 256\n16:58:20 |     eval_batchsize: None\n16:58:20 |     eval_candidates: inline\n16:58:20 |     eval_dynamic_batching: None\n16:58:20 |     evaltask: None\n16:58:20 |     ffn_size: 3072\n16:58:20 |     final_extra_opt: \n16:58:20 |     fixed_candidate_vecs: reuse\n16:58:20 |     fixed_candidates_path: None\n16:58:20 |     force_fp16_tokens: True\n16:58:20 |     fp16: True\n16:58:20 |     fp16_impl: safe\n16:58:20 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-b.txt\n16:58:20 |     fromfile_datatype_extension: False\n16:58:20 |     gpu: -1\n16:58:20 |     gradient_clip: 0.1\n16:58:20 |     hide_labels: False\n16:58:20 |     history_add_global_end_token: None\n16:58:20 |     history_reversed: False\n16:58:20 |     history_size: 20\n16:58:20 |     ignore_bad_candidates: False\n16:58:20 |     ignore_labels: None\n16:58:20 |     image_cropsize: 224\n16:58:20 |     image_mode: raw\n16:58:20 |     image_size: 256\n16:58:20 |     inference: max\n16:58:20 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:58:20 |     init_opt: None\n16:58:20 |     interactive_candidates: fixed\n16:58:20 |     interactive_mode: False\n16:58:20 |     invsqrt_lr_decay_gamma: -1\n16:58:20 |     is_debug: False\n16:58:20 |     label_truncate: 72\n16:58:20 |     learn_embeddings: True\n16:58:20 |     learn_positional_embeddings: True\n16:58:20 |     learningrate: 5e-05\n16:58:20 |     load_from_pretrained_ranker: True\n16:58:20 |     log_every_n_secs: 10.0\n16:58:20 |     log_every_n_steps: 50\n16:58:20 |     log_keep_fields: all\n16:58:20 |     loglevel: info\n16:58:20 |     lr_scheduler: reduceonplateau\n16:58:20 |     lr_scheduler_decay: 0.5\n16:58:20 |     lr_scheduler_patience: 3\n16:58:20 |     max_train_steps: -1\n16:58:20 |     max_train_time: 7200.0\n16:58:20 |     memory_attention: sqrt\n16:58:20 |     metrics: default\n16:58:20 |     model: transformer/classifier\n16:58:20 |     model_file: /tmp/model3\n16:58:20 |     model_parallel: False\n16:58:20 |     momentum: 0\n16:58:20 |     multitask_weights: [1]\n16:58:20 |     mutators: None\n16:58:20 |     n_decoder_layers: -1\n16:58:20 |     n_encoder_layers: -1\n16:58:20 |     n_heads: 12\n16:58:20 |     n_layers: 12\n16:58:20 |     n_positions: 1024\n16:58:20 |     n_segments: 2\n16:58:20 |     nesterov: True\n16:58:20 |     no_cuda: False\n16:58:20 |     normalize_sent_emb: False\n16:58:20 |     num_epochs: -1\n16:58:20 |     num_examples: -1\n16:58:20 |     num_workers: 0\n16:58:20 |     nus: [0.7]\n16:58:20 |     optimizer: adamax\n16:58:20 |     output_scaling: 0.06\n16:58:20 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n16:58:20 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:58:20 |     person_tokens: False\n16:58:20 |     print_scores: False\n16:58:20 |     rank_candidates: False\n16:58:20 |     rank_top_k: -1\n16:58:20 |     reduction_type: mean\n16:58:20 |     ref_class: None\n16:58:20 |     relu_dropout: 0.0\n16:58:20 |     repeat_blocking_heuristic: True\n16:58:20 |     report_filename: \n16:58:20 |     return_cand_scores: False\n16:58:20 |     save_after_valid: True\n16:58:20 |     save_every_n_secs: -1\n16:58:20 |     save_format: conversations\n16:58:20 |     share_encoders: False\n16:58:20 |     share_word_embeddings: False\n16:58:20 |     short_final_eval: False\n16:58:20 |     special_tok_lst: None\n16:58:20 |     split_lines: False\n16:58:20 |     starttime: Dec03_16-56\n16:58:20 |     task: fromfile:parlaiformat\n16:58:20 |     tensorboard_log: False\n16:58:20 |     tensorboard_logdir: None\n16:58:20 |     text_truncate: 360\n16:58:20 |     threshold: 0.5\n16:58:20 |     topk: 5\n16:58:20 |     train_predict: False\n16:58:20 |     truncate: 1024\n16:58:20 |     update_classifier_head_only: False\n16:58:20 |     update_freq: 1\n16:58:20 |     use_memories: False\n16:58:20 |     use_reply: none\n16:58:20 |     validation_cutoff: 1.0\n16:58:20 |     validation_every_n_epochs: -1\n16:58:20 |     validation_every_n_secs: 20.0\n16:58:20 |     validation_every_n_steps: -1\n16:58:20 |     validation_max_exs: -1\n16:58:20 |     validation_metric: accuracy\n16:58:20 |     validation_metric_mode: max\n16:58:20 |     validation_patience: 30\n16:58:20 |     validation_share_agent: False\n16:58:20 |     variant: xlm\n16:58:20 |     verbose: False\n16:58:20 |     wandb_entity: None\n16:58:20 |     wandb_log: False\n16:58:20 |     wandb_name: None\n16:58:20 |     wandb_project: None\n16:58:20 |     warmup_rate: 0.0001\n16:58:20 |     warmup_updates: 1000\n16:58:20 |     weight_decay: None\n16:58:20 |     world_logs: \n16:58:20 |     wrap_memory_encoder: False\n16:58:20 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:58:20 | creating task(s): fromfile:parlaiformat\n16:58:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:58:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run3/data_train-b.txt\n16:58:25 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .4200 4.2e-10               .3012                 .3788   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2500            .5043              .4403   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5900 11.47   539 514.9       0          0 38.21  200 .4200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7152 2.905e-06   240 229.3       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 744.2        .4027\u001b[0m\n16:58:25 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .4200 4.2e-10               .3012                 .3788   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2500            .5043              .4403   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5900 11.47   539 514.9       0          0 38.21  200 .4200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7152 2.905e-06   240 229.3       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 744.2        .4027\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:58:27.580275Z","iopub.execute_input":"2022-12-03T16:58:27.580690Z","iopub.status.idle":"2022-12-03T16:58:28.808817Z","shell.execute_reply.started":"2022-12-03T16:58:27.580650Z","shell.execute_reply":"2022-12-03T16:58:28.807334Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:58:28.814260Z","iopub.execute_input":"2022-12-03T16:58:28.814636Z","iopub.status.idle":"2022-12-03T16:59:21.069170Z","shell.execute_reply.started":"2022-12-03T16:58:28.814601Z","shell.execute_reply":"2022-12-03T16:59:21.067969Z"},"scrolled":true,"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"16:58:36 | building dictionary first...\n16:58:36 | No model with opt yet at: /tmp/model4(.opt)\n16:58:36 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n16:58:36 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:58:36 | Using CUDA\n16:58:36 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:58:36 | num words = 54944\n16:58:40 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:58:46 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:58:46 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n16:58:46 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n16:58:46 | Opt:\n16:58:46 |     activation: gelu\n16:58:46 |     adafactor_eps: '(1e-30, 0.001)'\n16:58:46 |     adam_eps: 1e-08\n16:58:46 |     add_p1_after_newln: False\n16:58:46 |     aggregate_micro: False\n16:58:46 |     allow_missing_init_opts: False\n16:58:46 |     attention_dropout: 0.1\n16:58:46 |     batchsize: 20\n16:58:46 |     betas: '(0.9, 0.999)'\n16:58:46 |     bpe_add_prefix_space: None\n16:58:46 |     bpe_debug: False\n16:58:46 |     bpe_dropout: None\n16:58:46 |     bpe_merge: None\n16:58:46 |     bpe_vocab: None\n16:58:46 |     candidates: inline\n16:58:46 |     cap_num_predictions: 100\n16:58:46 |     checkpoint_activations: False\n16:58:46 |     class_weights: None\n16:58:46 |     classes: \"['__notok__', '__ok__']\"\n16:58:46 |     classes_from_file: None\n16:58:46 |     data_parallel: True\n16:58:46 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:58:46 |     datatype: train\n16:58:46 |     delimiter: '\\n'\n16:58:46 |     dict_class: parlai.core.dict:DictionaryAgent\n16:58:46 |     dict_endtoken: __start__\n16:58:46 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n16:58:46 |     dict_include_test: False\n16:58:46 |     dict_include_valid: False\n16:58:46 |     dict_initpath: None\n16:58:46 |     dict_language: english\n16:58:46 |     dict_loaded: True\n16:58:46 |     dict_lower: True\n16:58:46 |     dict_max_ngram_size: -1\n16:58:46 |     dict_maxexs: -1\n16:58:46 |     dict_maxtokens: -1\n16:58:46 |     dict_minfreq: 0\n16:58:46 |     dict_nulltoken: __null__\n16:58:46 |     dict_starttoken: __start__\n16:58:46 |     dict_textfields: text,labels\n16:58:46 |     dict_tokenizer: bpe\n16:58:46 |     dict_unktoken: __unk__\n16:58:46 |     display_examples: False\n16:58:46 |     download_path: None\n16:58:46 |     dropout: 0.1\n16:58:46 |     dynamic_batching: None\n16:58:46 |     embedding_projection: random\n16:58:46 |     embedding_size: 768\n16:58:46 |     embedding_type: random\n16:58:46 |     embeddings_scale: False\n16:58:46 |     encode_candidate_vecs: True\n16:58:46 |     encode_candidate_vecs_batchsize: 256\n16:58:46 |     eval_batchsize: None\n16:58:46 |     eval_candidates: inline\n16:58:46 |     eval_dynamic_batching: None\n16:58:46 |     evaltask: None\n16:58:46 |     ffn_size: 3072\n16:58:46 |     final_extra_opt: \n16:58:46 |     fixed_candidate_vecs: reuse\n16:58:46 |     fixed_candidates_path: None\n16:58:46 |     force_fp16_tokens: False\n16:58:46 |     fp16: True\n16:58:46 |     fp16_impl: safe\n16:58:46 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt\n16:58:46 |     fromfile_datatype_extension: False\n16:58:46 |     gpu: -1\n16:58:46 |     gradient_clip: 0.1\n16:58:46 |     hide_labels: False\n16:58:46 |     history_add_global_end_token: None\n16:58:46 |     history_reversed: False\n16:58:46 |     history_size: 20\n16:58:46 |     ignore_bad_candidates: False\n16:58:46 |     ignore_labels: None\n16:58:46 |     image_cropsize: 224\n16:58:46 |     image_mode: raw\n16:58:46 |     image_size: 256\n16:58:46 |     inference: max\n16:58:46 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:58:46 |     init_opt: None\n16:58:46 |     interactive_candidates: fixed\n16:58:46 |     interactive_mode: False\n16:58:46 |     invsqrt_lr_decay_gamma: -1\n16:58:46 |     is_debug: False\n16:58:46 |     label_truncate: 72\n16:58:46 |     learn_embeddings: True\n16:58:46 |     learn_positional_embeddings: True\n16:58:46 |     learningrate: 5e-05\n16:58:46 |     load_from_checkpoint: False\n16:58:46 |     load_from_pretrained_ranker: True\n16:58:46 |     log_every_n_secs: 10.0\n16:58:46 |     log_every_n_steps: 50\n16:58:46 |     log_keep_fields: all\n16:58:46 |     loglevel: info\n16:58:46 |     lr_scheduler: reduceonplateau\n16:58:46 |     lr_scheduler_decay: 0.5\n16:58:46 |     lr_scheduler_patience: 3\n16:58:46 |     max_train_steps: -1\n16:58:46 |     max_train_time: 7200.0\n16:58:46 |     memory_attention: sqrt\n16:58:46 |     metrics: default\n16:58:46 |     model: transformer/classifier\n16:58:46 |     model_file: /tmp/model4\n16:58:46 |     model_parallel: False\n16:58:46 |     momentum: 0\n16:58:46 |     multitask_weights: [1]\n16:58:46 |     mutators: None\n16:58:46 |     n_decoder_layers: -1\n16:58:46 |     n_encoder_layers: -1\n16:58:46 |     n_heads: 12\n16:58:46 |     n_layers: 12\n16:58:46 |     n_positions: 1024\n16:58:46 |     n_segments: 2\n16:58:46 |     nesterov: True\n16:58:46 |     no_cuda: False\n16:58:46 |     normalize_sent_emb: False\n16:58:46 |     num_epochs: -1\n16:58:46 |     num_workers: 0\n16:58:46 |     nus: (0.7,)\n16:58:46 |     optimizer: adamax\n16:58:46 |     output_scaling: 0.06\n16:58:46 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n16:58:46 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:58:46 |     person_tokens: False\n16:58:46 |     print_scores: False\n16:58:46 |     rank_candidates: False\n16:58:46 |     rank_top_k: -1\n16:58:46 |     reduction_type: mean\n16:58:46 |     ref_class: None\n16:58:46 |     relu_dropout: 0.0\n16:58:46 |     repeat_blocking_heuristic: True\n16:58:46 |     return_cand_scores: False\n16:58:46 |     save_after_valid: True\n16:58:46 |     save_every_n_secs: -1\n16:58:46 |     save_format: conversations\n16:58:46 |     share_encoders: False\n16:58:46 |     share_word_embeddings: False\n16:58:46 |     short_final_eval: False\n16:58:46 |     special_tok_lst: None\n16:58:46 |     split_lines: False\n16:58:46 |     starttime: Dec03_16-58\n16:58:46 |     task: fromfile:parlaiformat\n16:58:46 |     tensorboard_log: False\n16:58:46 |     tensorboard_logdir: None\n16:58:46 |     text_truncate: 360\n16:58:46 |     threshold: 0.5\n16:58:46 |     topk: 5\n16:58:46 |     train_predict: False\n16:58:46 |     truncate: 1024\n16:58:46 |     update_classifier_head_only: False\n16:58:46 |     update_freq: 1\n16:58:46 |     use_memories: False\n16:58:46 |     use_reply: none\n16:58:46 |     validation_cutoff: 1.0\n16:58:46 |     validation_every_n_epochs: -1\n16:58:46 |     validation_every_n_secs: 20.0\n16:58:46 |     validation_every_n_steps: -1\n16:58:46 |     validation_max_exs: -1\n16:58:46 |     validation_metric: accuracy\n16:58:46 |     validation_metric_mode: max\n16:58:46 |     validation_patience: 30\n16:58:46 |     validation_share_agent: False\n16:58:46 |     variant: xlm\n16:58:46 |     verbose: False\n16:58:46 |     wandb_entity: None\n16:58:46 |     wandb_log: False\n16:58:46 |     wandb_name: None\n16:58:46 |     wandb_project: None\n16:58:46 |     warmup_rate: 0.0001\n16:58:46 |     warmup_updates: 1000\n16:58:46 |     weight_decay: None\n16:58:46 |     world_logs: \n16:58:46 |     wrap_memory_encoder: False\n16:58:46 | creating task(s): fromfile:parlaiformat\n16:58:46 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt\n16:58:46 | training...\n16:58:56 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .5800 5.8e-10               .3438                 .8462   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2157            .6912              .5402   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9592 12.22     1 284.4 565.3       0          0 39.75  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5800             32768  2.907    .1189  6.02 .6810 1.005e-06 120.4 239.3   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 404.8 804.6 1.992        .5140\n\n16:59:06 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8419 8.419e-10               .8163                 .9848   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6971            .8612              .7626   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9891  11.8     1   276  1026       0          0 74.36  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8419             32768  2.557    .1189 6.008 .6325 2.855e-06 120.2 446.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 396.2 1473 3.726        .8386\n\n16:59:06 | creating task(s): fromfile:parlaiformat\n16:59:06 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:59:06 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt\n16:59:06 | running eval: valid\n16:59:07 | eval completed in 0.19s\n16:59:07 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1945       0          0 138.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5697 2.855e-06    72 831.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2777            1\n\u001b[0m\n16:59:07 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n16:59:07 | saving best valid model: /tmp/model4\n16:59:07 | Saving dictionary to /tmp/model4.dict\n16:59:10 | task solved! stopping.\n16:59:10 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n16:59:10 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n16:59:10 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n16:59:10 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n16:59:10 | Using CUDA\n16:59:10 | loading dictionary from /tmp/model4.dict\n16:59:11 | num words = 54944\n16:59:16 | Loading existing model parameters from /tmp/model4\n16:59:17 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:59:18 | creating task(s): fromfile:parlaiformat\n16:59:18 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:59:18 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt\n16:59:18 | running eval: valid\n16:59:19 | eval completed in 0.20s\n16:59:19 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1872       0          0 133.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5697 2.855e-06    72 799.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2672            1\n\u001b[0m\n16:59:19 | creating task(s): fromfile:parlaiformat\n16:59:19 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:59:19 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt\n16:59:19 | running eval: test\n16:59:19 | eval completed in 0.20s\n16:59:19 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1902       0          0 135.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5697 2.855e-06    72 812.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2714            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:59:21.071082Z","iopub.execute_input":"2022-12-03T16:59:21.071469Z","iopub.status.idle":"2022-12-03T16:59:49.598365Z","shell.execute_reply.started":"2022-12-03T16:59:21.071423Z","shell.execute_reply":"2022-12-03T16:59:49.597173Z"},"scrolled":true,"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"16:59:28 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt)\u001b[0m\n16:59:28 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:59:28 | Using CUDA\n16:59:28 | loading dictionary from /tmp/model4.dict\n16:59:28 | num words = 54944\n16:59:33 | Loading existing model parameters from /tmp/model4\n16:59:40 | Total parameters: 128,042,498 (128,042,498 trainable)\n16:59:41 | Opt:\n16:59:41 |     activation: gelu\n16:59:41 |     adafactor_eps: '[1e-30, 0.001]'\n16:59:41 |     adam_eps: 1e-08\n16:59:41 |     add_p1_after_newln: False\n16:59:41 |     aggregate_micro: False\n16:59:41 |     allow_missing_init_opts: False\n16:59:41 |     area_under_curve_class: None\n16:59:41 |     area_under_curve_digits: -1\n16:59:41 |     attention_dropout: 0.1\n16:59:41 |     batchsize: 40\n16:59:41 |     betas: '[0.9, 0.999]'\n16:59:41 |     bpe_add_prefix_space: None\n16:59:41 |     bpe_debug: False\n16:59:41 |     bpe_dropout: None\n16:59:41 |     bpe_merge: None\n16:59:41 |     bpe_vocab: None\n16:59:41 |     candidates: inline\n16:59:41 |     cap_num_predictions: 100\n16:59:41 |     checkpoint_activations: False\n16:59:41 |     class_weights: None\n16:59:41 |     classes: \"['__notok__', '__ok__']\"\n16:59:41 |     classes_from_file: None\n16:59:41 |     data_parallel: True\n16:59:41 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n16:59:41 |     datatype: train\n16:59:41 |     delimiter: '\\n'\n16:59:41 |     dict_class: parlai.core.dict:DictionaryAgent\n16:59:41 |     dict_endtoken: __start__\n16:59:41 |     dict_file: /tmp/model4.dict\n16:59:41 |     dict_include_test: False\n16:59:41 |     dict_include_valid: False\n16:59:41 |     dict_initpath: None\n16:59:41 |     dict_language: english\n16:59:41 |     dict_loaded: True\n16:59:41 |     dict_lower: True\n16:59:41 |     dict_max_ngram_size: -1\n16:59:41 |     dict_maxexs: -1\n16:59:41 |     dict_maxtokens: -1\n16:59:41 |     dict_minfreq: 0\n16:59:41 |     dict_nulltoken: __null__\n16:59:41 |     dict_starttoken: __start__\n16:59:41 |     dict_textfields: text,labels\n16:59:41 |     dict_tokenizer: bpe\n16:59:41 |     dict_unktoken: __unk__\n16:59:41 |     display_examples: False\n16:59:41 |     download_path: None\n16:59:41 |     dropout: 0.1\n16:59:41 |     dynamic_batching: None\n16:59:41 |     embedding_projection: random\n16:59:41 |     embedding_size: 768\n16:59:41 |     embedding_type: random\n16:59:41 |     embeddings_scale: False\n16:59:41 |     encode_candidate_vecs: True\n16:59:41 |     encode_candidate_vecs_batchsize: 256\n16:59:41 |     eval_batchsize: None\n16:59:41 |     eval_candidates: inline\n16:59:41 |     eval_dynamic_batching: None\n16:59:41 |     evaltask: None\n16:59:41 |     ffn_size: 3072\n16:59:41 |     final_extra_opt: \n16:59:41 |     fixed_candidate_vecs: reuse\n16:59:41 |     fixed_candidates_path: None\n16:59:41 |     force_fp16_tokens: True\n16:59:41 |     fp16: True\n16:59:41 |     fp16_impl: safe\n16:59:41 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-a.txt\n16:59:41 |     fromfile_datatype_extension: False\n16:59:41 |     gpu: -1\n16:59:41 |     gradient_clip: 0.1\n16:59:41 |     hide_labels: False\n16:59:41 |     history_add_global_end_token: None\n16:59:41 |     history_reversed: False\n16:59:41 |     history_size: 20\n16:59:41 |     ignore_bad_candidates: False\n16:59:41 |     ignore_labels: None\n16:59:41 |     image_cropsize: 224\n16:59:41 |     image_mode: raw\n16:59:41 |     image_size: 256\n16:59:41 |     inference: max\n16:59:41 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n16:59:41 |     init_opt: None\n16:59:41 |     interactive_candidates: fixed\n16:59:41 |     interactive_mode: False\n16:59:41 |     invsqrt_lr_decay_gamma: -1\n16:59:41 |     is_debug: False\n16:59:41 |     label_truncate: 72\n16:59:41 |     learn_embeddings: True\n16:59:41 |     learn_positional_embeddings: True\n16:59:41 |     learningrate: 5e-05\n16:59:41 |     load_from_pretrained_ranker: True\n16:59:41 |     log_every_n_secs: 10.0\n16:59:41 |     log_every_n_steps: 50\n16:59:41 |     log_keep_fields: all\n16:59:41 |     loglevel: info\n16:59:41 |     lr_scheduler: reduceonplateau\n16:59:41 |     lr_scheduler_decay: 0.5\n16:59:41 |     lr_scheduler_patience: 3\n16:59:41 |     max_train_steps: -1\n16:59:41 |     max_train_time: 7200.0\n16:59:41 |     memory_attention: sqrt\n16:59:41 |     metrics: default\n16:59:41 |     model: transformer/classifier\n16:59:41 |     model_file: /tmp/model4\n16:59:41 |     model_parallel: False\n16:59:41 |     momentum: 0\n16:59:41 |     multitask_weights: [1]\n16:59:41 |     mutators: None\n16:59:41 |     n_decoder_layers: -1\n16:59:41 |     n_encoder_layers: -1\n16:59:41 |     n_heads: 12\n16:59:41 |     n_layers: 12\n16:59:41 |     n_positions: 1024\n16:59:41 |     n_segments: 2\n16:59:41 |     nesterov: True\n16:59:41 |     no_cuda: False\n16:59:41 |     normalize_sent_emb: False\n16:59:41 |     num_epochs: -1\n16:59:41 |     num_examples: -1\n16:59:41 |     num_workers: 0\n16:59:41 |     nus: [0.7]\n16:59:41 |     optimizer: adamax\n16:59:41 |     output_scaling: 0.06\n16:59:41 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n16:59:41 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n16:59:41 |     person_tokens: False\n16:59:41 |     print_scores: False\n16:59:41 |     rank_candidates: False\n16:59:41 |     rank_top_k: -1\n16:59:41 |     reduction_type: mean\n16:59:41 |     ref_class: None\n16:59:41 |     relu_dropout: 0.0\n16:59:41 |     repeat_blocking_heuristic: True\n16:59:41 |     report_filename: \n16:59:41 |     return_cand_scores: False\n16:59:41 |     save_after_valid: True\n16:59:41 |     save_every_n_secs: -1\n16:59:41 |     save_format: conversations\n16:59:41 |     share_encoders: False\n16:59:41 |     share_word_embeddings: False\n16:59:41 |     short_final_eval: False\n16:59:41 |     special_tok_lst: None\n16:59:41 |     split_lines: False\n16:59:41 |     starttime: Dec03_16-58\n16:59:41 |     task: fromfile:parlaiformat\n16:59:41 |     tensorboard_log: False\n16:59:41 |     tensorboard_logdir: None\n16:59:41 |     text_truncate: 360\n16:59:41 |     threshold: 0.5\n16:59:41 |     topk: 5\n16:59:41 |     train_predict: False\n16:59:41 |     truncate: 1024\n16:59:41 |     update_classifier_head_only: False\n16:59:41 |     update_freq: 1\n16:59:41 |     use_memories: False\n16:59:41 |     use_reply: none\n16:59:41 |     validation_cutoff: 1.0\n16:59:41 |     validation_every_n_epochs: -1\n16:59:41 |     validation_every_n_secs: 20.0\n16:59:41 |     validation_every_n_steps: -1\n16:59:41 |     validation_max_exs: -1\n16:59:41 |     validation_metric: accuracy\n16:59:41 |     validation_metric_mode: max\n16:59:41 |     validation_patience: 30\n16:59:41 |     validation_share_agent: False\n16:59:41 |     variant: xlm\n16:59:41 |     verbose: False\n16:59:41 |     wandb_entity: None\n16:59:41 |     wandb_log: False\n16:59:41 |     wandb_name: None\n16:59:41 |     wandb_project: None\n16:59:41 |     warmup_rate: 0.0001\n16:59:41 |     warmup_updates: 1000\n16:59:41 |     weight_decay: None\n16:59:41 |     world_logs: \n16:59:41 |     wrap_memory_encoder: False\n16:59:42 | Evaluating task fromfile:parlaiformat using datatype valid.\n16:59:42 | creating task(s): fromfile:parlaiformat\n16:59:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n16:59:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-a.txt\n16:59:48 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6800 6.8e-10               .6768                 .6768   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6768            .6832              .6832   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6832 11.46 538.2 496.6       0          0  36.9  200 .6800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .6652 2.855e-06 239.6 221.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 777.8 717.6        .6800\u001b[0m\n16:59:48 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6800 6.8e-10               .6768                 .6768   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6768            .6832              .6832   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6832 11.46 538.2 496.6       0          0  36.9  200 .6800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .6652 2.855e-06 239.6 221.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 777.8 717.6        .6800\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T16:59:49.600205Z","iopub.execute_input":"2022-12-03T16:59:49.600653Z","iopub.status.idle":"2022-12-03T17:00:15.875045Z","shell.execute_reply.started":"2022-12-03T16:59:49.600610Z","shell.execute_reply":"2022-12-03T17:00:15.873694Z"},"scrolled":true,"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"16:59:56 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_valid.txt)\u001b[0m\n16:59:56 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n16:59:56 | Using CUDA\n16:59:56 | loading dictionary from /tmp/model4.dict\n16:59:56 | num words = 54944\n17:00:01 | Loading existing model parameters from /tmp/model4\n17:00:07 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:00:08 | Opt:\n17:00:08 |     activation: gelu\n17:00:08 |     adafactor_eps: '[1e-30, 0.001]'\n17:00:08 |     adam_eps: 1e-08\n17:00:08 |     add_p1_after_newln: False\n17:00:08 |     aggregate_micro: False\n17:00:08 |     allow_missing_init_opts: False\n17:00:08 |     area_under_curve_class: None\n17:00:08 |     area_under_curve_digits: -1\n17:00:08 |     attention_dropout: 0.1\n17:00:08 |     batchsize: 40\n17:00:08 |     betas: '[0.9, 0.999]'\n17:00:08 |     bpe_add_prefix_space: None\n17:00:08 |     bpe_debug: False\n17:00:08 |     bpe_dropout: None\n17:00:08 |     bpe_merge: None\n17:00:08 |     bpe_vocab: None\n17:00:08 |     candidates: inline\n17:00:08 |     cap_num_predictions: 100\n17:00:08 |     checkpoint_activations: False\n17:00:08 |     class_weights: None\n17:00:08 |     classes: \"['__notok__', '__ok__']\"\n17:00:08 |     classes_from_file: None\n17:00:08 |     data_parallel: True\n17:00:08 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:00:08 |     datatype: train\n17:00:08 |     delimiter: '\\n'\n17:00:08 |     dict_class: parlai.core.dict:DictionaryAgent\n17:00:08 |     dict_endtoken: __start__\n17:00:08 |     dict_file: /tmp/model4.dict\n17:00:08 |     dict_include_test: False\n17:00:08 |     dict_include_valid: False\n17:00:08 |     dict_initpath: None\n17:00:08 |     dict_language: english\n17:00:08 |     dict_loaded: True\n17:00:08 |     dict_lower: True\n17:00:08 |     dict_max_ngram_size: -1\n17:00:08 |     dict_maxexs: -1\n17:00:08 |     dict_maxtokens: -1\n17:00:08 |     dict_minfreq: 0\n17:00:08 |     dict_nulltoken: __null__\n17:00:08 |     dict_starttoken: __start__\n17:00:08 |     dict_textfields: text,labels\n17:00:08 |     dict_tokenizer: bpe\n17:00:08 |     dict_unktoken: __unk__\n17:00:08 |     display_examples: False\n17:00:08 |     download_path: None\n17:00:08 |     dropout: 0.1\n17:00:08 |     dynamic_batching: None\n17:00:08 |     embedding_projection: random\n17:00:08 |     embedding_size: 768\n17:00:08 |     embedding_type: random\n17:00:08 |     embeddings_scale: False\n17:00:08 |     encode_candidate_vecs: True\n17:00:08 |     encode_candidate_vecs_batchsize: 256\n17:00:08 |     eval_batchsize: None\n17:00:08 |     eval_candidates: inline\n17:00:08 |     eval_dynamic_batching: None\n17:00:08 |     evaltask: None\n17:00:08 |     ffn_size: 3072\n17:00:08 |     final_extra_opt: \n17:00:08 |     fixed_candidate_vecs: reuse\n17:00:08 |     fixed_candidates_path: None\n17:00:08 |     force_fp16_tokens: True\n17:00:08 |     fp16: True\n17:00:08 |     fp16_impl: safe\n17:00:08 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-b.txt\n17:00:08 |     fromfile_datatype_extension: False\n17:00:08 |     gpu: -1\n17:00:08 |     gradient_clip: 0.1\n17:00:08 |     hide_labels: False\n17:00:08 |     history_add_global_end_token: None\n17:00:08 |     history_reversed: False\n17:00:08 |     history_size: 20\n17:00:08 |     ignore_bad_candidates: False\n17:00:08 |     ignore_labels: None\n17:00:08 |     image_cropsize: 224\n17:00:08 |     image_mode: raw\n17:00:08 |     image_size: 256\n17:00:08 |     inference: max\n17:00:08 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:00:08 |     init_opt: None\n17:00:08 |     interactive_candidates: fixed\n17:00:08 |     interactive_mode: False\n17:00:08 |     invsqrt_lr_decay_gamma: -1\n17:00:08 |     is_debug: False\n17:00:08 |     label_truncate: 72\n17:00:08 |     learn_embeddings: True\n17:00:08 |     learn_positional_embeddings: True\n17:00:08 |     learningrate: 5e-05\n17:00:08 |     load_from_pretrained_ranker: True\n17:00:08 |     log_every_n_secs: 10.0\n17:00:08 |     log_every_n_steps: 50\n17:00:08 |     log_keep_fields: all\n17:00:08 |     loglevel: info\n17:00:08 |     lr_scheduler: reduceonplateau\n17:00:08 |     lr_scheduler_decay: 0.5\n17:00:08 |     lr_scheduler_patience: 3\n17:00:08 |     max_train_steps: -1\n17:00:08 |     max_train_time: 7200.0\n17:00:08 |     memory_attention: sqrt\n17:00:08 |     metrics: default\n17:00:08 |     model: transformer/classifier\n17:00:08 |     model_file: /tmp/model4\n17:00:08 |     model_parallel: False\n17:00:08 |     momentum: 0\n17:00:08 |     multitask_weights: [1]\n17:00:08 |     mutators: None\n17:00:08 |     n_decoder_layers: -1\n17:00:08 |     n_encoder_layers: -1\n17:00:08 |     n_heads: 12\n17:00:08 |     n_layers: 12\n17:00:08 |     n_positions: 1024\n17:00:08 |     n_segments: 2\n17:00:08 |     nesterov: True\n17:00:08 |     no_cuda: False\n17:00:08 |     normalize_sent_emb: False\n17:00:08 |     num_epochs: -1\n17:00:08 |     num_examples: -1\n17:00:08 |     num_workers: 0\n17:00:08 |     nus: [0.7]\n17:00:08 |     optimizer: adamax\n17:00:08 |     output_scaling: 0.06\n17:00:08 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n17:00:08 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:00:08 |     person_tokens: False\n17:00:08 |     print_scores: False\n17:00:08 |     rank_candidates: False\n17:00:08 |     rank_top_k: -1\n17:00:08 |     reduction_type: mean\n17:00:08 |     ref_class: None\n17:00:08 |     relu_dropout: 0.0\n17:00:08 |     repeat_blocking_heuristic: True\n17:00:08 |     report_filename: \n17:00:08 |     return_cand_scores: False\n17:00:08 |     save_after_valid: True\n17:00:08 |     save_every_n_secs: -1\n17:00:08 |     save_format: conversations\n17:00:08 |     share_encoders: False\n17:00:08 |     share_word_embeddings: False\n17:00:08 |     short_final_eval: False\n17:00:08 |     special_tok_lst: None\n17:00:08 |     split_lines: False\n17:00:08 |     starttime: Dec03_16-58\n17:00:08 |     task: fromfile:parlaiformat\n17:00:08 |     tensorboard_log: False\n17:00:08 |     tensorboard_logdir: None\n17:00:08 |     text_truncate: 360\n17:00:08 |     threshold: 0.5\n17:00:08 |     topk: 5\n17:00:08 |     train_predict: False\n17:00:08 |     truncate: 1024\n17:00:08 |     update_classifier_head_only: False\n17:00:08 |     update_freq: 1\n17:00:08 |     use_memories: False\n17:00:08 |     use_reply: none\n17:00:08 |     validation_cutoff: 1.0\n17:00:08 |     validation_every_n_epochs: -1\n17:00:08 |     validation_every_n_secs: 20.0\n17:00:08 |     validation_every_n_steps: -1\n17:00:08 |     validation_max_exs: -1\n17:00:08 |     validation_metric: accuracy\n17:00:08 |     validation_metric_mode: max\n17:00:08 |     validation_patience: 30\n17:00:08 |     validation_share_agent: False\n17:00:08 |     variant: xlm\n17:00:08 |     verbose: False\n17:00:08 |     wandb_entity: None\n17:00:08 |     wandb_log: False\n17:00:08 |     wandb_name: None\n17:00:08 |     wandb_project: None\n17:00:08 |     warmup_rate: 0.0001\n17:00:08 |     warmup_updates: 1000\n17:00:08 |     weight_decay: None\n17:00:08 |     world_logs: \n17:00:08 |     wrap_memory_encoder: False\n17:00:08 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:00:08 | creating task(s): fromfile:parlaiformat\n17:00:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:00:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run4/data_train-b.txt\n17:00:14 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3200 3.2e-10               .3200                 .3232   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3168            .3200              .3168   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3232 11.46 538.2 517.8       0          0 38.48  200 .3200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .7270 2.855e-06 240.4 231.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 778.6 749.1        .3200\u001b[0m\n17:00:14 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3200 3.2e-10               .3200                 .3232   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3168            .3200              .3168   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3232 11.46 538.2 517.8       0          0 38.48  200 .3200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .7270 2.855e-06 240.4 231.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 778.6 749.1        .3200\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:00:15.877061Z","iopub.execute_input":"2022-12-03T17:00:15.877462Z","iopub.status.idle":"2022-12-03T17:00:16.982943Z","shell.execute_reply.started":"2022-12-03T17:00:15.877420Z","shell.execute_reply":"2022-12-03T17:00:16.980956Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:00:16.990660Z","iopub.execute_input":"2022-12-03T17:00:16.995047Z","iopub.status.idle":"2022-12-03T17:01:08.712003Z","shell.execute_reply.started":"2022-12-03T17:00:16.994986Z","shell.execute_reply":"2022-12-03T17:01:08.710473Z"},"scrolled":true,"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"17:00:24 | building dictionary first...\n17:00:24 | No model with opt yet at: /tmp/model5(.opt)\n17:00:24 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:00:24 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:00:24 | Using CUDA\n17:00:24 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:00:24 | num words = 54944\n17:00:28 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:00:34 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:00:34 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:00:34 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:00:34 | Opt:\n17:00:34 |     activation: gelu\n17:00:34 |     adafactor_eps: '(1e-30, 0.001)'\n17:00:34 |     adam_eps: 1e-08\n17:00:34 |     add_p1_after_newln: False\n17:00:34 |     aggregate_micro: False\n17:00:34 |     allow_missing_init_opts: False\n17:00:34 |     attention_dropout: 0.1\n17:00:34 |     batchsize: 20\n17:00:34 |     betas: '(0.9, 0.999)'\n17:00:34 |     bpe_add_prefix_space: None\n17:00:34 |     bpe_debug: False\n17:00:34 |     bpe_dropout: None\n17:00:34 |     bpe_merge: None\n17:00:34 |     bpe_vocab: None\n17:00:34 |     candidates: inline\n17:00:34 |     cap_num_predictions: 100\n17:00:34 |     checkpoint_activations: False\n17:00:34 |     class_weights: None\n17:00:34 |     classes: \"['__notok__', '__ok__']\"\n17:00:34 |     classes_from_file: None\n17:00:34 |     data_parallel: True\n17:00:34 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:00:34 |     datatype: train\n17:00:34 |     delimiter: '\\n'\n17:00:34 |     dict_class: parlai.core.dict:DictionaryAgent\n17:00:34 |     dict_endtoken: __start__\n17:00:34 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:00:34 |     dict_include_test: False\n17:00:34 |     dict_include_valid: False\n17:00:34 |     dict_initpath: None\n17:00:34 |     dict_language: english\n17:00:34 |     dict_loaded: True\n17:00:34 |     dict_lower: True\n17:00:34 |     dict_max_ngram_size: -1\n17:00:34 |     dict_maxexs: -1\n17:00:34 |     dict_maxtokens: -1\n17:00:34 |     dict_minfreq: 0\n17:00:34 |     dict_nulltoken: __null__\n17:00:34 |     dict_starttoken: __start__\n17:00:34 |     dict_textfields: text,labels\n17:00:34 |     dict_tokenizer: bpe\n17:00:34 |     dict_unktoken: __unk__\n17:00:34 |     display_examples: False\n17:00:34 |     download_path: None\n17:00:34 |     dropout: 0.1\n17:00:34 |     dynamic_batching: None\n17:00:34 |     embedding_projection: random\n17:00:34 |     embedding_size: 768\n17:00:34 |     embedding_type: random\n17:00:34 |     embeddings_scale: False\n17:00:34 |     encode_candidate_vecs: True\n17:00:34 |     encode_candidate_vecs_batchsize: 256\n17:00:34 |     eval_batchsize: None\n17:00:34 |     eval_candidates: inline\n17:00:34 |     eval_dynamic_batching: None\n17:00:34 |     evaltask: None\n17:00:34 |     ffn_size: 3072\n17:00:34 |     final_extra_opt: \n17:00:34 |     fixed_candidate_vecs: reuse\n17:00:34 |     fixed_candidates_path: None\n17:00:34 |     force_fp16_tokens: False\n17:00:34 |     fp16: True\n17:00:34 |     fp16_impl: safe\n17:00:34 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt\n17:00:34 |     fromfile_datatype_extension: False\n17:00:34 |     gpu: -1\n17:00:34 |     gradient_clip: 0.1\n17:00:34 |     hide_labels: False\n17:00:34 |     history_add_global_end_token: None\n17:00:34 |     history_reversed: False\n17:00:34 |     history_size: 20\n17:00:34 |     ignore_bad_candidates: False\n17:00:34 |     ignore_labels: None\n17:00:34 |     image_cropsize: 224\n17:00:34 |     image_mode: raw\n17:00:34 |     image_size: 256\n17:00:34 |     inference: max\n17:00:34 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:00:34 |     init_opt: None\n17:00:34 |     interactive_candidates: fixed\n17:00:34 |     interactive_mode: False\n17:00:34 |     invsqrt_lr_decay_gamma: -1\n17:00:34 |     is_debug: False\n17:00:34 |     label_truncate: 72\n17:00:34 |     learn_embeddings: True\n17:00:34 |     learn_positional_embeddings: True\n17:00:34 |     learningrate: 5e-05\n17:00:34 |     load_from_checkpoint: False\n17:00:34 |     load_from_pretrained_ranker: True\n17:00:34 |     log_every_n_secs: 10.0\n17:00:34 |     log_every_n_steps: 50\n17:00:34 |     log_keep_fields: all\n17:00:34 |     loglevel: info\n17:00:34 |     lr_scheduler: reduceonplateau\n17:00:34 |     lr_scheduler_decay: 0.5\n17:00:34 |     lr_scheduler_patience: 3\n17:00:34 |     max_train_steps: -1\n17:00:34 |     max_train_time: 7200.0\n17:00:34 |     memory_attention: sqrt\n17:00:34 |     metrics: default\n17:00:34 |     model: transformer/classifier\n17:00:34 |     model_file: /tmp/model5\n17:00:34 |     model_parallel: False\n17:00:34 |     momentum: 0\n17:00:34 |     multitask_weights: [1]\n17:00:34 |     mutators: None\n17:00:34 |     n_decoder_layers: -1\n17:00:34 |     n_encoder_layers: -1\n17:00:34 |     n_heads: 12\n17:00:34 |     n_layers: 12\n17:00:34 |     n_positions: 1024\n17:00:34 |     n_segments: 2\n17:00:34 |     nesterov: True\n17:00:34 |     no_cuda: False\n17:00:34 |     normalize_sent_emb: False\n17:00:34 |     num_epochs: -1\n17:00:34 |     num_workers: 0\n17:00:34 |     nus: (0.7,)\n17:00:34 |     optimizer: adamax\n17:00:34 |     output_scaling: 0.06\n17:00:34 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n17:00:34 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:00:34 |     person_tokens: False\n17:00:34 |     print_scores: False\n17:00:34 |     rank_candidates: False\n17:00:34 |     rank_top_k: -1\n17:00:34 |     reduction_type: mean\n17:00:34 |     ref_class: None\n17:00:34 |     relu_dropout: 0.0\n17:00:34 |     repeat_blocking_heuristic: True\n17:00:34 |     return_cand_scores: False\n17:00:34 |     save_after_valid: True\n17:00:34 |     save_every_n_secs: -1\n17:00:34 |     save_format: conversations\n17:00:34 |     share_encoders: False\n17:00:34 |     share_word_embeddings: False\n17:00:34 |     short_final_eval: False\n17:00:34 |     special_tok_lst: None\n17:00:34 |     split_lines: False\n17:00:34 |     starttime: Dec03_17-00\n17:00:34 |     task: fromfile:parlaiformat\n17:00:34 |     tensorboard_log: False\n17:00:34 |     tensorboard_logdir: None\n17:00:34 |     text_truncate: 360\n17:00:34 |     threshold: 0.5\n17:00:34 |     topk: 5\n17:00:34 |     train_predict: False\n17:00:34 |     truncate: 1024\n17:00:34 |     update_classifier_head_only: False\n17:00:34 |     update_freq: 1\n17:00:34 |     use_memories: False\n17:00:34 |     use_reply: none\n17:00:34 |     validation_cutoff: 1.0\n17:00:34 |     validation_every_n_epochs: -1\n17:00:34 |     validation_every_n_secs: 20.0\n17:00:34 |     validation_every_n_steps: -1\n17:00:34 |     validation_max_exs: -1\n17:00:34 |     validation_metric: accuracy\n17:00:34 |     validation_metric_mode: max\n17:00:34 |     validation_patience: 30\n17:00:34 |     validation_share_agent: False\n17:00:34 |     variant: xlm\n17:00:34 |     verbose: False\n17:00:34 |     wandb_entity: None\n17:00:34 |     wandb_log: False\n17:00:34 |     wandb_name: None\n17:00:34 |     wandb_project: None\n17:00:34 |     warmup_rate: 0.0001\n17:00:34 |     warmup_updates: 1000\n17:00:34 |     weight_decay: None\n17:00:34 |     world_logs: \n17:00:34 |     wrap_memory_encoder: False\n17:00:34 | creating task(s): fromfile:parlaiformat\n17:00:34 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt\n17:00:34 | training...\n17:00:44 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .5650 5.65e-10               .4790                 .5839   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4061            .6266              .5551   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7192 10.79     1 255.8 504.2       0          0 39.42  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5650             32768  2.971    .1206 5.985 .6827 1.005e-06 119.7 235.9   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 375.5 740.2 1.976        .5539\n\n17:00:54 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8355 8.355e-10               .8120                 .9441   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7124            .8538              .7700   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9580 10.57     1 251.5 987.4       0          0 78.52  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8355             32768  2.961    .1207 5.997 .6300 2.905e-06 119.9 470.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 371.4 1458 3.935        .8330\n\n17:00:54 | creating task(s): fromfile:parlaiformat\n17:00:54 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:00:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt\n17:00:54 | running eval: valid\n17:00:54 | eval completed in 0.19s\n17:00:54 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1813       0          0   143   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5518 2.905e-06    72 858.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  224 2671            1\n\u001b[0m\n17:00:54 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:00:54 | saving best valid model: /tmp/model5\n17:00:54 | Saving dictionary to /tmp/model5.dict\n17:00:58 | task solved! stopping.\n17:00:58 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:00:58 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:00:58 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:00:58 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:00:58 | Using CUDA\n17:00:58 | loading dictionary from /tmp/model5.dict\n17:00:58 | num words = 54944\n17:01:03 | Loading existing model parameters from /tmp/model5\n17:01:05 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:01:06 | creating task(s): fromfile:parlaiformat\n17:01:06 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:01:06 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt\n17:01:06 | running eval: valid\n17:01:06 | eval completed in 0.20s\n17:01:06 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1709       0          0 134.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5518 2.905e-06    72 809.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  224 2518            1\n\u001b[0m\n17:01:06 | creating task(s): fromfile:parlaiformat\n17:01:06 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:01:06 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt\n17:01:06 | running eval: test\n17:01:06 | eval completed in 0.19s\n17:01:06 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1765       0          0 139.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5518 2.905e-06    72 836.1       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  224 2602            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:01:08.713948Z","iopub.execute_input":"2022-12-03T17:01:08.714392Z","iopub.status.idle":"2022-12-03T17:01:36.546832Z","shell.execute_reply.started":"2022-12-03T17:01:08.714350Z","shell.execute_reply":"2022-12-03T17:01:36.545645Z"},"scrolled":true,"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"17:01:16 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt)\u001b[0m\n17:01:16 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:01:16 | Using CUDA\n17:01:16 | loading dictionary from /tmp/model5.dict\n17:01:16 | num words = 54944\n17:01:20 | Loading existing model parameters from /tmp/model5\n17:01:27 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:01:28 | Opt:\n17:01:28 |     activation: gelu\n17:01:28 |     adafactor_eps: '[1e-30, 0.001]'\n17:01:28 |     adam_eps: 1e-08\n17:01:28 |     add_p1_after_newln: False\n17:01:28 |     aggregate_micro: False\n17:01:28 |     allow_missing_init_opts: False\n17:01:28 |     area_under_curve_class: None\n17:01:28 |     area_under_curve_digits: -1\n17:01:28 |     attention_dropout: 0.1\n17:01:28 |     batchsize: 40\n17:01:28 |     betas: '[0.9, 0.999]'\n17:01:28 |     bpe_add_prefix_space: None\n17:01:28 |     bpe_debug: False\n17:01:28 |     bpe_dropout: None\n17:01:28 |     bpe_merge: None\n17:01:28 |     bpe_vocab: None\n17:01:28 |     candidates: inline\n17:01:28 |     cap_num_predictions: 100\n17:01:28 |     checkpoint_activations: False\n17:01:28 |     class_weights: None\n17:01:28 |     classes: \"['__notok__', '__ok__']\"\n17:01:28 |     classes_from_file: None\n17:01:28 |     data_parallel: True\n17:01:28 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:01:28 |     datatype: train\n17:01:28 |     delimiter: '\\n'\n17:01:28 |     dict_class: parlai.core.dict:DictionaryAgent\n17:01:28 |     dict_endtoken: __start__\n17:01:28 |     dict_file: /tmp/model5.dict\n17:01:28 |     dict_include_test: False\n17:01:28 |     dict_include_valid: False\n17:01:28 |     dict_initpath: None\n17:01:28 |     dict_language: english\n17:01:28 |     dict_loaded: True\n17:01:28 |     dict_lower: True\n17:01:28 |     dict_max_ngram_size: -1\n17:01:28 |     dict_maxexs: -1\n17:01:28 |     dict_maxtokens: -1\n17:01:28 |     dict_minfreq: 0\n17:01:28 |     dict_nulltoken: __null__\n17:01:28 |     dict_starttoken: __start__\n17:01:28 |     dict_textfields: text,labels\n17:01:28 |     dict_tokenizer: bpe\n17:01:28 |     dict_unktoken: __unk__\n17:01:28 |     display_examples: False\n17:01:28 |     download_path: None\n17:01:28 |     dropout: 0.1\n17:01:28 |     dynamic_batching: None\n17:01:28 |     embedding_projection: random\n17:01:28 |     embedding_size: 768\n17:01:28 |     embedding_type: random\n17:01:28 |     embeddings_scale: False\n17:01:28 |     encode_candidate_vecs: True\n17:01:28 |     encode_candidate_vecs_batchsize: 256\n17:01:28 |     eval_batchsize: None\n17:01:28 |     eval_candidates: inline\n17:01:28 |     eval_dynamic_batching: None\n17:01:28 |     evaltask: None\n17:01:28 |     ffn_size: 3072\n17:01:28 |     final_extra_opt: \n17:01:28 |     fixed_candidate_vecs: reuse\n17:01:28 |     fixed_candidates_path: None\n17:01:28 |     force_fp16_tokens: True\n17:01:28 |     fp16: True\n17:01:28 |     fp16_impl: safe\n17:01:28 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-a.txt\n17:01:28 |     fromfile_datatype_extension: False\n17:01:28 |     gpu: -1\n17:01:28 |     gradient_clip: 0.1\n17:01:28 |     hide_labels: False\n17:01:28 |     history_add_global_end_token: None\n17:01:28 |     history_reversed: False\n17:01:28 |     history_size: 20\n17:01:28 |     ignore_bad_candidates: False\n17:01:28 |     ignore_labels: None\n17:01:28 |     image_cropsize: 224\n17:01:28 |     image_mode: raw\n17:01:28 |     image_size: 256\n17:01:28 |     inference: max\n17:01:28 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:01:28 |     init_opt: None\n17:01:28 |     interactive_candidates: fixed\n17:01:28 |     interactive_mode: False\n17:01:28 |     invsqrt_lr_decay_gamma: -1\n17:01:28 |     is_debug: False\n17:01:28 |     label_truncate: 72\n17:01:28 |     learn_embeddings: True\n17:01:28 |     learn_positional_embeddings: True\n17:01:28 |     learningrate: 5e-05\n17:01:28 |     load_from_pretrained_ranker: True\n17:01:28 |     log_every_n_secs: 10.0\n17:01:28 |     log_every_n_steps: 50\n17:01:28 |     log_keep_fields: all\n17:01:28 |     loglevel: info\n17:01:28 |     lr_scheduler: reduceonplateau\n17:01:28 |     lr_scheduler_decay: 0.5\n17:01:28 |     lr_scheduler_patience: 3\n17:01:28 |     max_train_steps: -1\n17:01:28 |     max_train_time: 7200.0\n17:01:28 |     memory_attention: sqrt\n17:01:28 |     metrics: default\n17:01:28 |     model: transformer/classifier\n17:01:28 |     model_file: /tmp/model5\n17:01:28 |     model_parallel: False\n17:01:28 |     momentum: 0\n17:01:28 |     multitask_weights: [1]\n17:01:28 |     mutators: None\n17:01:28 |     n_decoder_layers: -1\n17:01:28 |     n_encoder_layers: -1\n17:01:28 |     n_heads: 12\n17:01:28 |     n_layers: 12\n17:01:28 |     n_positions: 1024\n17:01:28 |     n_segments: 2\n17:01:28 |     nesterov: True\n17:01:28 |     no_cuda: False\n17:01:28 |     normalize_sent_emb: False\n17:01:28 |     num_epochs: -1\n17:01:28 |     num_examples: -1\n17:01:28 |     num_workers: 0\n17:01:28 |     nus: [0.7]\n17:01:28 |     optimizer: adamax\n17:01:28 |     output_scaling: 0.06\n17:01:28 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n17:01:28 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:01:28 |     person_tokens: False\n17:01:28 |     print_scores: False\n17:01:28 |     rank_candidates: False\n17:01:28 |     rank_top_k: -1\n17:01:28 |     reduction_type: mean\n17:01:28 |     ref_class: None\n17:01:28 |     relu_dropout: 0.0\n17:01:28 |     repeat_blocking_heuristic: True\n17:01:28 |     report_filename: \n17:01:28 |     return_cand_scores: False\n17:01:28 |     save_after_valid: True\n17:01:28 |     save_every_n_secs: -1\n17:01:28 |     save_format: conversations\n17:01:28 |     share_encoders: False\n17:01:28 |     share_word_embeddings: False\n17:01:28 |     short_final_eval: False\n17:01:28 |     special_tok_lst: None\n17:01:28 |     split_lines: False\n17:01:28 |     starttime: Dec03_17-00\n17:01:28 |     task: fromfile:parlaiformat\n17:01:28 |     tensorboard_log: False\n17:01:28 |     tensorboard_logdir: None\n17:01:28 |     text_truncate: 360\n17:01:28 |     threshold: 0.5\n17:01:28 |     topk: 5\n17:01:28 |     train_predict: False\n17:01:28 |     truncate: 1024\n17:01:28 |     update_classifier_head_only: False\n17:01:28 |     update_freq: 1\n17:01:28 |     use_memories: False\n17:01:28 |     use_reply: none\n17:01:28 |     validation_cutoff: 1.0\n17:01:28 |     validation_every_n_epochs: -1\n17:01:28 |     validation_every_n_secs: 20.0\n17:01:28 |     validation_every_n_steps: -1\n17:01:28 |     validation_max_exs: -1\n17:01:28 |     validation_metric: accuracy\n17:01:28 |     validation_metric_mode: max\n17:01:28 |     validation_patience: 30\n17:01:28 |     validation_share_agent: False\n17:01:28 |     variant: xlm\n17:01:28 |     verbose: False\n17:01:28 |     wandb_entity: None\n17:01:28 |     wandb_log: False\n17:01:28 |     wandb_name: None\n17:01:28 |     wandb_project: None\n17:01:28 |     warmup_rate: 0.0001\n17:01:28 |     warmup_updates: 1000\n17:01:28 |     weight_decay: None\n17:01:28 |     world_logs: \n17:01:28 |     wrap_memory_encoder: False\n17:01:29 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:01:29 | creating task(s): fromfile:parlaiformat\n17:01:29 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:01:29 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-a.txt\n17:01:34 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8000   8e-10               .8077                 .7850   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8317            .7917              .8172   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7677 11.79 551.8 518.6       0          0 37.59  200 .8000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .6394 2.905e-06 240.4 225.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 792.2 744.5        .7998\u001b[0m\n17:01:34 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8000   8e-10               .8077                 .7850   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8317            .7917              .8172   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7677 11.79 551.8 518.6       0          0 37.59  200 .8000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .6394 2.905e-06 240.4 225.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 792.2 744.5        .7998\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:01:36.550372Z","iopub.execute_input":"2022-12-03T17:01:36.550773Z","iopub.status.idle":"2022-12-03T17:02:03.008856Z","shell.execute_reply.started":"2022-12-03T17:01:36.550735Z","shell.execute_reply":"2022-12-03T17:02:03.007696Z"},"scrolled":true,"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"17:01:44 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_valid.txt)\u001b[0m\n17:01:44 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:01:44 | Using CUDA\n17:01:44 | loading dictionary from /tmp/model5.dict\n17:01:44 | num words = 54944\n17:01:48 | Loading existing model parameters from /tmp/model5\n17:01:54 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:01:55 | Opt:\n17:01:55 |     activation: gelu\n17:01:55 |     adafactor_eps: '[1e-30, 0.001]'\n17:01:55 |     adam_eps: 1e-08\n17:01:55 |     add_p1_after_newln: False\n17:01:55 |     aggregate_micro: False\n17:01:55 |     allow_missing_init_opts: False\n17:01:55 |     area_under_curve_class: None\n17:01:55 |     area_under_curve_digits: -1\n17:01:55 |     attention_dropout: 0.1\n17:01:55 |     batchsize: 40\n17:01:55 |     betas: '[0.9, 0.999]'\n17:01:55 |     bpe_add_prefix_space: None\n17:01:55 |     bpe_debug: False\n17:01:55 |     bpe_dropout: None\n17:01:55 |     bpe_merge: None\n17:01:55 |     bpe_vocab: None\n17:01:55 |     candidates: inline\n17:01:55 |     cap_num_predictions: 100\n17:01:55 |     checkpoint_activations: False\n17:01:55 |     class_weights: None\n17:01:55 |     classes: \"['__notok__', '__ok__']\"\n17:01:55 |     classes_from_file: None\n17:01:55 |     data_parallel: True\n17:01:55 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:01:55 |     datatype: train\n17:01:55 |     delimiter: '\\n'\n17:01:55 |     dict_class: parlai.core.dict:DictionaryAgent\n17:01:55 |     dict_endtoken: __start__\n17:01:55 |     dict_file: /tmp/model5.dict\n17:01:55 |     dict_include_test: False\n17:01:55 |     dict_include_valid: False\n17:01:55 |     dict_initpath: None\n17:01:55 |     dict_language: english\n17:01:55 |     dict_loaded: True\n17:01:55 |     dict_lower: True\n17:01:55 |     dict_max_ngram_size: -1\n17:01:55 |     dict_maxexs: -1\n17:01:55 |     dict_maxtokens: -1\n17:01:55 |     dict_minfreq: 0\n17:01:55 |     dict_nulltoken: __null__\n17:01:55 |     dict_starttoken: __start__\n17:01:55 |     dict_textfields: text,labels\n17:01:55 |     dict_tokenizer: bpe\n17:01:55 |     dict_unktoken: __unk__\n17:01:55 |     display_examples: False\n17:01:55 |     download_path: None\n17:01:55 |     dropout: 0.1\n17:01:55 |     dynamic_batching: None\n17:01:55 |     embedding_projection: random\n17:01:55 |     embedding_size: 768\n17:01:55 |     embedding_type: random\n17:01:55 |     embeddings_scale: False\n17:01:55 |     encode_candidate_vecs: True\n17:01:55 |     encode_candidate_vecs_batchsize: 256\n17:01:55 |     eval_batchsize: None\n17:01:55 |     eval_candidates: inline\n17:01:55 |     eval_dynamic_batching: None\n17:01:55 |     evaltask: None\n17:01:55 |     ffn_size: 3072\n17:01:55 |     final_extra_opt: \n17:01:55 |     fixed_candidate_vecs: reuse\n17:01:55 |     fixed_candidates_path: None\n17:01:55 |     force_fp16_tokens: True\n17:01:55 |     fp16: True\n17:01:55 |     fp16_impl: safe\n17:01:55 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-b.txt\n17:01:55 |     fromfile_datatype_extension: False\n17:01:55 |     gpu: -1\n17:01:55 |     gradient_clip: 0.1\n17:01:55 |     hide_labels: False\n17:01:55 |     history_add_global_end_token: None\n17:01:55 |     history_reversed: False\n17:01:55 |     history_size: 20\n17:01:55 |     ignore_bad_candidates: False\n17:01:55 |     ignore_labels: None\n17:01:55 |     image_cropsize: 224\n17:01:55 |     image_mode: raw\n17:01:55 |     image_size: 256\n17:01:55 |     inference: max\n17:01:55 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:01:55 |     init_opt: None\n17:01:55 |     interactive_candidates: fixed\n17:01:55 |     interactive_mode: False\n17:01:55 |     invsqrt_lr_decay_gamma: -1\n17:01:55 |     is_debug: False\n17:01:55 |     label_truncate: 72\n17:01:55 |     learn_embeddings: True\n17:01:55 |     learn_positional_embeddings: True\n17:01:55 |     learningrate: 5e-05\n17:01:55 |     load_from_pretrained_ranker: True\n17:01:55 |     log_every_n_secs: 10.0\n17:01:55 |     log_every_n_steps: 50\n17:01:55 |     log_keep_fields: all\n17:01:55 |     loglevel: info\n17:01:55 |     lr_scheduler: reduceonplateau\n17:01:55 |     lr_scheduler_decay: 0.5\n17:01:55 |     lr_scheduler_patience: 3\n17:01:55 |     max_train_steps: -1\n17:01:55 |     max_train_time: 7200.0\n17:01:55 |     memory_attention: sqrt\n17:01:55 |     metrics: default\n17:01:55 |     model: transformer/classifier\n17:01:55 |     model_file: /tmp/model5\n17:01:55 |     model_parallel: False\n17:01:55 |     momentum: 0\n17:01:55 |     multitask_weights: [1]\n17:01:55 |     mutators: None\n17:01:55 |     n_decoder_layers: -1\n17:01:55 |     n_encoder_layers: -1\n17:01:55 |     n_heads: 12\n17:01:55 |     n_layers: 12\n17:01:55 |     n_positions: 1024\n17:01:55 |     n_segments: 2\n17:01:55 |     nesterov: True\n17:01:55 |     no_cuda: False\n17:01:55 |     normalize_sent_emb: False\n17:01:55 |     num_epochs: -1\n17:01:55 |     num_examples: -1\n17:01:55 |     num_workers: 0\n17:01:55 |     nus: [0.7]\n17:01:55 |     optimizer: adamax\n17:01:55 |     output_scaling: 0.06\n17:01:55 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n17:01:55 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:01:55 |     person_tokens: False\n17:01:55 |     print_scores: False\n17:01:55 |     rank_candidates: False\n17:01:55 |     rank_top_k: -1\n17:01:55 |     reduction_type: mean\n17:01:55 |     ref_class: None\n17:01:55 |     relu_dropout: 0.0\n17:01:55 |     repeat_blocking_heuristic: True\n17:01:55 |     report_filename: \n17:01:55 |     return_cand_scores: False\n17:01:55 |     save_after_valid: True\n17:01:55 |     save_every_n_secs: -1\n17:01:55 |     save_format: conversations\n17:01:55 |     share_encoders: False\n17:01:55 |     share_word_embeddings: False\n17:01:55 |     short_final_eval: False\n17:01:55 |     special_tok_lst: None\n17:01:55 |     split_lines: False\n17:01:55 |     starttime: Dec03_17-00\n17:01:55 |     task: fromfile:parlaiformat\n17:01:55 |     tensorboard_log: False\n17:01:55 |     tensorboard_logdir: None\n17:01:55 |     text_truncate: 360\n17:01:55 |     threshold: 0.5\n17:01:55 |     topk: 5\n17:01:55 |     train_predict: False\n17:01:55 |     truncate: 1024\n17:01:55 |     update_classifier_head_only: False\n17:01:55 |     update_freq: 1\n17:01:55 |     use_memories: False\n17:01:55 |     use_reply: none\n17:01:55 |     validation_cutoff: 1.0\n17:01:55 |     validation_every_n_epochs: -1\n17:01:55 |     validation_every_n_secs: 20.0\n17:01:55 |     validation_every_n_steps: -1\n17:01:55 |     validation_max_exs: -1\n17:01:55 |     validation_metric: accuracy\n17:01:55 |     validation_metric_mode: max\n17:01:55 |     validation_patience: 30\n17:01:55 |     validation_share_agent: False\n17:01:55 |     variant: xlm\n17:01:55 |     verbose: False\n17:01:55 |     wandb_entity: None\n17:01:55 |     wandb_log: False\n17:01:55 |     wandb_name: None\n17:01:55 |     wandb_project: None\n17:01:55 |     warmup_rate: 0.0001\n17:01:55 |     warmup_updates: 1000\n17:01:55 |     weight_decay: None\n17:01:55 |     world_logs: \n17:01:55 |     wrap_memory_encoder: False\n17:01:55 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:01:55 | creating task(s): fromfile:parlaiformat\n17:01:55 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:01:55 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type1/run5/data_train-b.txt\n17:02:01 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2000   2e-10               .2233                 .2150   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2323            .1753              .1828   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1683 11.79 551.8 516.5       0          0 37.44  200 .2000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .7556 2.905e-06 239.6 224.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 791.4 740.8        .1990\u001b[0m\n17:02:01 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2000   2e-10               .2233                 .2150   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2323            .1753              .1828   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1683 11.79 551.8 516.5       0          0 37.44  200 .2000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .7556 2.905e-06 239.6 224.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 791.4 740.8        .1990\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:02:03.010720Z","iopub.execute_input":"2022-12-03T17:02:03.011118Z","iopub.status.idle":"2022-12-03T17:02:04.103434Z","shell.execute_reply.started":"2022-12-03T17:02:03.011077Z","shell.execute_reply":"2022-12-03T17:02:04.102088Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"Choose completion prev1corr2type2","metadata":{}},{"cell_type":"markdown","source":"run 1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:02:04.105583Z","iopub.execute_input":"2022-12-03T17:02:04.105968Z","iopub.status.idle":"2022-12-03T17:02:56.746678Z","shell.execute_reply.started":"2022-12-03T17:02:04.105931Z","shell.execute_reply":"2022-12-03T17:02:56.745401Z"},"scrolled":true,"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"17:02:11 | building dictionary first...\n17:02:11 | No model with opt yet at: /tmp/model1(.opt)\n17:02:11 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:02:11 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:02:11 | Using CUDA\n17:02:11 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:02:11 | num words = 54944\n17:02:16 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:02:21 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:02:21 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:02:21 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:02:21 | Opt:\n17:02:21 |     activation: gelu\n17:02:21 |     adafactor_eps: '(1e-30, 0.001)'\n17:02:21 |     adam_eps: 1e-08\n17:02:21 |     add_p1_after_newln: False\n17:02:21 |     aggregate_micro: False\n17:02:21 |     allow_missing_init_opts: False\n17:02:21 |     attention_dropout: 0.1\n17:02:21 |     batchsize: 20\n17:02:21 |     betas: '(0.9, 0.999)'\n17:02:21 |     bpe_add_prefix_space: None\n17:02:21 |     bpe_debug: False\n17:02:21 |     bpe_dropout: None\n17:02:21 |     bpe_merge: None\n17:02:21 |     bpe_vocab: None\n17:02:21 |     candidates: inline\n17:02:21 |     cap_num_predictions: 100\n17:02:21 |     checkpoint_activations: False\n17:02:21 |     class_weights: None\n17:02:21 |     classes: \"['__notok__', '__ok__']\"\n17:02:21 |     classes_from_file: None\n17:02:21 |     data_parallel: True\n17:02:21 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:02:21 |     datatype: train\n17:02:21 |     delimiter: '\\n'\n17:02:21 |     dict_class: parlai.core.dict:DictionaryAgent\n17:02:21 |     dict_endtoken: __start__\n17:02:21 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:02:21 |     dict_include_test: False\n17:02:21 |     dict_include_valid: False\n17:02:21 |     dict_initpath: None\n17:02:21 |     dict_language: english\n17:02:21 |     dict_loaded: True\n17:02:21 |     dict_lower: True\n17:02:21 |     dict_max_ngram_size: -1\n17:02:21 |     dict_maxexs: -1\n17:02:21 |     dict_maxtokens: -1\n17:02:21 |     dict_minfreq: 0\n17:02:21 |     dict_nulltoken: __null__\n17:02:21 |     dict_starttoken: __start__\n17:02:21 |     dict_textfields: text,labels\n17:02:21 |     dict_tokenizer: bpe\n17:02:21 |     dict_unktoken: __unk__\n17:02:21 |     display_examples: False\n17:02:21 |     download_path: None\n17:02:21 |     dropout: 0.1\n17:02:21 |     dynamic_batching: None\n17:02:21 |     embedding_projection: random\n17:02:21 |     embedding_size: 768\n17:02:21 |     embedding_type: random\n17:02:21 |     embeddings_scale: False\n17:02:21 |     encode_candidate_vecs: True\n17:02:21 |     encode_candidate_vecs_batchsize: 256\n17:02:21 |     eval_batchsize: None\n17:02:21 |     eval_candidates: inline\n17:02:21 |     eval_dynamic_batching: None\n17:02:21 |     evaltask: None\n17:02:21 |     ffn_size: 3072\n17:02:21 |     final_extra_opt: \n17:02:21 |     fixed_candidate_vecs: reuse\n17:02:21 |     fixed_candidates_path: None\n17:02:21 |     force_fp16_tokens: False\n17:02:21 |     fp16: True\n17:02:21 |     fp16_impl: safe\n17:02:21 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt\n17:02:21 |     fromfile_datatype_extension: False\n17:02:21 |     gpu: -1\n17:02:21 |     gradient_clip: 0.1\n17:02:21 |     hide_labels: False\n17:02:21 |     history_add_global_end_token: None\n17:02:21 |     history_reversed: False\n17:02:21 |     history_size: 20\n17:02:21 |     ignore_bad_candidates: False\n17:02:21 |     ignore_labels: None\n17:02:21 |     image_cropsize: 224\n17:02:21 |     image_mode: raw\n17:02:21 |     image_size: 256\n17:02:21 |     inference: max\n17:02:21 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:02:21 |     init_opt: None\n17:02:21 |     interactive_candidates: fixed\n17:02:21 |     interactive_mode: False\n17:02:21 |     invsqrt_lr_decay_gamma: -1\n17:02:21 |     is_debug: False\n17:02:21 |     label_truncate: 72\n17:02:21 |     learn_embeddings: True\n17:02:21 |     learn_positional_embeddings: True\n17:02:21 |     learningrate: 5e-05\n17:02:21 |     load_from_checkpoint: False\n17:02:21 |     load_from_pretrained_ranker: True\n17:02:21 |     log_every_n_secs: 10.0\n17:02:21 |     log_every_n_steps: 50\n17:02:21 |     log_keep_fields: all\n17:02:21 |     loglevel: info\n17:02:21 |     lr_scheduler: reduceonplateau\n17:02:21 |     lr_scheduler_decay: 0.5\n17:02:21 |     lr_scheduler_patience: 3\n17:02:21 |     max_train_steps: -1\n17:02:21 |     max_train_time: 7200.0\n17:02:21 |     memory_attention: sqrt\n17:02:21 |     metrics: default\n17:02:21 |     model: transformer/classifier\n17:02:21 |     model_file: /tmp/model1\n17:02:21 |     model_parallel: False\n17:02:21 |     momentum: 0\n17:02:21 |     multitask_weights: [1]\n17:02:21 |     mutators: None\n17:02:21 |     n_decoder_layers: -1\n17:02:21 |     n_encoder_layers: -1\n17:02:21 |     n_heads: 12\n17:02:21 |     n_layers: 12\n17:02:21 |     n_positions: 1024\n17:02:21 |     n_segments: 2\n17:02:21 |     nesterov: True\n17:02:21 |     no_cuda: False\n17:02:21 |     normalize_sent_emb: False\n17:02:21 |     num_epochs: -1\n17:02:21 |     num_workers: 0\n17:02:21 |     nus: (0.7,)\n17:02:21 |     optimizer: adamax\n17:02:21 |     output_scaling: 0.06\n17:02:21 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n17:02:21 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:02:21 |     person_tokens: False\n17:02:21 |     print_scores: False\n17:02:21 |     rank_candidates: False\n17:02:21 |     rank_top_k: -1\n17:02:21 |     reduction_type: mean\n17:02:21 |     ref_class: None\n17:02:21 |     relu_dropout: 0.0\n17:02:21 |     repeat_blocking_heuristic: True\n17:02:21 |     return_cand_scores: False\n17:02:21 |     save_after_valid: True\n17:02:21 |     save_every_n_secs: -1\n17:02:21 |     save_format: conversations\n17:02:21 |     share_encoders: False\n17:02:21 |     share_word_embeddings: False\n17:02:21 |     short_final_eval: False\n17:02:21 |     special_tok_lst: None\n17:02:21 |     split_lines: False\n17:02:21 |     starttime: Dec03_17-02\n17:02:21 |     task: fromfile:parlaiformat\n17:02:21 |     tensorboard_log: False\n17:02:21 |     tensorboard_logdir: None\n17:02:21 |     text_truncate: 360\n17:02:21 |     threshold: 0.5\n17:02:21 |     topk: 5\n17:02:21 |     train_predict: False\n17:02:21 |     truncate: 1024\n17:02:21 |     update_classifier_head_only: False\n17:02:21 |     update_freq: 1\n17:02:21 |     use_memories: False\n17:02:21 |     use_reply: none\n17:02:21 |     validation_cutoff: 1.0\n17:02:21 |     validation_every_n_epochs: -1\n17:02:21 |     validation_every_n_secs: 20.0\n17:02:21 |     validation_every_n_steps: -1\n17:02:21 |     validation_max_exs: -1\n17:02:21 |     validation_metric: accuracy\n17:02:21 |     validation_metric_mode: max\n17:02:21 |     validation_patience: 30\n17:02:21 |     validation_share_agent: False\n17:02:21 |     variant: xlm\n17:02:21 |     verbose: False\n17:02:21 |     wandb_entity: None\n17:02:21 |     wandb_log: False\n17:02:21 |     wandb_name: None\n17:02:21 |     wandb_project: None\n17:02:21 |     warmup_rate: 0.0001\n17:02:21 |     warmup_updates: 1000\n17:02:21 |     weight_decay: None\n17:02:21 |     world_logs: \n17:02:21 |     wrap_memory_encoder: False\n17:02:22 | creating task(s): fromfile:parlaiformat\n17:02:22 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt\n17:02:22 | training...\n17:02:32 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6024 6.024e-10               .7044                 .5574   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9567            .3927              .8571   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .2547 11.25     1   265 551.9       0          0 41.64  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6024             32768  2.888    .1189  5.99 .6740 1.055e-06 119.8 249.5   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 384.9 801.4 2.087        .5471\n\n17:02:42 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8197 8.197e-10               .8390                 .7376   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9728            .7952              .9638   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6768 11.72     1 274.5  1053       0          0 76.73  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8197             32768  2.802    .1189 5.966 .6301 2.955e-06 119.3 457.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   59 393.8 1511 3.845        .8164\n\n17:02:42 | creating task(s): fromfile:parlaiformat\n17:02:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:02:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt\n17:02:42 | running eval: valid\n17:02:42 | eval completed in 0.22s\n17:02:42 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1633       0          0 121.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5614 2.955e-06    72 728.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 233.5 2362            1\n\u001b[0m\n17:02:42 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:02:42 | saving best valid model: /tmp/model1\n17:02:42 | Saving dictionary to /tmp/model1.dict\n17:02:46 | task solved! stopping.\n17:02:46 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:02:46 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:02:46 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:02:46 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:02:46 | Using CUDA\n17:02:46 | loading dictionary from /tmp/model1.dict\n17:02:46 | num words = 54944\n17:02:51 | Loading existing model parameters from /tmp/model1\n17:02:53 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:02:54 | creating task(s): fromfile:parlaiformat\n17:02:54 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:02:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt\n17:02:54 | running eval: valid\n17:02:54 | eval completed in 0.21s\n17:02:54 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1709       0          0   127   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5614 2.955e-06    72   762       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 233.5 2472            1\n\u001b[0m\n17:02:54 | creating task(s): fromfile:parlaiformat\n17:02:54 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:02:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt\n17:02:54 | running eval: test\n17:02:55 | eval completed in 0.20s\n17:02:55 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1770       0          0 131.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5614 2.955e-06    72   789       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 233.5 2559            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:02:56.748323Z","iopub.execute_input":"2022-12-03T17:02:56.748821Z","iopub.status.idle":"2022-12-03T17:03:26.030752Z","shell.execute_reply.started":"2022-12-03T17:02:56.748778Z","shell.execute_reply":"2022-12-03T17:03:26.029455Z"},"scrolled":true,"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"17:03:04 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt)\u001b[0m\n17:03:04 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:03:04 | Using CUDA\n17:03:04 | loading dictionary from /tmp/model1.dict\n17:03:04 | num words = 54944\n17:03:09 | Loading existing model parameters from /tmp/model1\n17:03:17 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:03:18 | Opt:\n17:03:18 |     activation: gelu\n17:03:18 |     adafactor_eps: '[1e-30, 0.001]'\n17:03:18 |     adam_eps: 1e-08\n17:03:18 |     add_p1_after_newln: False\n17:03:18 |     aggregate_micro: False\n17:03:18 |     allow_missing_init_opts: False\n17:03:18 |     area_under_curve_class: None\n17:03:18 |     area_under_curve_digits: -1\n17:03:18 |     attention_dropout: 0.1\n17:03:18 |     batchsize: 40\n17:03:18 |     betas: '[0.9, 0.999]'\n17:03:18 |     bpe_add_prefix_space: None\n17:03:18 |     bpe_debug: False\n17:03:18 |     bpe_dropout: None\n17:03:18 |     bpe_merge: None\n17:03:18 |     bpe_vocab: None\n17:03:18 |     candidates: inline\n17:03:18 |     cap_num_predictions: 100\n17:03:18 |     checkpoint_activations: False\n17:03:18 |     class_weights: None\n17:03:18 |     classes: \"['__notok__', '__ok__']\"\n17:03:18 |     classes_from_file: None\n17:03:18 |     data_parallel: True\n17:03:18 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:03:18 |     datatype: train\n17:03:18 |     delimiter: '\\n'\n17:03:18 |     dict_class: parlai.core.dict:DictionaryAgent\n17:03:18 |     dict_endtoken: __start__\n17:03:18 |     dict_file: /tmp/model1.dict\n17:03:18 |     dict_include_test: False\n17:03:18 |     dict_include_valid: False\n17:03:18 |     dict_initpath: None\n17:03:18 |     dict_language: english\n17:03:18 |     dict_loaded: True\n17:03:18 |     dict_lower: True\n17:03:18 |     dict_max_ngram_size: -1\n17:03:18 |     dict_maxexs: -1\n17:03:18 |     dict_maxtokens: -1\n17:03:18 |     dict_minfreq: 0\n17:03:18 |     dict_nulltoken: __null__\n17:03:18 |     dict_starttoken: __start__\n17:03:18 |     dict_textfields: text,labels\n17:03:18 |     dict_tokenizer: bpe\n17:03:18 |     dict_unktoken: __unk__\n17:03:18 |     display_examples: False\n17:03:18 |     download_path: None\n17:03:18 |     dropout: 0.1\n17:03:18 |     dynamic_batching: None\n17:03:18 |     embedding_projection: random\n17:03:18 |     embedding_size: 768\n17:03:18 |     embedding_type: random\n17:03:18 |     embeddings_scale: False\n17:03:18 |     encode_candidate_vecs: True\n17:03:18 |     encode_candidate_vecs_batchsize: 256\n17:03:18 |     eval_batchsize: None\n17:03:18 |     eval_candidates: inline\n17:03:18 |     eval_dynamic_batching: None\n17:03:18 |     evaltask: None\n17:03:18 |     ffn_size: 3072\n17:03:18 |     final_extra_opt: \n17:03:18 |     fixed_candidate_vecs: reuse\n17:03:18 |     fixed_candidates_path: None\n17:03:18 |     force_fp16_tokens: True\n17:03:18 |     fp16: True\n17:03:18 |     fp16_impl: safe\n17:03:18 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-a.txt\n17:03:18 |     fromfile_datatype_extension: False\n17:03:18 |     gpu: -1\n17:03:18 |     gradient_clip: 0.1\n17:03:18 |     hide_labels: False\n17:03:18 |     history_add_global_end_token: None\n17:03:18 |     history_reversed: False\n17:03:18 |     history_size: 20\n17:03:18 |     ignore_bad_candidates: False\n17:03:18 |     ignore_labels: None\n17:03:18 |     image_cropsize: 224\n17:03:18 |     image_mode: raw\n17:03:18 |     image_size: 256\n17:03:18 |     inference: max\n17:03:18 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:03:18 |     init_opt: None\n17:03:18 |     interactive_candidates: fixed\n17:03:18 |     interactive_mode: False\n17:03:18 |     invsqrt_lr_decay_gamma: -1\n17:03:18 |     is_debug: False\n17:03:18 |     label_truncate: 72\n17:03:18 |     learn_embeddings: True\n17:03:18 |     learn_positional_embeddings: True\n17:03:18 |     learningrate: 5e-05\n17:03:18 |     load_from_pretrained_ranker: True\n17:03:18 |     log_every_n_secs: 10.0\n17:03:18 |     log_every_n_steps: 50\n17:03:18 |     log_keep_fields: all\n17:03:18 |     loglevel: info\n17:03:18 |     lr_scheduler: reduceonplateau\n17:03:18 |     lr_scheduler_decay: 0.5\n17:03:18 |     lr_scheduler_patience: 3\n17:03:18 |     max_train_steps: -1\n17:03:18 |     max_train_time: 7200.0\n17:03:18 |     memory_attention: sqrt\n17:03:18 |     metrics: default\n17:03:18 |     model: transformer/classifier\n17:03:18 |     model_file: /tmp/model1\n17:03:18 |     model_parallel: False\n17:03:18 |     momentum: 0\n17:03:18 |     multitask_weights: [1]\n17:03:18 |     mutators: None\n17:03:18 |     n_decoder_layers: -1\n17:03:18 |     n_encoder_layers: -1\n17:03:18 |     n_heads: 12\n17:03:18 |     n_layers: 12\n17:03:18 |     n_positions: 1024\n17:03:18 |     n_segments: 2\n17:03:18 |     nesterov: True\n17:03:18 |     no_cuda: False\n17:03:18 |     normalize_sent_emb: False\n17:03:18 |     num_epochs: -1\n17:03:18 |     num_examples: -1\n17:03:18 |     num_workers: 0\n17:03:18 |     nus: [0.7]\n17:03:18 |     optimizer: adamax\n17:03:18 |     output_scaling: 0.06\n17:03:18 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n17:03:18 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:03:18 |     person_tokens: False\n17:03:18 |     print_scores: False\n17:03:18 |     rank_candidates: False\n17:03:18 |     rank_top_k: -1\n17:03:18 |     reduction_type: mean\n17:03:18 |     ref_class: None\n17:03:18 |     relu_dropout: 0.0\n17:03:18 |     repeat_blocking_heuristic: True\n17:03:18 |     report_filename: \n17:03:18 |     return_cand_scores: False\n17:03:18 |     save_after_valid: True\n17:03:18 |     save_every_n_secs: -1\n17:03:18 |     save_format: conversations\n17:03:18 |     share_encoders: False\n17:03:18 |     share_word_embeddings: False\n17:03:18 |     short_final_eval: False\n17:03:18 |     special_tok_lst: None\n17:03:18 |     split_lines: False\n17:03:18 |     starttime: Dec03_17-02\n17:03:18 |     task: fromfile:parlaiformat\n17:03:18 |     tensorboard_log: False\n17:03:18 |     tensorboard_logdir: None\n17:03:18 |     text_truncate: 360\n17:03:18 |     threshold: 0.5\n17:03:18 |     topk: 5\n17:03:18 |     train_predict: False\n17:03:18 |     truncate: 1024\n17:03:18 |     update_classifier_head_only: False\n17:03:18 |     update_freq: 1\n17:03:18 |     use_memories: False\n17:03:18 |     use_reply: none\n17:03:18 |     validation_cutoff: 1.0\n17:03:18 |     validation_every_n_epochs: -1\n17:03:18 |     validation_every_n_secs: 20.0\n17:03:18 |     validation_every_n_steps: -1\n17:03:18 |     validation_max_exs: -1\n17:03:18 |     validation_metric: accuracy\n17:03:18 |     validation_metric_mode: max\n17:03:18 |     validation_patience: 30\n17:03:18 |     validation_share_agent: False\n17:03:18 |     variant: xlm\n17:03:18 |     verbose: False\n17:03:18 |     wandb_entity: None\n17:03:18 |     wandb_log: False\n17:03:18 |     wandb_name: None\n17:03:18 |     wandb_project: None\n17:03:18 |     warmup_rate: 0.0001\n17:03:18 |     warmup_updates: 1000\n17:03:18 |     weight_decay: None\n17:03:18 |     world_logs: \n17:03:18 |     wrap_memory_encoder: False\n17:03:18 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:03:18 | creating task(s): fromfile:parlaiformat\n17:03:18 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:03:18 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-a.txt\n17:03:24 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3050 3.05e-10               .3767                 .3443   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4158            .2147              .2436   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1919 11.13 525.2 502.4       0          0 38.26  200 .3050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.01 .7392 2.955e-06 240.4   230       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 765.6 732.3        .2965\u001b[0m\n17:03:24 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3050 3.05e-10               .3767                 .3443   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4158            .2147              .2436   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1919 11.13 525.2 502.4       0          0 38.26  200 .3050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.01 .7392 2.955e-06 240.4   230       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 765.6 732.3        .2965\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:03:26.032853Z","iopub.execute_input":"2022-12-03T17:03:26.033253Z","iopub.status.idle":"2022-12-03T17:03:53.418021Z","shell.execute_reply.started":"2022-12-03T17:03:26.033214Z","shell.execute_reply":"2022-12-03T17:03:53.416720Z"},"scrolled":true,"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"17:03:33 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_valid.txt)\u001b[0m\n17:03:33 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:03:33 | Using CUDA\n17:03:33 | loading dictionary from /tmp/model1.dict\n17:03:33 | num words = 54944\n17:03:37 | Loading existing model parameters from /tmp/model1\n17:03:44 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:03:45 | Opt:\n17:03:45 |     activation: gelu\n17:03:45 |     adafactor_eps: '[1e-30, 0.001]'\n17:03:45 |     adam_eps: 1e-08\n17:03:45 |     add_p1_after_newln: False\n17:03:45 |     aggregate_micro: False\n17:03:45 |     allow_missing_init_opts: False\n17:03:45 |     area_under_curve_class: None\n17:03:45 |     area_under_curve_digits: -1\n17:03:45 |     attention_dropout: 0.1\n17:03:45 |     batchsize: 40\n17:03:45 |     betas: '[0.9, 0.999]'\n17:03:45 |     bpe_add_prefix_space: None\n17:03:45 |     bpe_debug: False\n17:03:45 |     bpe_dropout: None\n17:03:45 |     bpe_merge: None\n17:03:45 |     bpe_vocab: None\n17:03:45 |     candidates: inline\n17:03:45 |     cap_num_predictions: 100\n17:03:45 |     checkpoint_activations: False\n17:03:45 |     class_weights: None\n17:03:45 |     classes: \"['__notok__', '__ok__']\"\n17:03:45 |     classes_from_file: None\n17:03:45 |     data_parallel: True\n17:03:45 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:03:45 |     datatype: train\n17:03:45 |     delimiter: '\\n'\n17:03:45 |     dict_class: parlai.core.dict:DictionaryAgent\n17:03:45 |     dict_endtoken: __start__\n17:03:45 |     dict_file: /tmp/model1.dict\n17:03:45 |     dict_include_test: False\n17:03:45 |     dict_include_valid: False\n17:03:45 |     dict_initpath: None\n17:03:45 |     dict_language: english\n17:03:45 |     dict_loaded: True\n17:03:45 |     dict_lower: True\n17:03:45 |     dict_max_ngram_size: -1\n17:03:45 |     dict_maxexs: -1\n17:03:45 |     dict_maxtokens: -1\n17:03:45 |     dict_minfreq: 0\n17:03:45 |     dict_nulltoken: __null__\n17:03:45 |     dict_starttoken: __start__\n17:03:45 |     dict_textfields: text,labels\n17:03:45 |     dict_tokenizer: bpe\n17:03:45 |     dict_unktoken: __unk__\n17:03:45 |     display_examples: False\n17:03:45 |     download_path: None\n17:03:45 |     dropout: 0.1\n17:03:45 |     dynamic_batching: None\n17:03:45 |     embedding_projection: random\n17:03:45 |     embedding_size: 768\n17:03:45 |     embedding_type: random\n17:03:45 |     embeddings_scale: False\n17:03:45 |     encode_candidate_vecs: True\n17:03:45 |     encode_candidate_vecs_batchsize: 256\n17:03:45 |     eval_batchsize: None\n17:03:45 |     eval_candidates: inline\n17:03:45 |     eval_dynamic_batching: None\n17:03:45 |     evaltask: None\n17:03:45 |     ffn_size: 3072\n17:03:45 |     final_extra_opt: \n17:03:45 |     fixed_candidate_vecs: reuse\n17:03:45 |     fixed_candidates_path: None\n17:03:45 |     force_fp16_tokens: True\n17:03:45 |     fp16: True\n17:03:45 |     fp16_impl: safe\n17:03:45 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-b.txt\n17:03:45 |     fromfile_datatype_extension: False\n17:03:45 |     gpu: -1\n17:03:45 |     gradient_clip: 0.1\n17:03:45 |     hide_labels: False\n17:03:45 |     history_add_global_end_token: None\n17:03:45 |     history_reversed: False\n17:03:45 |     history_size: 20\n17:03:45 |     ignore_bad_candidates: False\n17:03:45 |     ignore_labels: None\n17:03:45 |     image_cropsize: 224\n17:03:45 |     image_mode: raw\n17:03:45 |     image_size: 256\n17:03:45 |     inference: max\n17:03:45 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:03:45 |     init_opt: None\n17:03:45 |     interactive_candidates: fixed\n17:03:45 |     interactive_mode: False\n17:03:45 |     invsqrt_lr_decay_gamma: -1\n17:03:45 |     is_debug: False\n17:03:45 |     label_truncate: 72\n17:03:45 |     learn_embeddings: True\n17:03:45 |     learn_positional_embeddings: True\n17:03:45 |     learningrate: 5e-05\n17:03:45 |     load_from_pretrained_ranker: True\n17:03:45 |     log_every_n_secs: 10.0\n17:03:45 |     log_every_n_steps: 50\n17:03:45 |     log_keep_fields: all\n17:03:45 |     loglevel: info\n17:03:45 |     lr_scheduler: reduceonplateau\n17:03:45 |     lr_scheduler_decay: 0.5\n17:03:45 |     lr_scheduler_patience: 3\n17:03:45 |     max_train_steps: -1\n17:03:45 |     max_train_time: 7200.0\n17:03:45 |     memory_attention: sqrt\n17:03:45 |     metrics: default\n17:03:45 |     model: transformer/classifier\n17:03:45 |     model_file: /tmp/model1\n17:03:45 |     model_parallel: False\n17:03:45 |     momentum: 0\n17:03:45 |     multitask_weights: [1]\n17:03:45 |     mutators: None\n17:03:45 |     n_decoder_layers: -1\n17:03:45 |     n_encoder_layers: -1\n17:03:45 |     n_heads: 12\n17:03:45 |     n_layers: 12\n17:03:45 |     n_positions: 1024\n17:03:45 |     n_segments: 2\n17:03:45 |     nesterov: True\n17:03:45 |     no_cuda: False\n17:03:45 |     normalize_sent_emb: False\n17:03:45 |     num_epochs: -1\n17:03:45 |     num_examples: -1\n17:03:45 |     num_workers: 0\n17:03:45 |     nus: [0.7]\n17:03:45 |     optimizer: adamax\n17:03:45 |     output_scaling: 0.06\n17:03:45 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n17:03:45 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:03:45 |     person_tokens: False\n17:03:45 |     print_scores: False\n17:03:45 |     rank_candidates: False\n17:03:45 |     rank_top_k: -1\n17:03:45 |     reduction_type: mean\n17:03:45 |     ref_class: None\n17:03:45 |     relu_dropout: 0.0\n17:03:45 |     repeat_blocking_heuristic: True\n17:03:45 |     report_filename: \n17:03:45 |     return_cand_scores: False\n17:03:45 |     save_after_valid: True\n17:03:45 |     save_every_n_secs: -1\n17:03:45 |     save_format: conversations\n17:03:45 |     share_encoders: False\n17:03:45 |     share_word_embeddings: False\n17:03:45 |     short_final_eval: False\n17:03:45 |     special_tok_lst: None\n17:03:45 |     split_lines: False\n17:03:45 |     starttime: Dec03_17-02\n17:03:45 |     task: fromfile:parlaiformat\n17:03:45 |     tensorboard_log: False\n17:03:45 |     tensorboard_logdir: None\n17:03:45 |     text_truncate: 360\n17:03:45 |     threshold: 0.5\n17:03:45 |     topk: 5\n17:03:45 |     train_predict: False\n17:03:45 |     truncate: 1024\n17:03:45 |     update_classifier_head_only: False\n17:03:45 |     update_freq: 1\n17:03:45 |     use_memories: False\n17:03:45 |     use_reply: none\n17:03:45 |     validation_cutoff: 1.0\n17:03:45 |     validation_every_n_epochs: -1\n17:03:45 |     validation_every_n_secs: 20.0\n17:03:45 |     validation_every_n_steps: -1\n17:03:45 |     validation_max_exs: -1\n17:03:45 |     validation_metric: accuracy\n17:03:45 |     validation_metric_mode: max\n17:03:45 |     validation_patience: 30\n17:03:45 |     validation_share_agent: False\n17:03:45 |     variant: xlm\n17:03:45 |     verbose: False\n17:03:45 |     wandb_entity: None\n17:03:45 |     wandb_log: False\n17:03:45 |     wandb_name: None\n17:03:45 |     wandb_project: None\n17:03:45 |     warmup_rate: 0.0001\n17:03:45 |     warmup_updates: 1000\n17:03:45 |     weight_decay: None\n17:03:45 |     world_logs: \n17:03:45 |     wrap_memory_encoder: False\n17:03:45 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:03:45 | creating task(s): fromfile:parlaiformat\n17:03:45 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:03:45 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run1/data_train-b.txt\n17:03:51 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6950 6.95e-10               .7240                 .6557   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8081            .6592              .7564   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5842 11.13 525.2 473.3       0          0 36.05  200 .6950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.99 .6557 2.955e-06 239.6 215.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 764.8 689.3        .6913\u001b[0m\n17:03:51 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6950 6.95e-10               .7240                 .6557   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8081            .6592              .7564   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5842 11.13 525.2 473.3       0          0 36.05  200 .6950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.99 .6557 2.955e-06 239.6 215.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 764.8 689.3        .6913\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:03:53.420234Z","iopub.execute_input":"2022-12-03T17:03:53.420659Z","iopub.status.idle":"2022-12-03T17:03:54.521227Z","shell.execute_reply.started":"2022-12-03T17:03:53.420617Z","shell.execute_reply":"2022-12-03T17:03:54.519885Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:03:54.523573Z","iopub.execute_input":"2022-12-03T17:03:54.523963Z","iopub.status.idle":"2022-12-03T17:04:47.282347Z","shell.execute_reply.started":"2022-12-03T17:03:54.523923Z","shell.execute_reply":"2022-12-03T17:04:47.281191Z"},"scrolled":true,"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"17:04:02 | building dictionary first...\n17:04:02 | No model with opt yet at: /tmp/model2(.opt)\n17:04:02 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:04:02 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:04:02 | Using CUDA\n17:04:02 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:04:02 | num words = 54944\n17:04:06 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:04:12 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:04:12 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:04:12 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:04:12 | Opt:\n17:04:12 |     activation: gelu\n17:04:12 |     adafactor_eps: '(1e-30, 0.001)'\n17:04:12 |     adam_eps: 1e-08\n17:04:12 |     add_p1_after_newln: False\n17:04:12 |     aggregate_micro: False\n17:04:12 |     allow_missing_init_opts: False\n17:04:12 |     attention_dropout: 0.1\n17:04:12 |     batchsize: 20\n17:04:12 |     betas: '(0.9, 0.999)'\n17:04:12 |     bpe_add_prefix_space: None\n17:04:12 |     bpe_debug: False\n17:04:12 |     bpe_dropout: None\n17:04:12 |     bpe_merge: None\n17:04:12 |     bpe_vocab: None\n17:04:12 |     candidates: inline\n17:04:12 |     cap_num_predictions: 100\n17:04:12 |     checkpoint_activations: False\n17:04:12 |     class_weights: None\n17:04:12 |     classes: \"['__notok__', '__ok__']\"\n17:04:12 |     classes_from_file: None\n17:04:12 |     data_parallel: True\n17:04:12 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:04:12 |     datatype: train\n17:04:12 |     delimiter: '\\n'\n17:04:12 |     dict_class: parlai.core.dict:DictionaryAgent\n17:04:12 |     dict_endtoken: __start__\n17:04:12 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:04:12 |     dict_include_test: False\n17:04:12 |     dict_include_valid: False\n17:04:12 |     dict_initpath: None\n17:04:12 |     dict_language: english\n17:04:12 |     dict_loaded: True\n17:04:12 |     dict_lower: True\n17:04:12 |     dict_max_ngram_size: -1\n17:04:12 |     dict_maxexs: -1\n17:04:12 |     dict_maxtokens: -1\n17:04:12 |     dict_minfreq: 0\n17:04:12 |     dict_nulltoken: __null__\n17:04:12 |     dict_starttoken: __start__\n17:04:12 |     dict_textfields: text,labels\n17:04:12 |     dict_tokenizer: bpe\n17:04:12 |     dict_unktoken: __unk__\n17:04:12 |     display_examples: False\n17:04:12 |     download_path: None\n17:04:12 |     dropout: 0.1\n17:04:12 |     dynamic_batching: None\n17:04:12 |     embedding_projection: random\n17:04:12 |     embedding_size: 768\n17:04:12 |     embedding_type: random\n17:04:12 |     embeddings_scale: False\n17:04:12 |     encode_candidate_vecs: True\n17:04:12 |     encode_candidate_vecs_batchsize: 256\n17:04:12 |     eval_batchsize: None\n17:04:12 |     eval_candidates: inline\n17:04:12 |     eval_dynamic_batching: None\n17:04:12 |     evaltask: None\n17:04:12 |     ffn_size: 3072\n17:04:12 |     final_extra_opt: \n17:04:12 |     fixed_candidate_vecs: reuse\n17:04:12 |     fixed_candidates_path: None\n17:04:12 |     force_fp16_tokens: False\n17:04:12 |     fp16: True\n17:04:12 |     fp16_impl: safe\n17:04:12 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt\n17:04:12 |     fromfile_datatype_extension: False\n17:04:12 |     gpu: -1\n17:04:12 |     gradient_clip: 0.1\n17:04:12 |     hide_labels: False\n17:04:12 |     history_add_global_end_token: None\n17:04:12 |     history_reversed: False\n17:04:12 |     history_size: 20\n17:04:12 |     ignore_bad_candidates: False\n17:04:12 |     ignore_labels: None\n17:04:12 |     image_cropsize: 224\n17:04:12 |     image_mode: raw\n17:04:12 |     image_size: 256\n17:04:12 |     inference: max\n17:04:12 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:04:12 |     init_opt: None\n17:04:12 |     interactive_candidates: fixed\n17:04:12 |     interactive_mode: False\n17:04:12 |     invsqrt_lr_decay_gamma: -1\n17:04:12 |     is_debug: False\n17:04:12 |     label_truncate: 72\n17:04:12 |     learn_embeddings: True\n17:04:12 |     learn_positional_embeddings: True\n17:04:12 |     learningrate: 5e-05\n17:04:12 |     load_from_checkpoint: False\n17:04:12 |     load_from_pretrained_ranker: True\n17:04:12 |     log_every_n_secs: 10.0\n17:04:12 |     log_every_n_steps: 50\n17:04:12 |     log_keep_fields: all\n17:04:12 |     loglevel: info\n17:04:12 |     lr_scheduler: reduceonplateau\n17:04:12 |     lr_scheduler_decay: 0.5\n17:04:12 |     lr_scheduler_patience: 3\n17:04:12 |     max_train_steps: -1\n17:04:12 |     max_train_time: 7200.0\n17:04:12 |     memory_attention: sqrt\n17:04:12 |     metrics: default\n17:04:12 |     model: transformer/classifier\n17:04:12 |     model_file: /tmp/model2\n17:04:12 |     model_parallel: False\n17:04:12 |     momentum: 0\n17:04:12 |     multitask_weights: [1]\n17:04:12 |     mutators: None\n17:04:12 |     n_decoder_layers: -1\n17:04:12 |     n_encoder_layers: -1\n17:04:12 |     n_heads: 12\n17:04:12 |     n_layers: 12\n17:04:12 |     n_positions: 1024\n17:04:12 |     n_segments: 2\n17:04:12 |     nesterov: True\n17:04:12 |     no_cuda: False\n17:04:12 |     normalize_sent_emb: False\n17:04:12 |     num_epochs: -1\n17:04:12 |     num_workers: 0\n17:04:12 |     nus: (0.7,)\n17:04:12 |     optimizer: adamax\n17:04:12 |     output_scaling: 0.06\n17:04:12 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n17:04:12 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:04:12 |     person_tokens: False\n17:04:12 |     print_scores: False\n17:04:12 |     rank_candidates: False\n17:04:12 |     rank_top_k: -1\n17:04:12 |     reduction_type: mean\n17:04:12 |     ref_class: None\n17:04:12 |     relu_dropout: 0.0\n17:04:12 |     repeat_blocking_heuristic: True\n17:04:12 |     return_cand_scores: False\n17:04:12 |     save_after_valid: True\n17:04:12 |     save_every_n_secs: -1\n17:04:12 |     save_format: conversations\n17:04:12 |     share_encoders: False\n17:04:12 |     share_word_embeddings: False\n17:04:12 |     short_final_eval: False\n17:04:12 |     special_tok_lst: None\n17:04:12 |     split_lines: False\n17:04:12 |     starttime: Dec03_17-04\n17:04:12 |     task: fromfile:parlaiformat\n17:04:12 |     tensorboard_log: False\n17:04:12 |     tensorboard_logdir: None\n17:04:12 |     text_truncate: 360\n17:04:12 |     threshold: 0.5\n17:04:12 |     topk: 5\n17:04:12 |     train_predict: False\n17:04:12 |     truncate: 1024\n17:04:12 |     update_classifier_head_only: False\n17:04:12 |     update_freq: 1\n17:04:12 |     use_memories: False\n17:04:12 |     use_reply: none\n17:04:12 |     validation_cutoff: 1.0\n17:04:12 |     validation_every_n_epochs: -1\n17:04:12 |     validation_every_n_secs: 20.0\n17:04:12 |     validation_every_n_steps: -1\n17:04:12 |     validation_max_exs: -1\n17:04:12 |     validation_metric: accuracy\n17:04:12 |     validation_metric_mode: max\n17:04:12 |     validation_patience: 30\n17:04:12 |     validation_share_agent: False\n17:04:12 |     variant: xlm\n17:04:12 |     verbose: False\n17:04:12 |     wandb_entity: None\n17:04:12 |     wandb_log: False\n17:04:12 |     wandb_name: None\n17:04:12 |     wandb_project: None\n17:04:12 |     warmup_rate: 0.0001\n17:04:12 |     warmup_updates: 1000\n17:04:12 |     weight_decay: None\n17:04:12 |     world_logs: \n17:04:12 |     wrap_memory_encoder: False\n17:04:13 | creating task(s): fromfile:parlaiformat\n17:04:13 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt\n17:04:13 | training...\n17:04:23 | time:10s total_exs:380 total_steps:19 epochs:15.83\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4789 4.789e-10               .5541                 .4624   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6910            .3734              .5175   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .2921 11.21     1 264.1 496.1       0          0 37.57  380   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4789             32768  2.876    .1206 5.937 .6949 9.549e-07 118.7   223   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   19 382.8 719.1 1.883        .4580\n\n17:04:33 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7987 7.987e-10               .8145                 .7636   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8727            .7799              .8469   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7227 10.99     1 259.8  1001       0          0 77.06  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7987             32768  2.761    .1207 6.013 .6412 2.855e-06 120.3 463.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 380.1 1464 3.862        .7974\n\n17:04:33 | creating task(s): fromfile:parlaiformat\n17:04:33 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:04:33 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt\n17:04:33 | running eval: valid\n17:04:33 | eval completed in 0.19s\n17:04:33 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1826       0          0 140.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5781 2.855e-06    72 842.7       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  228 2669            1\n\u001b[0m\n17:04:33 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:04:33 | saving best valid model: /tmp/model2\n17:04:33 | Saving dictionary to /tmp/model2.dict\n17:04:37 | task solved! stopping.\n17:04:37 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:04:37 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:04:37 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:04:37 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:04:37 | Using CUDA\n17:04:37 | loading dictionary from /tmp/model2.dict\n17:04:37 | num words = 54944\n17:04:42 | Loading existing model parameters from /tmp/model2\n17:04:43 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:04:45 | creating task(s): fromfile:parlaiformat\n17:04:45 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:04:45 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt\n17:04:45 | running eval: valid\n17:04:45 | eval completed in 0.20s\n17:04:45 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1748       0          0 134.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5781 2.855e-06    72 806.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  228 2554            1\n\u001b[0m\n17:04:45 | creating task(s): fromfile:parlaiformat\n17:04:45 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:04:45 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt\n17:04:45 | running eval: test\n17:04:45 | eval completed in 0.19s\n17:04:45 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1828       0          0 140.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5781 2.855e-06    72 843.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  228 2671            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:04:47.284474Z","iopub.execute_input":"2022-12-03T17:04:47.284872Z","iopub.status.idle":"2022-12-03T17:05:15.630829Z","shell.execute_reply.started":"2022-12-03T17:04:47.284833Z","shell.execute_reply":"2022-12-03T17:05:15.629555Z"},"scrolled":true,"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"17:04:54 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt)\u001b[0m\n17:04:54 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:04:54 | Using CUDA\n17:04:54 | loading dictionary from /tmp/model2.dict\n17:04:55 | num words = 54944\n17:04:59 | Loading existing model parameters from /tmp/model2\n17:05:06 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:05:08 | Opt:\n17:05:08 |     activation: gelu\n17:05:08 |     adafactor_eps: '[1e-30, 0.001]'\n17:05:08 |     adam_eps: 1e-08\n17:05:08 |     add_p1_after_newln: False\n17:05:08 |     aggregate_micro: False\n17:05:08 |     allow_missing_init_opts: False\n17:05:08 |     area_under_curve_class: None\n17:05:08 |     area_under_curve_digits: -1\n17:05:08 |     attention_dropout: 0.1\n17:05:08 |     batchsize: 40\n17:05:08 |     betas: '[0.9, 0.999]'\n17:05:08 |     bpe_add_prefix_space: None\n17:05:08 |     bpe_debug: False\n17:05:08 |     bpe_dropout: None\n17:05:08 |     bpe_merge: None\n17:05:08 |     bpe_vocab: None\n17:05:08 |     candidates: inline\n17:05:08 |     cap_num_predictions: 100\n17:05:08 |     checkpoint_activations: False\n17:05:08 |     class_weights: None\n17:05:08 |     classes: \"['__notok__', '__ok__']\"\n17:05:08 |     classes_from_file: None\n17:05:08 |     data_parallel: True\n17:05:08 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:05:08 |     datatype: train\n17:05:08 |     delimiter: '\\n'\n17:05:08 |     dict_class: parlai.core.dict:DictionaryAgent\n17:05:08 |     dict_endtoken: __start__\n17:05:08 |     dict_file: /tmp/model2.dict\n17:05:08 |     dict_include_test: False\n17:05:08 |     dict_include_valid: False\n17:05:08 |     dict_initpath: None\n17:05:08 |     dict_language: english\n17:05:08 |     dict_loaded: True\n17:05:08 |     dict_lower: True\n17:05:08 |     dict_max_ngram_size: -1\n17:05:08 |     dict_maxexs: -1\n17:05:08 |     dict_maxtokens: -1\n17:05:08 |     dict_minfreq: 0\n17:05:08 |     dict_nulltoken: __null__\n17:05:08 |     dict_starttoken: __start__\n17:05:08 |     dict_textfields: text,labels\n17:05:08 |     dict_tokenizer: bpe\n17:05:08 |     dict_unktoken: __unk__\n17:05:08 |     display_examples: False\n17:05:08 |     download_path: None\n17:05:08 |     dropout: 0.1\n17:05:08 |     dynamic_batching: None\n17:05:08 |     embedding_projection: random\n17:05:08 |     embedding_size: 768\n17:05:08 |     embedding_type: random\n17:05:08 |     embeddings_scale: False\n17:05:08 |     encode_candidate_vecs: True\n17:05:08 |     encode_candidate_vecs_batchsize: 256\n17:05:08 |     eval_batchsize: None\n17:05:08 |     eval_candidates: inline\n17:05:08 |     eval_dynamic_batching: None\n17:05:08 |     evaltask: None\n17:05:08 |     ffn_size: 3072\n17:05:08 |     final_extra_opt: \n17:05:08 |     fixed_candidate_vecs: reuse\n17:05:08 |     fixed_candidates_path: None\n17:05:08 |     force_fp16_tokens: True\n17:05:08 |     fp16: True\n17:05:08 |     fp16_impl: safe\n17:05:08 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-a.txt\n17:05:08 |     fromfile_datatype_extension: False\n17:05:08 |     gpu: -1\n17:05:08 |     gradient_clip: 0.1\n17:05:08 |     hide_labels: False\n17:05:08 |     history_add_global_end_token: None\n17:05:08 |     history_reversed: False\n17:05:08 |     history_size: 20\n17:05:08 |     ignore_bad_candidates: False\n17:05:08 |     ignore_labels: None\n17:05:08 |     image_cropsize: 224\n17:05:08 |     image_mode: raw\n17:05:08 |     image_size: 256\n17:05:08 |     inference: max\n17:05:08 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:05:08 |     init_opt: None\n17:05:08 |     interactive_candidates: fixed\n17:05:08 |     interactive_mode: False\n17:05:08 |     invsqrt_lr_decay_gamma: -1\n17:05:08 |     is_debug: False\n17:05:08 |     label_truncate: 72\n17:05:08 |     learn_embeddings: True\n17:05:08 |     learn_positional_embeddings: True\n17:05:08 |     learningrate: 5e-05\n17:05:08 |     load_from_pretrained_ranker: True\n17:05:08 |     log_every_n_secs: 10.0\n17:05:08 |     log_every_n_steps: 50\n17:05:08 |     log_keep_fields: all\n17:05:08 |     loglevel: info\n17:05:08 |     lr_scheduler: reduceonplateau\n17:05:08 |     lr_scheduler_decay: 0.5\n17:05:08 |     lr_scheduler_patience: 3\n17:05:08 |     max_train_steps: -1\n17:05:08 |     max_train_time: 7200.0\n17:05:08 |     memory_attention: sqrt\n17:05:08 |     metrics: default\n17:05:08 |     model: transformer/classifier\n17:05:08 |     model_file: /tmp/model2\n17:05:08 |     model_parallel: False\n17:05:08 |     momentum: 0\n17:05:08 |     multitask_weights: [1]\n17:05:08 |     mutators: None\n17:05:08 |     n_decoder_layers: -1\n17:05:08 |     n_encoder_layers: -1\n17:05:08 |     n_heads: 12\n17:05:08 |     n_layers: 12\n17:05:08 |     n_positions: 1024\n17:05:08 |     n_segments: 2\n17:05:08 |     nesterov: True\n17:05:08 |     no_cuda: False\n17:05:08 |     normalize_sent_emb: False\n17:05:08 |     num_epochs: -1\n17:05:08 |     num_examples: -1\n17:05:08 |     num_workers: 0\n17:05:08 |     nus: [0.7]\n17:05:08 |     optimizer: adamax\n17:05:08 |     output_scaling: 0.06\n17:05:08 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n17:05:08 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:05:08 |     person_tokens: False\n17:05:08 |     print_scores: False\n17:05:08 |     rank_candidates: False\n17:05:08 |     rank_top_k: -1\n17:05:08 |     reduction_type: mean\n17:05:08 |     ref_class: None\n17:05:08 |     relu_dropout: 0.0\n17:05:08 |     repeat_blocking_heuristic: True\n17:05:08 |     report_filename: \n17:05:08 |     return_cand_scores: False\n17:05:08 |     save_after_valid: True\n17:05:08 |     save_every_n_secs: -1\n17:05:08 |     save_format: conversations\n17:05:08 |     share_encoders: False\n17:05:08 |     share_word_embeddings: False\n17:05:08 |     short_final_eval: False\n17:05:08 |     special_tok_lst: None\n17:05:08 |     split_lines: False\n17:05:08 |     starttime: Dec03_17-04\n17:05:08 |     task: fromfile:parlaiformat\n17:05:08 |     tensorboard_log: False\n17:05:08 |     tensorboard_logdir: None\n17:05:08 |     text_truncate: 360\n17:05:08 |     threshold: 0.5\n17:05:08 |     topk: 5\n17:05:08 |     train_predict: False\n17:05:08 |     truncate: 1024\n17:05:08 |     update_classifier_head_only: False\n17:05:08 |     update_freq: 1\n17:05:08 |     use_memories: False\n17:05:08 |     use_reply: none\n17:05:08 |     validation_cutoff: 1.0\n17:05:08 |     validation_every_n_epochs: -1\n17:05:08 |     validation_every_n_secs: 20.0\n17:05:08 |     validation_every_n_steps: -1\n17:05:08 |     validation_max_exs: -1\n17:05:08 |     validation_metric: accuracy\n17:05:08 |     validation_metric_mode: max\n17:05:08 |     validation_patience: 30\n17:05:08 |     validation_share_agent: False\n17:05:08 |     variant: xlm\n17:05:08 |     verbose: False\n17:05:08 |     wandb_entity: None\n17:05:08 |     wandb_log: False\n17:05:08 |     wandb_name: None\n17:05:08 |     wandb_project: None\n17:05:08 |     warmup_rate: 0.0001\n17:05:08 |     warmup_updates: 1000\n17:05:08 |     weight_decay: None\n17:05:08 |     world_logs: \n17:05:08 |     wrap_memory_encoder: False\n17:05:08 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:05:08 | creating task(s): fromfile:parlaiformat\n17:05:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:05:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-a.txt\n17:05:13 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2650 2.65e-10               .1404                 .1714   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1188            .3581              .3154   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4141  11.7 547.8 519.2       0          0 37.91  200 .2650   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .7440 2.855e-06 240.4 227.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 788.2 747.1        .2481\u001b[0m\n17:05:13 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2650 2.65e-10               .1404                 .1714   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1188            .3581              .3154   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4141  11.7 547.8 519.2       0          0 37.91  200 .2650   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .7440 2.855e-06 240.4 227.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 788.2 747.1        .2481\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:05:15.632522Z","iopub.execute_input":"2022-12-03T17:05:15.632901Z","iopub.status.idle":"2022-12-03T17:05:42.205603Z","shell.execute_reply.started":"2022-12-03T17:05:15.632860Z","shell.execute_reply":"2022-12-03T17:05:42.204132Z"},"scrolled":true,"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"17:05:23 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_valid.txt)\u001b[0m\n17:05:23 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:05:23 | Using CUDA\n17:05:23 | loading dictionary from /tmp/model2.dict\n17:05:23 | num words = 54944\n17:05:27 | Loading existing model parameters from /tmp/model2\n17:05:33 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:05:34 | Opt:\n17:05:34 |     activation: gelu\n17:05:34 |     adafactor_eps: '[1e-30, 0.001]'\n17:05:34 |     adam_eps: 1e-08\n17:05:34 |     add_p1_after_newln: False\n17:05:34 |     aggregate_micro: False\n17:05:34 |     allow_missing_init_opts: False\n17:05:34 |     area_under_curve_class: None\n17:05:34 |     area_under_curve_digits: -1\n17:05:34 |     attention_dropout: 0.1\n17:05:34 |     batchsize: 40\n17:05:34 |     betas: '[0.9, 0.999]'\n17:05:34 |     bpe_add_prefix_space: None\n17:05:34 |     bpe_debug: False\n17:05:34 |     bpe_dropout: None\n17:05:34 |     bpe_merge: None\n17:05:34 |     bpe_vocab: None\n17:05:34 |     candidates: inline\n17:05:34 |     cap_num_predictions: 100\n17:05:34 |     checkpoint_activations: False\n17:05:34 |     class_weights: None\n17:05:34 |     classes: \"['__notok__', '__ok__']\"\n17:05:34 |     classes_from_file: None\n17:05:34 |     data_parallel: True\n17:05:34 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:05:34 |     datatype: train\n17:05:34 |     delimiter: '\\n'\n17:05:34 |     dict_class: parlai.core.dict:DictionaryAgent\n17:05:34 |     dict_endtoken: __start__\n17:05:34 |     dict_file: /tmp/model2.dict\n17:05:34 |     dict_include_test: False\n17:05:34 |     dict_include_valid: False\n17:05:34 |     dict_initpath: None\n17:05:34 |     dict_language: english\n17:05:34 |     dict_loaded: True\n17:05:34 |     dict_lower: True\n17:05:34 |     dict_max_ngram_size: -1\n17:05:34 |     dict_maxexs: -1\n17:05:34 |     dict_maxtokens: -1\n17:05:34 |     dict_minfreq: 0\n17:05:34 |     dict_nulltoken: __null__\n17:05:34 |     dict_starttoken: __start__\n17:05:34 |     dict_textfields: text,labels\n17:05:34 |     dict_tokenizer: bpe\n17:05:34 |     dict_unktoken: __unk__\n17:05:34 |     display_examples: False\n17:05:34 |     download_path: None\n17:05:34 |     dropout: 0.1\n17:05:34 |     dynamic_batching: None\n17:05:34 |     embedding_projection: random\n17:05:34 |     embedding_size: 768\n17:05:34 |     embedding_type: random\n17:05:34 |     embeddings_scale: False\n17:05:34 |     encode_candidate_vecs: True\n17:05:34 |     encode_candidate_vecs_batchsize: 256\n17:05:34 |     eval_batchsize: None\n17:05:34 |     eval_candidates: inline\n17:05:34 |     eval_dynamic_batching: None\n17:05:34 |     evaltask: None\n17:05:34 |     ffn_size: 3072\n17:05:34 |     final_extra_opt: \n17:05:34 |     fixed_candidate_vecs: reuse\n17:05:34 |     fixed_candidates_path: None\n17:05:34 |     force_fp16_tokens: True\n17:05:34 |     fp16: True\n17:05:34 |     fp16_impl: safe\n17:05:34 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-b.txt\n17:05:34 |     fromfile_datatype_extension: False\n17:05:34 |     gpu: -1\n17:05:34 |     gradient_clip: 0.1\n17:05:34 |     hide_labels: False\n17:05:34 |     history_add_global_end_token: None\n17:05:34 |     history_reversed: False\n17:05:34 |     history_size: 20\n17:05:34 |     ignore_bad_candidates: False\n17:05:34 |     ignore_labels: None\n17:05:34 |     image_cropsize: 224\n17:05:34 |     image_mode: raw\n17:05:34 |     image_size: 256\n17:05:34 |     inference: max\n17:05:34 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:05:34 |     init_opt: None\n17:05:34 |     interactive_candidates: fixed\n17:05:34 |     interactive_mode: False\n17:05:34 |     invsqrt_lr_decay_gamma: -1\n17:05:34 |     is_debug: False\n17:05:34 |     label_truncate: 72\n17:05:34 |     learn_embeddings: True\n17:05:34 |     learn_positional_embeddings: True\n17:05:34 |     learningrate: 5e-05\n17:05:34 |     load_from_pretrained_ranker: True\n17:05:34 |     log_every_n_secs: 10.0\n17:05:34 |     log_every_n_steps: 50\n17:05:34 |     log_keep_fields: all\n17:05:34 |     loglevel: info\n17:05:34 |     lr_scheduler: reduceonplateau\n17:05:34 |     lr_scheduler_decay: 0.5\n17:05:34 |     lr_scheduler_patience: 3\n17:05:34 |     max_train_steps: -1\n17:05:34 |     max_train_time: 7200.0\n17:05:34 |     memory_attention: sqrt\n17:05:34 |     metrics: default\n17:05:34 |     model: transformer/classifier\n17:05:34 |     model_file: /tmp/model2\n17:05:34 |     model_parallel: False\n17:05:34 |     momentum: 0\n17:05:34 |     multitask_weights: [1]\n17:05:34 |     mutators: None\n17:05:34 |     n_decoder_layers: -1\n17:05:34 |     n_encoder_layers: -1\n17:05:34 |     n_heads: 12\n17:05:34 |     n_layers: 12\n17:05:34 |     n_positions: 1024\n17:05:34 |     n_segments: 2\n17:05:34 |     nesterov: True\n17:05:34 |     no_cuda: False\n17:05:34 |     normalize_sent_emb: False\n17:05:34 |     num_epochs: -1\n17:05:34 |     num_examples: -1\n17:05:34 |     num_workers: 0\n17:05:34 |     nus: [0.7]\n17:05:34 |     optimizer: adamax\n17:05:34 |     output_scaling: 0.06\n17:05:34 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n17:05:34 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:05:34 |     person_tokens: False\n17:05:34 |     print_scores: False\n17:05:34 |     rank_candidates: False\n17:05:34 |     rank_top_k: -1\n17:05:34 |     reduction_type: mean\n17:05:34 |     ref_class: None\n17:05:34 |     relu_dropout: 0.0\n17:05:34 |     repeat_blocking_heuristic: True\n17:05:34 |     report_filename: \n17:05:34 |     return_cand_scores: False\n17:05:34 |     save_after_valid: True\n17:05:34 |     save_every_n_secs: -1\n17:05:34 |     save_format: conversations\n17:05:34 |     share_encoders: False\n17:05:34 |     share_word_embeddings: False\n17:05:34 |     short_final_eval: False\n17:05:34 |     special_tok_lst: None\n17:05:34 |     split_lines: False\n17:05:34 |     starttime: Dec03_17-04\n17:05:34 |     task: fromfile:parlaiformat\n17:05:34 |     tensorboard_log: False\n17:05:34 |     tensorboard_logdir: None\n17:05:34 |     text_truncate: 360\n17:05:34 |     threshold: 0.5\n17:05:34 |     topk: 5\n17:05:34 |     train_predict: False\n17:05:34 |     truncate: 1024\n17:05:34 |     update_classifier_head_only: False\n17:05:34 |     update_freq: 1\n17:05:34 |     use_memories: False\n17:05:34 |     use_reply: none\n17:05:34 |     validation_cutoff: 1.0\n17:05:34 |     validation_every_n_epochs: -1\n17:05:34 |     validation_every_n_secs: 20.0\n17:05:34 |     validation_every_n_steps: -1\n17:05:34 |     validation_max_exs: -1\n17:05:34 |     validation_metric: accuracy\n17:05:34 |     validation_metric_mode: max\n17:05:34 |     validation_patience: 30\n17:05:34 |     validation_share_agent: False\n17:05:34 |     variant: xlm\n17:05:34 |     verbose: False\n17:05:34 |     wandb_entity: None\n17:05:34 |     wandb_log: False\n17:05:34 |     wandb_name: None\n17:05:34 |     wandb_project: None\n17:05:34 |     warmup_rate: 0.0001\n17:05:34 |     warmup_updates: 1000\n17:05:34 |     weight_decay: None\n17:05:34 |     world_logs: \n17:05:34 |     wrap_memory_encoder: False\n17:05:34 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:05:34 | creating task(s): fromfile:parlaiformat\n17:05:34 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:05:34 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run2/data_train-b.txt\n17:05:40 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7350 7.35e-10               .6864                 .8286   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5859            .7706              .6846   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8812  11.7 547.8 507.2       0          0 37.03  200 .7350   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .6517 2.855e-06 239.6 221.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 787.4  729        .7289\u001b[0m\n17:05:40 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7350 7.35e-10               .6864                 .8286   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5859            .7706              .6846   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8812  11.7 547.8 507.2       0          0 37.03  200 .7350   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .6517 2.855e-06 239.6 221.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 787.4  729        .7289\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:05:42.207849Z","iopub.execute_input":"2022-12-03T17:05:42.208245Z","iopub.status.idle":"2022-12-03T17:05:43.328698Z","shell.execute_reply.started":"2022-12-03T17:05:42.208204Z","shell.execute_reply":"2022-12-03T17:05:43.327339Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:05:43.330551Z","iopub.execute_input":"2022-12-03T17:05:43.330953Z","iopub.status.idle":"2022-12-03T17:06:38.208623Z","shell.execute_reply.started":"2022-12-03T17:05:43.330915Z","shell.execute_reply":"2022-12-03T17:06:38.207344Z"},"scrolled":true,"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"17:05:50 | building dictionary first...\n17:05:50 | No model with opt yet at: /tmp/model3(.opt)\n17:05:50 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:05:50 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:05:50 | Using CUDA\n17:05:50 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:05:50 | num words = 54944\n17:05:55 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:06:03 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:06:03 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:06:03 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:06:03 | Opt:\n17:06:03 |     activation: gelu\n17:06:03 |     adafactor_eps: '(1e-30, 0.001)'\n17:06:03 |     adam_eps: 1e-08\n17:06:03 |     add_p1_after_newln: False\n17:06:03 |     aggregate_micro: False\n17:06:03 |     allow_missing_init_opts: False\n17:06:03 |     attention_dropout: 0.1\n17:06:03 |     batchsize: 20\n17:06:03 |     betas: '(0.9, 0.999)'\n17:06:03 |     bpe_add_prefix_space: None\n17:06:03 |     bpe_debug: False\n17:06:03 |     bpe_dropout: None\n17:06:03 |     bpe_merge: None\n17:06:03 |     bpe_vocab: None\n17:06:03 |     candidates: inline\n17:06:03 |     cap_num_predictions: 100\n17:06:03 |     checkpoint_activations: False\n17:06:03 |     class_weights: None\n17:06:03 |     classes: \"['__notok__', '__ok__']\"\n17:06:03 |     classes_from_file: None\n17:06:03 |     data_parallel: True\n17:06:03 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:06:03 |     datatype: train\n17:06:03 |     delimiter: '\\n'\n17:06:03 |     dict_class: parlai.core.dict:DictionaryAgent\n17:06:03 |     dict_endtoken: __start__\n17:06:03 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:06:03 |     dict_include_test: False\n17:06:03 |     dict_include_valid: False\n17:06:03 |     dict_initpath: None\n17:06:03 |     dict_language: english\n17:06:03 |     dict_loaded: True\n17:06:03 |     dict_lower: True\n17:06:03 |     dict_max_ngram_size: -1\n17:06:03 |     dict_maxexs: -1\n17:06:03 |     dict_maxtokens: -1\n17:06:03 |     dict_minfreq: 0\n17:06:03 |     dict_nulltoken: __null__\n17:06:03 |     dict_starttoken: __start__\n17:06:03 |     dict_textfields: text,labels\n17:06:03 |     dict_tokenizer: bpe\n17:06:03 |     dict_unktoken: __unk__\n17:06:03 |     display_examples: False\n17:06:03 |     download_path: None\n17:06:03 |     dropout: 0.1\n17:06:03 |     dynamic_batching: None\n17:06:03 |     embedding_projection: random\n17:06:03 |     embedding_size: 768\n17:06:03 |     embedding_type: random\n17:06:03 |     embeddings_scale: False\n17:06:03 |     encode_candidate_vecs: True\n17:06:03 |     encode_candidate_vecs_batchsize: 256\n17:06:03 |     eval_batchsize: None\n17:06:03 |     eval_candidates: inline\n17:06:03 |     eval_dynamic_batching: None\n17:06:03 |     evaltask: None\n17:06:03 |     ffn_size: 3072\n17:06:03 |     final_extra_opt: \n17:06:03 |     fixed_candidate_vecs: reuse\n17:06:03 |     fixed_candidates_path: None\n17:06:03 |     force_fp16_tokens: False\n17:06:03 |     fp16: True\n17:06:03 |     fp16_impl: safe\n17:06:03 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt\n17:06:03 |     fromfile_datatype_extension: False\n17:06:03 |     gpu: -1\n17:06:03 |     gradient_clip: 0.1\n17:06:03 |     hide_labels: False\n17:06:03 |     history_add_global_end_token: None\n17:06:03 |     history_reversed: False\n17:06:03 |     history_size: 20\n17:06:03 |     ignore_bad_candidates: False\n17:06:03 |     ignore_labels: None\n17:06:03 |     image_cropsize: 224\n17:06:03 |     image_mode: raw\n17:06:03 |     image_size: 256\n17:06:03 |     inference: max\n17:06:03 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:06:03 |     init_opt: None\n17:06:03 |     interactive_candidates: fixed\n17:06:03 |     interactive_mode: False\n17:06:03 |     invsqrt_lr_decay_gamma: -1\n17:06:03 |     is_debug: False\n17:06:03 |     label_truncate: 72\n17:06:03 |     learn_embeddings: True\n17:06:03 |     learn_positional_embeddings: True\n17:06:03 |     learningrate: 5e-05\n17:06:03 |     load_from_checkpoint: False\n17:06:03 |     load_from_pretrained_ranker: True\n17:06:03 |     log_every_n_secs: 10.0\n17:06:03 |     log_every_n_steps: 50\n17:06:03 |     log_keep_fields: all\n17:06:03 |     loglevel: info\n17:06:03 |     lr_scheduler: reduceonplateau\n17:06:03 |     lr_scheduler_decay: 0.5\n17:06:03 |     lr_scheduler_patience: 3\n17:06:03 |     max_train_steps: -1\n17:06:03 |     max_train_time: 7200.0\n17:06:03 |     memory_attention: sqrt\n17:06:03 |     metrics: default\n17:06:03 |     model: transformer/classifier\n17:06:03 |     model_file: /tmp/model3\n17:06:03 |     model_parallel: False\n17:06:03 |     momentum: 0\n17:06:03 |     multitask_weights: [1]\n17:06:03 |     mutators: None\n17:06:03 |     n_decoder_layers: -1\n17:06:03 |     n_encoder_layers: -1\n17:06:03 |     n_heads: 12\n17:06:03 |     n_layers: 12\n17:06:03 |     n_positions: 1024\n17:06:03 |     n_segments: 2\n17:06:03 |     nesterov: True\n17:06:03 |     no_cuda: False\n17:06:03 |     normalize_sent_emb: False\n17:06:03 |     num_epochs: -1\n17:06:03 |     num_workers: 0\n17:06:03 |     nus: (0.7,)\n17:06:03 |     optimizer: adamax\n17:06:03 |     output_scaling: 0.06\n17:06:03 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n17:06:03 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:06:03 |     person_tokens: False\n17:06:03 |     print_scores: False\n17:06:03 |     rank_candidates: False\n17:06:03 |     rank_top_k: -1\n17:06:03 |     reduction_type: mean\n17:06:03 |     ref_class: None\n17:06:03 |     relu_dropout: 0.0\n17:06:03 |     repeat_blocking_heuristic: True\n17:06:03 |     return_cand_scores: False\n17:06:03 |     save_after_valid: True\n17:06:03 |     save_every_n_secs: -1\n17:06:03 |     save_format: conversations\n17:06:03 |     share_encoders: False\n17:06:03 |     share_word_embeddings: False\n17:06:03 |     short_final_eval: False\n17:06:03 |     special_tok_lst: None\n17:06:03 |     split_lines: False\n17:06:03 |     starttime: Dec03_17-05\n17:06:03 |     task: fromfile:parlaiformat\n17:06:03 |     tensorboard_log: False\n17:06:03 |     tensorboard_logdir: None\n17:06:03 |     text_truncate: 360\n17:06:03 |     threshold: 0.5\n17:06:03 |     topk: 5\n17:06:03 |     train_predict: False\n17:06:03 |     truncate: 1024\n17:06:03 |     update_classifier_head_only: False\n17:06:03 |     update_freq: 1\n17:06:03 |     use_memories: False\n17:06:03 |     use_reply: none\n17:06:03 |     validation_cutoff: 1.0\n17:06:03 |     validation_every_n_epochs: -1\n17:06:03 |     validation_every_n_secs: 20.0\n17:06:03 |     validation_every_n_steps: -1\n17:06:03 |     validation_max_exs: -1\n17:06:03 |     validation_metric: accuracy\n17:06:03 |     validation_metric_mode: max\n17:06:03 |     validation_patience: 30\n17:06:03 |     validation_share_agent: False\n17:06:03 |     variant: xlm\n17:06:03 |     verbose: False\n17:06:03 |     wandb_entity: None\n17:06:03 |     wandb_log: False\n17:06:03 |     wandb_name: None\n17:06:03 |     wandb_project: None\n17:06:03 |     warmup_rate: 0.0001\n17:06:03 |     warmup_updates: 1000\n17:06:03 |     weight_decay: None\n17:06:03 |     world_logs: \n17:06:03 |     wrap_memory_encoder: False\n17:06:04 | creating task(s): fromfile:parlaiformat\n17:06:04 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt\n17:06:04 | training...\n17:06:14 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5833 5.833e-10               .6269                 .5927   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6652            .5283              .5698   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .4925 11.78     1 275.6 572.6       0          0 41.55  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5833             32768  2.679    .1206 6.052 .6784 1.055e-06   121 251.5   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 396.7 824.1 2.082        .5802\n\n17:06:24 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8149 8.149e-10               .8315                 .7752   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8966            .7946              .8717   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7300 11.79     1 275.8  1050       0          0 76.15  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8149             32768  2.616    .1207 6.019 .6247 2.905e-06 120.4 458.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 396.2 1508 3.816        .8134\n\n17:06:24 | creating task(s): fromfile:parlaiformat\n17:06:24 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:06:24 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt\n17:06:24 | running eval: valid\n17:06:24 | eval completed in 0.20s\n17:06:24 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1905       0          0 138.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5508 2.905e-06    72 833.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2739            1\n\u001b[0m\n17:06:24 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:06:24 | saving best valid model: /tmp/model3\n17:06:24 | Saving dictionary to /tmp/model3.dict\n17:06:27 | task solved! stopping.\n17:06:28 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:06:28 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:06:28 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:06:28 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:06:28 | Using CUDA\n17:06:28 | loading dictionary from /tmp/model3.dict\n17:06:28 | num words = 54944\n17:06:33 | Loading existing model parameters from /tmp/model3\n17:06:34 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:06:36 | creating task(s): fromfile:parlaiformat\n17:06:36 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:06:36 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt\n17:06:36 | running eval: valid\n17:06:36 | eval completed in 0.20s\n17:06:36 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1817       0          0 132.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5508 2.905e-06    72 795.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2613            1\n\u001b[0m\n17:06:36 | creating task(s): fromfile:parlaiformat\n17:06:36 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:06:36 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt\n17:06:36 | running eval: test\n17:06:36 | eval completed in 0.20s\n17:06:36 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1839       0          0 134.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5508 2.905e-06    72 804.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2644            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:06:38.210563Z","iopub.execute_input":"2022-12-03T17:06:38.210913Z","iopub.status.idle":"2022-12-03T17:07:06.005724Z","shell.execute_reply.started":"2022-12-03T17:06:38.210883Z","shell.execute_reply":"2022-12-03T17:07:06.004556Z"},"scrolled":true,"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"17:06:45 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt)\u001b[0m\n17:06:45 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:06:45 | Using CUDA\n17:06:45 | loading dictionary from /tmp/model3.dict\n17:06:45 | num words = 54944\n17:06:49 | Loading existing model parameters from /tmp/model3\n17:06:57 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:06:58 | Opt:\n17:06:58 |     activation: gelu\n17:06:58 |     adafactor_eps: '[1e-30, 0.001]'\n17:06:58 |     adam_eps: 1e-08\n17:06:58 |     add_p1_after_newln: False\n17:06:58 |     aggregate_micro: False\n17:06:58 |     allow_missing_init_opts: False\n17:06:58 |     area_under_curve_class: None\n17:06:58 |     area_under_curve_digits: -1\n17:06:58 |     attention_dropout: 0.1\n17:06:58 |     batchsize: 40\n17:06:58 |     betas: '[0.9, 0.999]'\n17:06:58 |     bpe_add_prefix_space: None\n17:06:58 |     bpe_debug: False\n17:06:58 |     bpe_dropout: None\n17:06:58 |     bpe_merge: None\n17:06:58 |     bpe_vocab: None\n17:06:58 |     candidates: inline\n17:06:58 |     cap_num_predictions: 100\n17:06:58 |     checkpoint_activations: False\n17:06:58 |     class_weights: None\n17:06:58 |     classes: \"['__notok__', '__ok__']\"\n17:06:58 |     classes_from_file: None\n17:06:58 |     data_parallel: True\n17:06:58 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:06:58 |     datatype: train\n17:06:58 |     delimiter: '\\n'\n17:06:58 |     dict_class: parlai.core.dict:DictionaryAgent\n17:06:58 |     dict_endtoken: __start__\n17:06:58 |     dict_file: /tmp/model3.dict\n17:06:58 |     dict_include_test: False\n17:06:58 |     dict_include_valid: False\n17:06:58 |     dict_initpath: None\n17:06:58 |     dict_language: english\n17:06:58 |     dict_loaded: True\n17:06:58 |     dict_lower: True\n17:06:58 |     dict_max_ngram_size: -1\n17:06:58 |     dict_maxexs: -1\n17:06:58 |     dict_maxtokens: -1\n17:06:58 |     dict_minfreq: 0\n17:06:58 |     dict_nulltoken: __null__\n17:06:58 |     dict_starttoken: __start__\n17:06:58 |     dict_textfields: text,labels\n17:06:58 |     dict_tokenizer: bpe\n17:06:58 |     dict_unktoken: __unk__\n17:06:58 |     display_examples: False\n17:06:58 |     download_path: None\n17:06:58 |     dropout: 0.1\n17:06:58 |     dynamic_batching: None\n17:06:58 |     embedding_projection: random\n17:06:58 |     embedding_size: 768\n17:06:58 |     embedding_type: random\n17:06:58 |     embeddings_scale: False\n17:06:58 |     encode_candidate_vecs: True\n17:06:58 |     encode_candidate_vecs_batchsize: 256\n17:06:58 |     eval_batchsize: None\n17:06:58 |     eval_candidates: inline\n17:06:58 |     eval_dynamic_batching: None\n17:06:58 |     evaltask: None\n17:06:58 |     ffn_size: 3072\n17:06:58 |     final_extra_opt: \n17:06:58 |     fixed_candidate_vecs: reuse\n17:06:58 |     fixed_candidates_path: None\n17:06:58 |     force_fp16_tokens: True\n17:06:58 |     fp16: True\n17:06:58 |     fp16_impl: safe\n17:06:58 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-a.txt\n17:06:58 |     fromfile_datatype_extension: False\n17:06:58 |     gpu: -1\n17:06:58 |     gradient_clip: 0.1\n17:06:58 |     hide_labels: False\n17:06:58 |     history_add_global_end_token: None\n17:06:58 |     history_reversed: False\n17:06:58 |     history_size: 20\n17:06:58 |     ignore_bad_candidates: False\n17:06:58 |     ignore_labels: None\n17:06:58 |     image_cropsize: 224\n17:06:58 |     image_mode: raw\n17:06:58 |     image_size: 256\n17:06:58 |     inference: max\n17:06:58 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:06:58 |     init_opt: None\n17:06:58 |     interactive_candidates: fixed\n17:06:58 |     interactive_mode: False\n17:06:58 |     invsqrt_lr_decay_gamma: -1\n17:06:58 |     is_debug: False\n17:06:58 |     label_truncate: 72\n17:06:58 |     learn_embeddings: True\n17:06:58 |     learn_positional_embeddings: True\n17:06:58 |     learningrate: 5e-05\n17:06:58 |     load_from_pretrained_ranker: True\n17:06:58 |     log_every_n_secs: 10.0\n17:06:58 |     log_every_n_steps: 50\n17:06:58 |     log_keep_fields: all\n17:06:58 |     loglevel: info\n17:06:58 |     lr_scheduler: reduceonplateau\n17:06:58 |     lr_scheduler_decay: 0.5\n17:06:58 |     lr_scheduler_patience: 3\n17:06:58 |     max_train_steps: -1\n17:06:58 |     max_train_time: 7200.0\n17:06:58 |     memory_attention: sqrt\n17:06:58 |     metrics: default\n17:06:58 |     model: transformer/classifier\n17:06:58 |     model_file: /tmp/model3\n17:06:58 |     model_parallel: False\n17:06:58 |     momentum: 0\n17:06:58 |     multitask_weights: [1]\n17:06:58 |     mutators: None\n17:06:58 |     n_decoder_layers: -1\n17:06:58 |     n_encoder_layers: -1\n17:06:58 |     n_heads: 12\n17:06:58 |     n_layers: 12\n17:06:58 |     n_positions: 1024\n17:06:58 |     n_segments: 2\n17:06:58 |     nesterov: True\n17:06:58 |     no_cuda: False\n17:06:58 |     normalize_sent_emb: False\n17:06:58 |     num_epochs: -1\n17:06:58 |     num_examples: -1\n17:06:58 |     num_workers: 0\n17:06:58 |     nus: [0.7]\n17:06:58 |     optimizer: adamax\n17:06:58 |     output_scaling: 0.06\n17:06:58 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n17:06:58 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:06:58 |     person_tokens: False\n17:06:58 |     print_scores: False\n17:06:58 |     rank_candidates: False\n17:06:58 |     rank_top_k: -1\n17:06:58 |     reduction_type: mean\n17:06:58 |     ref_class: None\n17:06:58 |     relu_dropout: 0.0\n17:06:58 |     repeat_blocking_heuristic: True\n17:06:58 |     report_filename: \n17:06:58 |     return_cand_scores: False\n17:06:58 |     save_after_valid: True\n17:06:58 |     save_every_n_secs: -1\n17:06:58 |     save_format: conversations\n17:06:58 |     share_encoders: False\n17:06:58 |     share_word_embeddings: False\n17:06:58 |     short_final_eval: False\n17:06:58 |     special_tok_lst: None\n17:06:58 |     split_lines: False\n17:06:58 |     starttime: Dec03_17-05\n17:06:58 |     task: fromfile:parlaiformat\n17:06:58 |     tensorboard_log: False\n17:06:58 |     tensorboard_logdir: None\n17:06:58 |     text_truncate: 360\n17:06:58 |     threshold: 0.5\n17:06:58 |     topk: 5\n17:06:58 |     train_predict: False\n17:06:58 |     truncate: 1024\n17:06:58 |     update_classifier_head_only: False\n17:06:58 |     update_freq: 1\n17:06:58 |     use_memories: False\n17:06:58 |     use_reply: none\n17:06:58 |     validation_cutoff: 1.0\n17:06:58 |     validation_every_n_epochs: -1\n17:06:58 |     validation_every_n_secs: 20.0\n17:06:58 |     validation_every_n_steps: -1\n17:06:58 |     validation_max_exs: -1\n17:06:58 |     validation_metric: accuracy\n17:06:58 |     validation_metric_mode: max\n17:06:58 |     validation_patience: 30\n17:06:58 |     validation_share_agent: False\n17:06:58 |     variant: xlm\n17:06:58 |     verbose: False\n17:06:58 |     wandb_entity: None\n17:06:58 |     wandb_log: False\n17:06:58 |     wandb_name: None\n17:06:58 |     wandb_project: None\n17:06:58 |     warmup_rate: 0.0001\n17:06:58 |     warmup_updates: 1000\n17:06:58 |     weight_decay: None\n17:06:58 |     world_logs: \n17:06:58 |     wrap_memory_encoder: False\n17:06:58 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:06:58 | creating task(s): fromfile:parlaiformat\n17:06:58 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:06:58 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-a.txt\n17:07:04 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3450 3.45e-10               .4229                 .3810   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4752            .2428              .2838   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2121 11.47   539 515.3       0          0 38.24  200 .3450   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .7334 2.905e-06 240.4 229.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 779.4 745.1        .3337\u001b[0m\n17:07:04 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3450 3.45e-10               .4229                 .3810   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4752            .2428              .2838   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2121 11.47   539 515.3       0          0 38.24  200 .3450   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .7334 2.905e-06 240.4 229.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 779.4 745.1        .3337\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:07:06.007443Z","iopub.execute_input":"2022-12-03T17:07:06.007842Z","iopub.status.idle":"2022-12-03T17:07:32.488282Z","shell.execute_reply.started":"2022-12-03T17:07:06.007801Z","shell.execute_reply":"2022-12-03T17:07:32.487034Z"},"scrolled":true,"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"17:07:12 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_valid.txt)\u001b[0m\n17:07:12 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:07:12 | Using CUDA\n17:07:12 | loading dictionary from /tmp/model3.dict\n17:07:13 | num words = 54944\n17:07:17 | Loading existing model parameters from /tmp/model3\n17:07:23 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:07:24 | Opt:\n17:07:24 |     activation: gelu\n17:07:24 |     adafactor_eps: '[1e-30, 0.001]'\n17:07:24 |     adam_eps: 1e-08\n17:07:24 |     add_p1_after_newln: False\n17:07:24 |     aggregate_micro: False\n17:07:24 |     allow_missing_init_opts: False\n17:07:24 |     area_under_curve_class: None\n17:07:24 |     area_under_curve_digits: -1\n17:07:24 |     attention_dropout: 0.1\n17:07:24 |     batchsize: 40\n17:07:24 |     betas: '[0.9, 0.999]'\n17:07:24 |     bpe_add_prefix_space: None\n17:07:24 |     bpe_debug: False\n17:07:24 |     bpe_dropout: None\n17:07:24 |     bpe_merge: None\n17:07:24 |     bpe_vocab: None\n17:07:24 |     candidates: inline\n17:07:24 |     cap_num_predictions: 100\n17:07:24 |     checkpoint_activations: False\n17:07:24 |     class_weights: None\n17:07:24 |     classes: \"['__notok__', '__ok__']\"\n17:07:24 |     classes_from_file: None\n17:07:24 |     data_parallel: True\n17:07:24 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:07:24 |     datatype: train\n17:07:24 |     delimiter: '\\n'\n17:07:24 |     dict_class: parlai.core.dict:DictionaryAgent\n17:07:24 |     dict_endtoken: __start__\n17:07:24 |     dict_file: /tmp/model3.dict\n17:07:24 |     dict_include_test: False\n17:07:24 |     dict_include_valid: False\n17:07:24 |     dict_initpath: None\n17:07:24 |     dict_language: english\n17:07:24 |     dict_loaded: True\n17:07:24 |     dict_lower: True\n17:07:24 |     dict_max_ngram_size: -1\n17:07:24 |     dict_maxexs: -1\n17:07:24 |     dict_maxtokens: -1\n17:07:24 |     dict_minfreq: 0\n17:07:24 |     dict_nulltoken: __null__\n17:07:24 |     dict_starttoken: __start__\n17:07:24 |     dict_textfields: text,labels\n17:07:24 |     dict_tokenizer: bpe\n17:07:24 |     dict_unktoken: __unk__\n17:07:24 |     display_examples: False\n17:07:24 |     download_path: None\n17:07:24 |     dropout: 0.1\n17:07:24 |     dynamic_batching: None\n17:07:24 |     embedding_projection: random\n17:07:24 |     embedding_size: 768\n17:07:24 |     embedding_type: random\n17:07:24 |     embeddings_scale: False\n17:07:24 |     encode_candidate_vecs: True\n17:07:24 |     encode_candidate_vecs_batchsize: 256\n17:07:24 |     eval_batchsize: None\n17:07:24 |     eval_candidates: inline\n17:07:24 |     eval_dynamic_batching: None\n17:07:24 |     evaltask: None\n17:07:24 |     ffn_size: 3072\n17:07:24 |     final_extra_opt: \n17:07:24 |     fixed_candidate_vecs: reuse\n17:07:24 |     fixed_candidates_path: None\n17:07:24 |     force_fp16_tokens: True\n17:07:24 |     fp16: True\n17:07:24 |     fp16_impl: safe\n17:07:24 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-b.txt\n17:07:24 |     fromfile_datatype_extension: False\n17:07:24 |     gpu: -1\n17:07:24 |     gradient_clip: 0.1\n17:07:24 |     hide_labels: False\n17:07:24 |     history_add_global_end_token: None\n17:07:24 |     history_reversed: False\n17:07:24 |     history_size: 20\n17:07:24 |     ignore_bad_candidates: False\n17:07:24 |     ignore_labels: None\n17:07:24 |     image_cropsize: 224\n17:07:24 |     image_mode: raw\n17:07:24 |     image_size: 256\n17:07:24 |     inference: max\n17:07:24 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:07:24 |     init_opt: None\n17:07:24 |     interactive_candidates: fixed\n17:07:24 |     interactive_mode: False\n17:07:24 |     invsqrt_lr_decay_gamma: -1\n17:07:24 |     is_debug: False\n17:07:24 |     label_truncate: 72\n17:07:24 |     learn_embeddings: True\n17:07:24 |     learn_positional_embeddings: True\n17:07:24 |     learningrate: 5e-05\n17:07:24 |     load_from_pretrained_ranker: True\n17:07:24 |     log_every_n_secs: 10.0\n17:07:24 |     log_every_n_steps: 50\n17:07:24 |     log_keep_fields: all\n17:07:24 |     loglevel: info\n17:07:24 |     lr_scheduler: reduceonplateau\n17:07:24 |     lr_scheduler_decay: 0.5\n17:07:24 |     lr_scheduler_patience: 3\n17:07:24 |     max_train_steps: -1\n17:07:24 |     max_train_time: 7200.0\n17:07:24 |     memory_attention: sqrt\n17:07:24 |     metrics: default\n17:07:24 |     model: transformer/classifier\n17:07:24 |     model_file: /tmp/model3\n17:07:24 |     model_parallel: False\n17:07:24 |     momentum: 0\n17:07:24 |     multitask_weights: [1]\n17:07:24 |     mutators: None\n17:07:24 |     n_decoder_layers: -1\n17:07:24 |     n_encoder_layers: -1\n17:07:24 |     n_heads: 12\n17:07:24 |     n_layers: 12\n17:07:24 |     n_positions: 1024\n17:07:24 |     n_segments: 2\n17:07:24 |     nesterov: True\n17:07:24 |     no_cuda: False\n17:07:24 |     normalize_sent_emb: False\n17:07:24 |     num_epochs: -1\n17:07:24 |     num_examples: -1\n17:07:24 |     num_workers: 0\n17:07:24 |     nus: [0.7]\n17:07:24 |     optimizer: adamax\n17:07:24 |     output_scaling: 0.06\n17:07:24 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n17:07:24 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:07:24 |     person_tokens: False\n17:07:24 |     print_scores: False\n17:07:24 |     rank_candidates: False\n17:07:24 |     rank_top_k: -1\n17:07:24 |     reduction_type: mean\n17:07:24 |     ref_class: None\n17:07:24 |     relu_dropout: 0.0\n17:07:24 |     repeat_blocking_heuristic: True\n17:07:24 |     report_filename: \n17:07:24 |     return_cand_scores: False\n17:07:24 |     save_after_valid: True\n17:07:24 |     save_every_n_secs: -1\n17:07:24 |     save_format: conversations\n17:07:24 |     share_encoders: False\n17:07:24 |     share_word_embeddings: False\n17:07:24 |     short_final_eval: False\n17:07:24 |     special_tok_lst: None\n17:07:24 |     split_lines: False\n17:07:24 |     starttime: Dec03_17-05\n17:07:24 |     task: fromfile:parlaiformat\n17:07:24 |     tensorboard_log: False\n17:07:24 |     tensorboard_logdir: None\n17:07:24 |     text_truncate: 360\n17:07:24 |     threshold: 0.5\n17:07:24 |     topk: 5\n17:07:24 |     train_predict: False\n17:07:24 |     truncate: 1024\n17:07:24 |     update_classifier_head_only: False\n17:07:24 |     update_freq: 1\n17:07:24 |     use_memories: False\n17:07:24 |     use_reply: none\n17:07:24 |     validation_cutoff: 1.0\n17:07:24 |     validation_every_n_epochs: -1\n17:07:24 |     validation_every_n_secs: 20.0\n17:07:24 |     validation_every_n_steps: -1\n17:07:24 |     validation_max_exs: -1\n17:07:24 |     validation_metric: accuracy\n17:07:24 |     validation_metric_mode: max\n17:07:24 |     validation_patience: 30\n17:07:24 |     validation_share_agent: False\n17:07:24 |     variant: xlm\n17:07:24 |     verbose: False\n17:07:24 |     wandb_entity: None\n17:07:24 |     wandb_log: False\n17:07:24 |     wandb_name: None\n17:07:24 |     wandb_project: None\n17:07:24 |     warmup_rate: 0.0001\n17:07:24 |     warmup_updates: 1000\n17:07:24 |     weight_decay: None\n17:07:24 |     world_logs: \n17:07:24 |     wrap_memory_encoder: False\n17:07:24 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:07:24 | creating task(s): fromfile:parlaiformat\n17:07:24 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:07:24 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run3/data_train-b.txt\n17:07:30 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6550 6.55e-10               .6933                 .6190   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7879            .6057              .7162   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5248 11.47   539 453.8       0          0 33.67  200 .6550   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .6613 2.905e-06 239.6 201.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 778.6 655.5        .6491\u001b[0m\n17:07:30 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6550 6.55e-10               .6933                 .6190   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7879            .6057              .7162   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5248 11.47   539 453.8       0          0 33.67  200 .6550   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .6613 2.905e-06 239.6 201.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 778.6 655.5        .6491\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:07:32.490142Z","iopub.execute_input":"2022-12-03T17:07:32.490617Z","iopub.status.idle":"2022-12-03T17:07:33.589635Z","shell.execute_reply.started":"2022-12-03T17:07:32.490575Z","shell.execute_reply":"2022-12-03T17:07:33.588208Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:07:33.591686Z","iopub.execute_input":"2022-12-03T17:07:33.592104Z","iopub.status.idle":"2022-12-03T17:08:25.979722Z","shell.execute_reply.started":"2022-12-03T17:07:33.592062Z","shell.execute_reply":"2022-12-03T17:08:25.978417Z"},"scrolled":true,"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"17:07:40 | building dictionary first...\n17:07:40 | No model with opt yet at: /tmp/model4(.opt)\n17:07:40 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:07:40 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:07:40 | Using CUDA\n17:07:40 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:07:40 | num words = 54944\n17:07:45 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:07:51 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:07:51 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:07:51 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:07:51 | Opt:\n17:07:51 |     activation: gelu\n17:07:51 |     adafactor_eps: '(1e-30, 0.001)'\n17:07:51 |     adam_eps: 1e-08\n17:07:51 |     add_p1_after_newln: False\n17:07:51 |     aggregate_micro: False\n17:07:51 |     allow_missing_init_opts: False\n17:07:51 |     attention_dropout: 0.1\n17:07:51 |     batchsize: 20\n17:07:51 |     betas: '(0.9, 0.999)'\n17:07:51 |     bpe_add_prefix_space: None\n17:07:51 |     bpe_debug: False\n17:07:51 |     bpe_dropout: None\n17:07:51 |     bpe_merge: None\n17:07:51 |     bpe_vocab: None\n17:07:51 |     candidates: inline\n17:07:51 |     cap_num_predictions: 100\n17:07:51 |     checkpoint_activations: False\n17:07:51 |     class_weights: None\n17:07:51 |     classes: \"['__notok__', '__ok__']\"\n17:07:51 |     classes_from_file: None\n17:07:51 |     data_parallel: True\n17:07:51 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:07:51 |     datatype: train\n17:07:51 |     delimiter: '\\n'\n17:07:51 |     dict_class: parlai.core.dict:DictionaryAgent\n17:07:51 |     dict_endtoken: __start__\n17:07:51 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:07:51 |     dict_include_test: False\n17:07:51 |     dict_include_valid: False\n17:07:51 |     dict_initpath: None\n17:07:51 |     dict_language: english\n17:07:51 |     dict_loaded: True\n17:07:51 |     dict_lower: True\n17:07:51 |     dict_max_ngram_size: -1\n17:07:51 |     dict_maxexs: -1\n17:07:51 |     dict_maxtokens: -1\n17:07:51 |     dict_minfreq: 0\n17:07:51 |     dict_nulltoken: __null__\n17:07:51 |     dict_starttoken: __start__\n17:07:51 |     dict_textfields: text,labels\n17:07:51 |     dict_tokenizer: bpe\n17:07:51 |     dict_unktoken: __unk__\n17:07:51 |     display_examples: False\n17:07:51 |     download_path: None\n17:07:51 |     dropout: 0.1\n17:07:51 |     dynamic_batching: None\n17:07:51 |     embedding_projection: random\n17:07:51 |     embedding_size: 768\n17:07:51 |     embedding_type: random\n17:07:51 |     embeddings_scale: False\n17:07:51 |     encode_candidate_vecs: True\n17:07:51 |     encode_candidate_vecs_batchsize: 256\n17:07:51 |     eval_batchsize: None\n17:07:51 |     eval_candidates: inline\n17:07:51 |     eval_dynamic_batching: None\n17:07:51 |     evaltask: None\n17:07:51 |     ffn_size: 3072\n17:07:51 |     final_extra_opt: \n17:07:51 |     fixed_candidate_vecs: reuse\n17:07:51 |     fixed_candidates_path: None\n17:07:51 |     force_fp16_tokens: False\n17:07:51 |     fp16: True\n17:07:51 |     fp16_impl: safe\n17:07:51 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt\n17:07:51 |     fromfile_datatype_extension: False\n17:07:51 |     gpu: -1\n17:07:51 |     gradient_clip: 0.1\n17:07:51 |     hide_labels: False\n17:07:51 |     history_add_global_end_token: None\n17:07:51 |     history_reversed: False\n17:07:51 |     history_size: 20\n17:07:51 |     ignore_bad_candidates: False\n17:07:51 |     ignore_labels: None\n17:07:51 |     image_cropsize: 224\n17:07:51 |     image_mode: raw\n17:07:51 |     image_size: 256\n17:07:51 |     inference: max\n17:07:51 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:07:51 |     init_opt: None\n17:07:51 |     interactive_candidates: fixed\n17:07:51 |     interactive_mode: False\n17:07:51 |     invsqrt_lr_decay_gamma: -1\n17:07:51 |     is_debug: False\n17:07:51 |     label_truncate: 72\n17:07:51 |     learn_embeddings: True\n17:07:51 |     learn_positional_embeddings: True\n17:07:51 |     learningrate: 5e-05\n17:07:51 |     load_from_checkpoint: False\n17:07:51 |     load_from_pretrained_ranker: True\n17:07:51 |     log_every_n_secs: 10.0\n17:07:51 |     log_every_n_steps: 50\n17:07:51 |     log_keep_fields: all\n17:07:51 |     loglevel: info\n17:07:51 |     lr_scheduler: reduceonplateau\n17:07:51 |     lr_scheduler_decay: 0.5\n17:07:51 |     lr_scheduler_patience: 3\n17:07:51 |     max_train_steps: -1\n17:07:51 |     max_train_time: 7200.0\n17:07:51 |     memory_attention: sqrt\n17:07:51 |     metrics: default\n17:07:51 |     model: transformer/classifier\n17:07:51 |     model_file: /tmp/model4\n17:07:51 |     model_parallel: False\n17:07:51 |     momentum: 0\n17:07:51 |     multitask_weights: [1]\n17:07:51 |     mutators: None\n17:07:51 |     n_decoder_layers: -1\n17:07:51 |     n_encoder_layers: -1\n17:07:51 |     n_heads: 12\n17:07:51 |     n_layers: 12\n17:07:51 |     n_positions: 1024\n17:07:51 |     n_segments: 2\n17:07:51 |     nesterov: True\n17:07:51 |     no_cuda: False\n17:07:51 |     normalize_sent_emb: False\n17:07:51 |     num_epochs: -1\n17:07:51 |     num_workers: 0\n17:07:51 |     nus: (0.7,)\n17:07:51 |     optimizer: adamax\n17:07:51 |     output_scaling: 0.06\n17:07:51 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n17:07:51 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:07:51 |     person_tokens: False\n17:07:51 |     print_scores: False\n17:07:51 |     rank_candidates: False\n17:07:51 |     rank_top_k: -1\n17:07:51 |     reduction_type: mean\n17:07:51 |     ref_class: None\n17:07:51 |     relu_dropout: 0.0\n17:07:51 |     repeat_blocking_heuristic: True\n17:07:51 |     return_cand_scores: False\n17:07:51 |     save_after_valid: True\n17:07:51 |     save_every_n_secs: -1\n17:07:51 |     save_format: conversations\n17:07:51 |     share_encoders: False\n17:07:51 |     share_word_embeddings: False\n17:07:51 |     short_final_eval: False\n17:07:51 |     special_tok_lst: None\n17:07:51 |     split_lines: False\n17:07:51 |     starttime: Dec03_17-07\n17:07:51 |     task: fromfile:parlaiformat\n17:07:51 |     tensorboard_log: False\n17:07:51 |     tensorboard_logdir: None\n17:07:51 |     text_truncate: 360\n17:07:51 |     threshold: 0.5\n17:07:51 |     topk: 5\n17:07:51 |     train_predict: False\n17:07:51 |     truncate: 1024\n17:07:51 |     update_classifier_head_only: False\n17:07:51 |     update_freq: 1\n17:07:51 |     use_memories: False\n17:07:51 |     use_reply: none\n17:07:51 |     validation_cutoff: 1.0\n17:07:51 |     validation_every_n_epochs: -1\n17:07:51 |     validation_every_n_secs: 20.0\n17:07:51 |     validation_every_n_steps: -1\n17:07:51 |     validation_max_exs: -1\n17:07:51 |     validation_metric: accuracy\n17:07:51 |     validation_metric_mode: max\n17:07:51 |     validation_patience: 30\n17:07:51 |     validation_share_agent: False\n17:07:51 |     variant: xlm\n17:07:51 |     verbose: False\n17:07:51 |     wandb_entity: None\n17:07:51 |     wandb_log: False\n17:07:51 |     wandb_name: None\n17:07:51 |     wandb_project: None\n17:07:51 |     warmup_rate: 0.0001\n17:07:51 |     warmup_updates: 1000\n17:07:51 |     weight_decay: None\n17:07:51 |     world_logs: \n17:07:51 |     wrap_memory_encoder: False\n17:07:51 | creating task(s): fromfile:parlaiformat\n17:07:51 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt\n17:07:51 | training...\n17:08:01 | time:10s total_exs:380 total_steps:19 epochs:15.83\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4289 4.289e-10              .09959                 .2105   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                    .06522            .5819              .4675   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7704 11.98     1 279.6 527.6       0          0 37.73  380   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4289             32768  2.745    .1189 5.968 .7067 9.549e-07 119.4 225.2   \n    ltrunc  ltrunclen  total_train_updates  tpb   tps   ups  weighted_f1  \n         0          0                   19  399 752.8 1.891        .3484\n\n17:08:11 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6842 6.842e-10               .6040                 .8394   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4716            .7374              .6218   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9059 12.03     1 280.6  1087       0          0 77.51  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6842             32768  2.731    .1189 6.021 .6680 2.855e-06 120.4 466.7   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                   57  401 1554 3.884        .6693\n\n17:08:11 | creating task(s): fromfile:parlaiformat\n17:08:11 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:08:11 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt\n17:08:11 | running eval: valid\n17:08:12 | eval completed in 0.20s\n17:08:12 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1903       0          0 135.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5999 2.855e-06    72 813.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2716            1\n\u001b[0m\n17:08:12 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:08:12 | saving best valid model: /tmp/model4\n17:08:12 | Saving dictionary to /tmp/model4.dict\n17:08:16 | task solved! stopping.\n17:08:16 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:08:16 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:08:16 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:08:16 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:08:16 | Using CUDA\n17:08:16 | loading dictionary from /tmp/model4.dict\n17:08:16 | num words = 54944\n17:08:20 | Loading existing model parameters from /tmp/model4\n17:08:22 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:08:23 | creating task(s): fromfile:parlaiformat\n17:08:23 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:08:23 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt\n17:08:23 | running eval: valid\n17:08:24 | eval completed in 0.21s\n17:08:24 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1853       0          0 131.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5999 2.855e-06    72 791.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2645            1\n\u001b[0m\n17:08:24 | creating task(s): fromfile:parlaiformat\n17:08:24 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:08:24 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt\n17:08:24 | running eval: test\n17:08:24 | eval completed in 0.19s\n17:08:24 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1906       0          0 135.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5999 2.855e-06    72 814.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2720            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:08:25.982416Z","iopub.execute_input":"2022-12-03T17:08:25.982860Z","iopub.status.idle":"2022-12-03T17:08:53.451757Z","shell.execute_reply.started":"2022-12-03T17:08:25.982819Z","shell.execute_reply":"2022-12-03T17:08:53.450560Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"17:08:33 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt)\u001b[0m\n17:08:33 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:08:33 | Using CUDA\n17:08:33 | loading dictionary from /tmp/model4.dict\n17:08:33 | num words = 54944\n17:08:37 | Loading existing model parameters from /tmp/model4\n17:08:44 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:08:46 | Opt:\n17:08:46 |     activation: gelu\n17:08:46 |     adafactor_eps: '[1e-30, 0.001]'\n17:08:46 |     adam_eps: 1e-08\n17:08:46 |     add_p1_after_newln: False\n17:08:46 |     aggregate_micro: False\n17:08:46 |     allow_missing_init_opts: False\n17:08:46 |     area_under_curve_class: None\n17:08:46 |     area_under_curve_digits: -1\n17:08:46 |     attention_dropout: 0.1\n17:08:46 |     batchsize: 40\n17:08:46 |     betas: '[0.9, 0.999]'\n17:08:46 |     bpe_add_prefix_space: None\n17:08:46 |     bpe_debug: False\n17:08:46 |     bpe_dropout: None\n17:08:46 |     bpe_merge: None\n17:08:46 |     bpe_vocab: None\n17:08:46 |     candidates: inline\n17:08:46 |     cap_num_predictions: 100\n17:08:46 |     checkpoint_activations: False\n17:08:46 |     class_weights: None\n17:08:46 |     classes: \"['__notok__', '__ok__']\"\n17:08:46 |     classes_from_file: None\n17:08:46 |     data_parallel: True\n17:08:46 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:08:46 |     datatype: train\n17:08:46 |     delimiter: '\\n'\n17:08:46 |     dict_class: parlai.core.dict:DictionaryAgent\n17:08:46 |     dict_endtoken: __start__\n17:08:46 |     dict_file: /tmp/model4.dict\n17:08:46 |     dict_include_test: False\n17:08:46 |     dict_include_valid: False\n17:08:46 |     dict_initpath: None\n17:08:46 |     dict_language: english\n17:08:46 |     dict_loaded: True\n17:08:46 |     dict_lower: True\n17:08:46 |     dict_max_ngram_size: -1\n17:08:46 |     dict_maxexs: -1\n17:08:46 |     dict_maxtokens: -1\n17:08:46 |     dict_minfreq: 0\n17:08:46 |     dict_nulltoken: __null__\n17:08:46 |     dict_starttoken: __start__\n17:08:46 |     dict_textfields: text,labels\n17:08:46 |     dict_tokenizer: bpe\n17:08:46 |     dict_unktoken: __unk__\n17:08:46 |     display_examples: False\n17:08:46 |     download_path: None\n17:08:46 |     dropout: 0.1\n17:08:46 |     dynamic_batching: None\n17:08:46 |     embedding_projection: random\n17:08:46 |     embedding_size: 768\n17:08:46 |     embedding_type: random\n17:08:46 |     embeddings_scale: False\n17:08:46 |     encode_candidate_vecs: True\n17:08:46 |     encode_candidate_vecs_batchsize: 256\n17:08:46 |     eval_batchsize: None\n17:08:46 |     eval_candidates: inline\n17:08:46 |     eval_dynamic_batching: None\n17:08:46 |     evaltask: None\n17:08:46 |     ffn_size: 3072\n17:08:46 |     final_extra_opt: \n17:08:46 |     fixed_candidate_vecs: reuse\n17:08:46 |     fixed_candidates_path: None\n17:08:46 |     force_fp16_tokens: True\n17:08:46 |     fp16: True\n17:08:46 |     fp16_impl: safe\n17:08:46 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-a.txt\n17:08:46 |     fromfile_datatype_extension: False\n17:08:46 |     gpu: -1\n17:08:46 |     gradient_clip: 0.1\n17:08:46 |     hide_labels: False\n17:08:46 |     history_add_global_end_token: None\n17:08:46 |     history_reversed: False\n17:08:46 |     history_size: 20\n17:08:46 |     ignore_bad_candidates: False\n17:08:46 |     ignore_labels: None\n17:08:46 |     image_cropsize: 224\n17:08:46 |     image_mode: raw\n17:08:46 |     image_size: 256\n17:08:46 |     inference: max\n17:08:46 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:08:46 |     init_opt: None\n17:08:46 |     interactive_candidates: fixed\n17:08:46 |     interactive_mode: False\n17:08:46 |     invsqrt_lr_decay_gamma: -1\n17:08:46 |     is_debug: False\n17:08:46 |     label_truncate: 72\n17:08:46 |     learn_embeddings: True\n17:08:46 |     learn_positional_embeddings: True\n17:08:46 |     learningrate: 5e-05\n17:08:46 |     load_from_pretrained_ranker: True\n17:08:46 |     log_every_n_secs: 10.0\n17:08:46 |     log_every_n_steps: 50\n17:08:46 |     log_keep_fields: all\n17:08:46 |     loglevel: info\n17:08:46 |     lr_scheduler: reduceonplateau\n17:08:46 |     lr_scheduler_decay: 0.5\n17:08:46 |     lr_scheduler_patience: 3\n17:08:46 |     max_train_steps: -1\n17:08:46 |     max_train_time: 7200.0\n17:08:46 |     memory_attention: sqrt\n17:08:46 |     metrics: default\n17:08:46 |     model: transformer/classifier\n17:08:46 |     model_file: /tmp/model4\n17:08:46 |     model_parallel: False\n17:08:46 |     momentum: 0\n17:08:46 |     multitask_weights: [1]\n17:08:46 |     mutators: None\n17:08:46 |     n_decoder_layers: -1\n17:08:46 |     n_encoder_layers: -1\n17:08:46 |     n_heads: 12\n17:08:46 |     n_layers: 12\n17:08:46 |     n_positions: 1024\n17:08:46 |     n_segments: 2\n17:08:46 |     nesterov: True\n17:08:46 |     no_cuda: False\n17:08:46 |     normalize_sent_emb: False\n17:08:46 |     num_epochs: -1\n17:08:46 |     num_examples: -1\n17:08:46 |     num_workers: 0\n17:08:46 |     nus: [0.7]\n17:08:46 |     optimizer: adamax\n17:08:46 |     output_scaling: 0.06\n17:08:46 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n17:08:46 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:08:46 |     person_tokens: False\n17:08:46 |     print_scores: False\n17:08:46 |     rank_candidates: False\n17:08:46 |     rank_top_k: -1\n17:08:46 |     reduction_type: mean\n17:08:46 |     ref_class: None\n17:08:46 |     relu_dropout: 0.0\n17:08:46 |     repeat_blocking_heuristic: True\n17:08:46 |     report_filename: \n17:08:46 |     return_cand_scores: False\n17:08:46 |     save_after_valid: True\n17:08:46 |     save_every_n_secs: -1\n17:08:46 |     save_format: conversations\n17:08:46 |     share_encoders: False\n17:08:46 |     share_word_embeddings: False\n17:08:46 |     short_final_eval: False\n17:08:46 |     special_tok_lst: None\n17:08:46 |     split_lines: False\n17:08:46 |     starttime: Dec03_17-07\n17:08:46 |     task: fromfile:parlaiformat\n17:08:46 |     tensorboard_log: False\n17:08:46 |     tensorboard_logdir: None\n17:08:46 |     text_truncate: 360\n17:08:46 |     threshold: 0.5\n17:08:46 |     topk: 5\n17:08:46 |     train_predict: False\n17:08:46 |     truncate: 1024\n17:08:46 |     update_classifier_head_only: False\n17:08:46 |     update_freq: 1\n17:08:46 |     use_memories: False\n17:08:46 |     use_reply: none\n17:08:46 |     validation_cutoff: 1.0\n17:08:46 |     validation_every_n_epochs: -1\n17:08:46 |     validation_every_n_secs: 20.0\n17:08:46 |     validation_every_n_steps: -1\n17:08:46 |     validation_max_exs: -1\n17:08:46 |     validation_metric: accuracy\n17:08:46 |     validation_metric_mode: max\n17:08:46 |     validation_patience: 30\n17:08:46 |     validation_share_agent: False\n17:08:46 |     variant: xlm\n17:08:46 |     verbose: False\n17:08:46 |     wandb_entity: None\n17:08:46 |     wandb_log: False\n17:08:46 |     wandb_name: None\n17:08:46 |     wandb_project: None\n17:08:46 |     warmup_rate: 0.0001\n17:08:46 |     warmup_updates: 1000\n17:08:46 |     weight_decay: None\n17:08:46 |     world_logs: \n17:08:46 |     wrap_memory_encoder: False\n17:08:46 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:08:46 | creating task(s): fromfile:parlaiformat\n17:08:46 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:08:46 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-a.txt\n17:08:51 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3050 3.05e-10               .2147                 .2794   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1743            .3767              .3182   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4615 11.46 538.2 521.4       0          0 38.75  200 .3050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.09 .7326 2.855e-06 243.6   236       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 781.8 757.4        .2884\u001b[0m\n17:08:51 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3050 3.05e-10               .2147                 .2794   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1743            .3767              .3182   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4615 11.46 538.2 521.4       0          0 38.75  200 .3050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.09 .7326 2.855e-06 243.6   236       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 781.8 757.4        .2884\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:08:53.453837Z","iopub.execute_input":"2022-12-03T17:08:53.454182Z","iopub.status.idle":"2022-12-03T17:09:20.080186Z","shell.execute_reply.started":"2022-12-03T17:08:53.454152Z","shell.execute_reply":"2022-12-03T17:09:20.078990Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"17:09:00 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_valid.txt)\u001b[0m\n17:09:00 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:09:00 | Using CUDA\n17:09:00 | loading dictionary from /tmp/model4.dict\n17:09:00 | num words = 54944\n17:09:05 | Loading existing model parameters from /tmp/model4\n17:09:11 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:09:12 | Opt:\n17:09:12 |     activation: gelu\n17:09:12 |     adafactor_eps: '[1e-30, 0.001]'\n17:09:12 |     adam_eps: 1e-08\n17:09:12 |     add_p1_after_newln: False\n17:09:12 |     aggregate_micro: False\n17:09:12 |     allow_missing_init_opts: False\n17:09:12 |     area_under_curve_class: None\n17:09:12 |     area_under_curve_digits: -1\n17:09:12 |     attention_dropout: 0.1\n17:09:12 |     batchsize: 40\n17:09:12 |     betas: '[0.9, 0.999]'\n17:09:12 |     bpe_add_prefix_space: None\n17:09:12 |     bpe_debug: False\n17:09:12 |     bpe_dropout: None\n17:09:12 |     bpe_merge: None\n17:09:12 |     bpe_vocab: None\n17:09:12 |     candidates: inline\n17:09:12 |     cap_num_predictions: 100\n17:09:12 |     checkpoint_activations: False\n17:09:12 |     class_weights: None\n17:09:12 |     classes: \"['__notok__', '__ok__']\"\n17:09:12 |     classes_from_file: None\n17:09:12 |     data_parallel: True\n17:09:12 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:09:12 |     datatype: train\n17:09:12 |     delimiter: '\\n'\n17:09:12 |     dict_class: parlai.core.dict:DictionaryAgent\n17:09:12 |     dict_endtoken: __start__\n17:09:12 |     dict_file: /tmp/model4.dict\n17:09:12 |     dict_include_test: False\n17:09:12 |     dict_include_valid: False\n17:09:12 |     dict_initpath: None\n17:09:12 |     dict_language: english\n17:09:12 |     dict_loaded: True\n17:09:12 |     dict_lower: True\n17:09:12 |     dict_max_ngram_size: -1\n17:09:12 |     dict_maxexs: -1\n17:09:12 |     dict_maxtokens: -1\n17:09:12 |     dict_minfreq: 0\n17:09:12 |     dict_nulltoken: __null__\n17:09:12 |     dict_starttoken: __start__\n17:09:12 |     dict_textfields: text,labels\n17:09:12 |     dict_tokenizer: bpe\n17:09:12 |     dict_unktoken: __unk__\n17:09:12 |     display_examples: False\n17:09:12 |     download_path: None\n17:09:12 |     dropout: 0.1\n17:09:12 |     dynamic_batching: None\n17:09:12 |     embedding_projection: random\n17:09:12 |     embedding_size: 768\n17:09:12 |     embedding_type: random\n17:09:12 |     embeddings_scale: False\n17:09:12 |     encode_candidate_vecs: True\n17:09:12 |     encode_candidate_vecs_batchsize: 256\n17:09:12 |     eval_batchsize: None\n17:09:12 |     eval_candidates: inline\n17:09:12 |     eval_dynamic_batching: None\n17:09:12 |     evaltask: None\n17:09:12 |     ffn_size: 3072\n17:09:12 |     final_extra_opt: \n17:09:12 |     fixed_candidate_vecs: reuse\n17:09:12 |     fixed_candidates_path: None\n17:09:12 |     force_fp16_tokens: True\n17:09:12 |     fp16: True\n17:09:12 |     fp16_impl: safe\n17:09:12 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-b.txt\n17:09:12 |     fromfile_datatype_extension: False\n17:09:12 |     gpu: -1\n17:09:12 |     gradient_clip: 0.1\n17:09:12 |     hide_labels: False\n17:09:12 |     history_add_global_end_token: None\n17:09:12 |     history_reversed: False\n17:09:12 |     history_size: 20\n17:09:12 |     ignore_bad_candidates: False\n17:09:12 |     ignore_labels: None\n17:09:12 |     image_cropsize: 224\n17:09:12 |     image_mode: raw\n17:09:12 |     image_size: 256\n17:09:12 |     inference: max\n17:09:12 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:09:12 |     init_opt: None\n17:09:12 |     interactive_candidates: fixed\n17:09:12 |     interactive_mode: False\n17:09:12 |     invsqrt_lr_decay_gamma: -1\n17:09:12 |     is_debug: False\n17:09:12 |     label_truncate: 72\n17:09:12 |     learn_embeddings: True\n17:09:12 |     learn_positional_embeddings: True\n17:09:12 |     learningrate: 5e-05\n17:09:12 |     load_from_pretrained_ranker: True\n17:09:12 |     log_every_n_secs: 10.0\n17:09:12 |     log_every_n_steps: 50\n17:09:12 |     log_keep_fields: all\n17:09:12 |     loglevel: info\n17:09:12 |     lr_scheduler: reduceonplateau\n17:09:12 |     lr_scheduler_decay: 0.5\n17:09:12 |     lr_scheduler_patience: 3\n17:09:12 |     max_train_steps: -1\n17:09:12 |     max_train_time: 7200.0\n17:09:12 |     memory_attention: sqrt\n17:09:12 |     metrics: default\n17:09:12 |     model: transformer/classifier\n17:09:12 |     model_file: /tmp/model4\n17:09:12 |     model_parallel: False\n17:09:12 |     momentum: 0\n17:09:12 |     multitask_weights: [1]\n17:09:12 |     mutators: None\n17:09:12 |     n_decoder_layers: -1\n17:09:12 |     n_encoder_layers: -1\n17:09:12 |     n_heads: 12\n17:09:12 |     n_layers: 12\n17:09:12 |     n_positions: 1024\n17:09:12 |     n_segments: 2\n17:09:12 |     nesterov: True\n17:09:12 |     no_cuda: False\n17:09:12 |     normalize_sent_emb: False\n17:09:12 |     num_epochs: -1\n17:09:12 |     num_examples: -1\n17:09:12 |     num_workers: 0\n17:09:12 |     nus: [0.7]\n17:09:12 |     optimizer: adamax\n17:09:12 |     output_scaling: 0.06\n17:09:12 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n17:09:12 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:09:12 |     person_tokens: False\n17:09:12 |     print_scores: False\n17:09:12 |     rank_candidates: False\n17:09:12 |     rank_top_k: -1\n17:09:12 |     reduction_type: mean\n17:09:12 |     ref_class: None\n17:09:12 |     relu_dropout: 0.0\n17:09:12 |     repeat_blocking_heuristic: True\n17:09:12 |     report_filename: \n17:09:12 |     return_cand_scores: False\n17:09:12 |     save_after_valid: True\n17:09:12 |     save_every_n_secs: -1\n17:09:12 |     save_format: conversations\n17:09:12 |     share_encoders: False\n17:09:12 |     share_word_embeddings: False\n17:09:12 |     short_final_eval: False\n17:09:12 |     special_tok_lst: None\n17:09:12 |     split_lines: False\n17:09:12 |     starttime: Dec03_17-07\n17:09:12 |     task: fromfile:parlaiformat\n17:09:12 |     tensorboard_log: False\n17:09:12 |     tensorboard_logdir: None\n17:09:12 |     text_truncate: 360\n17:09:12 |     threshold: 0.5\n17:09:12 |     topk: 5\n17:09:12 |     train_predict: False\n17:09:12 |     truncate: 1024\n17:09:12 |     update_classifier_head_only: False\n17:09:12 |     update_freq: 1\n17:09:12 |     use_memories: False\n17:09:12 |     use_reply: none\n17:09:12 |     validation_cutoff: 1.0\n17:09:12 |     validation_every_n_epochs: -1\n17:09:12 |     validation_every_n_secs: 20.0\n17:09:12 |     validation_every_n_steps: -1\n17:09:12 |     validation_max_exs: -1\n17:09:12 |     validation_metric: accuracy\n17:09:12 |     validation_metric_mode: max\n17:09:12 |     validation_patience: 30\n17:09:12 |     validation_share_agent: False\n17:09:12 |     variant: xlm\n17:09:12 |     verbose: False\n17:09:12 |     wandb_entity: None\n17:09:12 |     wandb_log: False\n17:09:12 |     wandb_name: None\n17:09:12 |     wandb_project: None\n17:09:12 |     warmup_rate: 0.0001\n17:09:12 |     warmup_updates: 1000\n17:09:12 |     weight_decay: None\n17:09:12 |     world_logs: \n17:09:12 |     wrap_memory_encoder: False\n17:09:12 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:09:12 | creating task(s): fromfile:parlaiformat\n17:09:12 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:09:12 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run4/data_train-b.txt\n17:09:18 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6950 6.95e-10               .6164                 .7206   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5385            .7469              .6818   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8257 11.46 538.2 499.1       0          0  37.1  200 .6950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.91 .6614 2.855e-06 236.4 219.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 774.6 718.4        .6875\u001b[0m\n17:09:18 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6950 6.95e-10               .6164                 .7206   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5385            .7469              .6818   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8257 11.46 538.2 499.1       0          0  37.1  200 .6950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.91 .6614 2.855e-06 236.4 219.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 774.6 718.4        .6875\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:09:20.082129Z","iopub.execute_input":"2022-12-03T17:09:20.082600Z","iopub.status.idle":"2022-12-03T17:09:21.234831Z","shell.execute_reply.started":"2022-12-03T17:09:20.082526Z","shell.execute_reply":"2022-12-03T17:09:21.233461Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:09:21.236704Z","iopub.execute_input":"2022-12-03T17:09:21.237180Z","iopub.status.idle":"2022-12-03T17:10:41.317925Z","shell.execute_reply.started":"2022-12-03T17:09:21.237137Z","shell.execute_reply":"2022-12-03T17:10:41.316745Z"},"scrolled":true,"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"17:09:28 | building dictionary first...\n17:09:28 | No model with opt yet at: /tmp/model5(.opt)\n17:09:28 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:09:28 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:09:28 | Using CUDA\n17:09:28 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:09:28 | num words = 54944\n17:09:33 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:09:39 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:09:39 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:09:39 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:09:39 | Opt:\n17:09:39 |     activation: gelu\n17:09:39 |     adafactor_eps: '(1e-30, 0.001)'\n17:09:39 |     adam_eps: 1e-08\n17:09:39 |     add_p1_after_newln: False\n17:09:39 |     aggregate_micro: False\n17:09:39 |     allow_missing_init_opts: False\n17:09:39 |     attention_dropout: 0.1\n17:09:39 |     batchsize: 20\n17:09:39 |     betas: '(0.9, 0.999)'\n17:09:39 |     bpe_add_prefix_space: None\n17:09:39 |     bpe_debug: False\n17:09:39 |     bpe_dropout: None\n17:09:39 |     bpe_merge: None\n17:09:39 |     bpe_vocab: None\n17:09:39 |     candidates: inline\n17:09:39 |     cap_num_predictions: 100\n17:09:39 |     checkpoint_activations: False\n17:09:39 |     class_weights: None\n17:09:39 |     classes: \"['__notok__', '__ok__']\"\n17:09:39 |     classes_from_file: None\n17:09:39 |     data_parallel: True\n17:09:39 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:09:39 |     datatype: train\n17:09:39 |     delimiter: '\\n'\n17:09:39 |     dict_class: parlai.core.dict:DictionaryAgent\n17:09:39 |     dict_endtoken: __start__\n17:09:39 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:09:39 |     dict_include_test: False\n17:09:39 |     dict_include_valid: False\n17:09:39 |     dict_initpath: None\n17:09:39 |     dict_language: english\n17:09:39 |     dict_loaded: True\n17:09:39 |     dict_lower: True\n17:09:39 |     dict_max_ngram_size: -1\n17:09:39 |     dict_maxexs: -1\n17:09:39 |     dict_maxtokens: -1\n17:09:39 |     dict_minfreq: 0\n17:09:39 |     dict_nulltoken: __null__\n17:09:39 |     dict_starttoken: __start__\n17:09:39 |     dict_textfields: text,labels\n17:09:39 |     dict_tokenizer: bpe\n17:09:39 |     dict_unktoken: __unk__\n17:09:39 |     display_examples: False\n17:09:39 |     download_path: None\n17:09:39 |     dropout: 0.1\n17:09:39 |     dynamic_batching: None\n17:09:39 |     embedding_projection: random\n17:09:39 |     embedding_size: 768\n17:09:39 |     embedding_type: random\n17:09:39 |     embeddings_scale: False\n17:09:39 |     encode_candidate_vecs: True\n17:09:39 |     encode_candidate_vecs_batchsize: 256\n17:09:39 |     eval_batchsize: None\n17:09:39 |     eval_candidates: inline\n17:09:39 |     eval_dynamic_batching: None\n17:09:39 |     evaltask: None\n17:09:39 |     ffn_size: 3072\n17:09:39 |     final_extra_opt: \n17:09:39 |     fixed_candidate_vecs: reuse\n17:09:39 |     fixed_candidates_path: None\n17:09:39 |     force_fp16_tokens: False\n17:09:39 |     fp16: True\n17:09:39 |     fp16_impl: safe\n17:09:39 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt\n17:09:39 |     fromfile_datatype_extension: False\n17:09:39 |     gpu: -1\n17:09:39 |     gradient_clip: 0.1\n17:09:39 |     hide_labels: False\n17:09:39 |     history_add_global_end_token: None\n17:09:39 |     history_reversed: False\n17:09:39 |     history_size: 20\n17:09:39 |     ignore_bad_candidates: False\n17:09:39 |     ignore_labels: None\n17:09:39 |     image_cropsize: 224\n17:09:39 |     image_mode: raw\n17:09:39 |     image_size: 256\n17:09:39 |     inference: max\n17:09:39 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:09:39 |     init_opt: None\n17:09:39 |     interactive_candidates: fixed\n17:09:39 |     interactive_mode: False\n17:09:39 |     invsqrt_lr_decay_gamma: -1\n17:09:39 |     is_debug: False\n17:09:39 |     label_truncate: 72\n17:09:39 |     learn_embeddings: True\n17:09:39 |     learn_positional_embeddings: True\n17:09:39 |     learningrate: 5e-05\n17:09:39 |     load_from_checkpoint: False\n17:09:39 |     load_from_pretrained_ranker: True\n17:09:39 |     log_every_n_secs: 10.0\n17:09:39 |     log_every_n_steps: 50\n17:09:39 |     log_keep_fields: all\n17:09:39 |     loglevel: info\n17:09:39 |     lr_scheduler: reduceonplateau\n17:09:39 |     lr_scheduler_decay: 0.5\n17:09:39 |     lr_scheduler_patience: 3\n17:09:39 |     max_train_steps: -1\n17:09:39 |     max_train_time: 7200.0\n17:09:39 |     memory_attention: sqrt\n17:09:39 |     metrics: default\n17:09:39 |     model: transformer/classifier\n17:09:39 |     model_file: /tmp/model5\n17:09:39 |     model_parallel: False\n17:09:39 |     momentum: 0\n17:09:39 |     multitask_weights: [1]\n17:09:39 |     mutators: None\n17:09:39 |     n_decoder_layers: -1\n17:09:39 |     n_encoder_layers: -1\n17:09:39 |     n_heads: 12\n17:09:39 |     n_layers: 12\n17:09:39 |     n_positions: 1024\n17:09:39 |     n_segments: 2\n17:09:39 |     nesterov: True\n17:09:39 |     no_cuda: False\n17:09:39 |     normalize_sent_emb: False\n17:09:39 |     num_epochs: -1\n17:09:39 |     num_workers: 0\n17:09:39 |     nus: (0.7,)\n17:09:39 |     optimizer: adamax\n17:09:39 |     output_scaling: 0.06\n17:09:39 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n17:09:39 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:09:39 |     person_tokens: False\n17:09:39 |     print_scores: False\n17:09:39 |     rank_candidates: False\n17:09:39 |     rank_top_k: -1\n17:09:39 |     reduction_type: mean\n17:09:39 |     ref_class: None\n17:09:39 |     relu_dropout: 0.0\n17:09:39 |     repeat_blocking_heuristic: True\n17:09:39 |     return_cand_scores: False\n17:09:39 |     save_after_valid: True\n17:09:39 |     save_every_n_secs: -1\n17:09:39 |     save_format: conversations\n17:09:39 |     share_encoders: False\n17:09:39 |     share_word_embeddings: False\n17:09:39 |     short_final_eval: False\n17:09:39 |     special_tok_lst: None\n17:09:39 |     split_lines: False\n17:09:39 |     starttime: Dec03_17-09\n17:09:39 |     task: fromfile:parlaiformat\n17:09:39 |     tensorboard_log: False\n17:09:39 |     tensorboard_logdir: None\n17:09:39 |     text_truncate: 360\n17:09:39 |     threshold: 0.5\n17:09:39 |     topk: 5\n17:09:39 |     train_predict: False\n17:09:39 |     truncate: 1024\n17:09:39 |     update_classifier_head_only: False\n17:09:39 |     update_freq: 1\n17:09:39 |     use_memories: False\n17:09:39 |     use_reply: none\n17:09:39 |     validation_cutoff: 1.0\n17:09:39 |     validation_every_n_epochs: -1\n17:09:39 |     validation_every_n_secs: 20.0\n17:09:39 |     validation_every_n_steps: -1\n17:09:39 |     validation_max_exs: -1\n17:09:39 |     validation_metric: accuracy\n17:09:39 |     validation_metric_mode: max\n17:09:39 |     validation_patience: 30\n17:09:39 |     validation_share_agent: False\n17:09:39 |     variant: xlm\n17:09:39 |     verbose: False\n17:09:39 |     wandb_entity: None\n17:09:39 |     wandb_log: False\n17:09:39 |     wandb_name: None\n17:09:39 |     wandb_project: None\n17:09:39 |     warmup_rate: 0.0001\n17:09:39 |     warmup_updates: 1000\n17:09:39 |     weight_decay: None\n17:09:39 |     world_logs: \n17:09:39 |     wrap_memory_encoder: False\n17:09:39 | creating task(s): fromfile:parlaiformat\n17:09:39 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt\n17:09:39 | training...\n17:09:50 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5119 5.119e-10               .5629                 .5077   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6316            .4474              .5188   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .3934  10.9     1 257.9   533       0          0 41.33  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5119             32768  2.807    .1206 5.995 .7082 1.055e-06 119.9 247.8   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 377.8 780.8 2.071        .5049\n\n17:09:59 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7230 7.23e-10               .7557                 .6788   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8522            .6802              .7985   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5924 10.48     1 249.6 958.8       0          0 76.82  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7230             32768  2.742    .1207 6.005 .6588 2.905e-06 120.1 461.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 369.7 1420 3.849        .7181\n\n17:09:59 | creating task(s): fromfile:parlaiformat\n17:09:59 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:09:59 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt\n17:09:59 | running eval: valid\n17:09:59 | eval completed in 0.20s\n17:09:59 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9565                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9167            .9600              .9231   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                      1 10.67   152  1644       0          0 129.7   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5944 2.905e-06    72 778.8       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  224 2423        .9583\n\u001b[0m\n17:09:59 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n17:09:59 | saving best valid model: /tmp/model5\n17:09:59 | Saving dictionary to /tmp/model5.dict\n17:10:03 | saving model checkpoint: /tmp/model5.checkpoint\n17:10:03 | Saving dictionary to /tmp/model5.checkpoint.dict\n17:10:20 | time:41s total_exs:1880 total_steps:94 epochs:78.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9931 9.931e-10               .9931                 .9917   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9945            .9930              .9944   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9916 10.63     1 252.6 909.4       0          0 72.01  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9931             32768  2.769    .1207 6.003 .5234 4.705e-06 120.1 432.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   94 372.6 1342 3.609        .9931\n\n17:10:23 | time:44s total_exs:2080 total_steps:104 epochs:86.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 10.11     1 242.1 892.8       0          0 73.75  200   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  3.085    .1207  5.99 .3857 5.204e-06 119.8 441.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  104 361.9 1335 3.718            1\n\n17:10:23 | running eval: valid\n17:10:23 | eval completed in 0.20s\n17:10:23 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1674       0          0 132.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3188 5.204e-06    72 792.7       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    104  224 2466            1\n\u001b[0m\n17:10:23 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n17:10:23 | saving best valid model: /tmp/model5\n17:10:28 | task solved! stopping.\n17:10:28 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:10:28 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:10:28 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:10:28 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:10:28 | Using CUDA\n17:10:28 | loading dictionary from /tmp/model5.dict\n17:10:28 | num words = 54944\n17:10:33 | Loading existing model parameters from /tmp/model5\n17:10:37 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:10:38 | creating task(s): fromfile:parlaiformat\n17:10:38 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:10:38 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt\n17:10:38 | running eval: valid\n17:10:39 | eval completed in 0.22s\n17:10:39 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1575       0          0 124.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3188 5.204e-06    72 746.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    104  224 2322            1\n\u001b[0m\n17:10:39 | creating task(s): fromfile:parlaiformat\n17:10:39 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:10:39 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt\n17:10:39 | running eval: test\n17:10:39 | eval completed in 0.20s\n17:10:39 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1643       0          0 129.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3188 5.204e-06    72 778.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    104  224 2422            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:10:41.320824Z","iopub.execute_input":"2022-12-03T17:10:41.321237Z","iopub.status.idle":"2022-12-03T17:11:08.160624Z","shell.execute_reply.started":"2022-12-03T17:10:41.321195Z","shell.execute_reply":"2022-12-03T17:11:08.159346Z"},"scrolled":true,"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"17:10:48 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt)\u001b[0m\n17:10:48 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:10:48 | Using CUDA\n17:10:48 | loading dictionary from /tmp/model5.dict\n17:10:48 | num words = 54944\n17:10:52 | Loading existing model parameters from /tmp/model5\n17:10:59 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:11:00 | Opt:\n17:11:00 |     activation: gelu\n17:11:00 |     adafactor_eps: '[1e-30, 0.001]'\n17:11:00 |     adam_eps: 1e-08\n17:11:00 |     add_p1_after_newln: False\n17:11:00 |     aggregate_micro: False\n17:11:00 |     allow_missing_init_opts: False\n17:11:00 |     area_under_curve_class: None\n17:11:00 |     area_under_curve_digits: -1\n17:11:00 |     attention_dropout: 0.1\n17:11:00 |     batchsize: 40\n17:11:00 |     betas: '[0.9, 0.999]'\n17:11:00 |     bpe_add_prefix_space: None\n17:11:00 |     bpe_debug: False\n17:11:00 |     bpe_dropout: None\n17:11:00 |     bpe_merge: None\n17:11:00 |     bpe_vocab: None\n17:11:00 |     candidates: inline\n17:11:00 |     cap_num_predictions: 100\n17:11:00 |     checkpoint_activations: False\n17:11:00 |     class_weights: None\n17:11:00 |     classes: \"['__notok__', '__ok__']\"\n17:11:00 |     classes_from_file: None\n17:11:00 |     data_parallel: True\n17:11:00 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:11:00 |     datatype: train\n17:11:00 |     delimiter: '\\n'\n17:11:00 |     dict_class: parlai.core.dict:DictionaryAgent\n17:11:00 |     dict_endtoken: __start__\n17:11:00 |     dict_file: /tmp/model5.dict\n17:11:00 |     dict_include_test: False\n17:11:00 |     dict_include_valid: False\n17:11:00 |     dict_initpath: None\n17:11:00 |     dict_language: english\n17:11:00 |     dict_loaded: True\n17:11:00 |     dict_lower: True\n17:11:00 |     dict_max_ngram_size: -1\n17:11:00 |     dict_maxexs: -1\n17:11:00 |     dict_maxtokens: -1\n17:11:00 |     dict_minfreq: 0\n17:11:00 |     dict_nulltoken: __null__\n17:11:00 |     dict_starttoken: __start__\n17:11:00 |     dict_textfields: text,labels\n17:11:00 |     dict_tokenizer: bpe\n17:11:00 |     dict_unktoken: __unk__\n17:11:00 |     display_examples: False\n17:11:00 |     download_path: None\n17:11:00 |     dropout: 0.1\n17:11:00 |     dynamic_batching: None\n17:11:00 |     embedding_projection: random\n17:11:00 |     embedding_size: 768\n17:11:00 |     embedding_type: random\n17:11:00 |     embeddings_scale: False\n17:11:00 |     encode_candidate_vecs: True\n17:11:00 |     encode_candidate_vecs_batchsize: 256\n17:11:00 |     eval_batchsize: None\n17:11:00 |     eval_candidates: inline\n17:11:00 |     eval_dynamic_batching: None\n17:11:00 |     evaltask: None\n17:11:00 |     ffn_size: 3072\n17:11:00 |     final_extra_opt: \n17:11:00 |     fixed_candidate_vecs: reuse\n17:11:00 |     fixed_candidates_path: None\n17:11:00 |     force_fp16_tokens: True\n17:11:00 |     fp16: True\n17:11:00 |     fp16_impl: safe\n17:11:00 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-a.txt\n17:11:00 |     fromfile_datatype_extension: False\n17:11:00 |     gpu: -1\n17:11:00 |     gradient_clip: 0.1\n17:11:00 |     hide_labels: False\n17:11:00 |     history_add_global_end_token: None\n17:11:00 |     history_reversed: False\n17:11:00 |     history_size: 20\n17:11:00 |     ignore_bad_candidates: False\n17:11:00 |     ignore_labels: None\n17:11:00 |     image_cropsize: 224\n17:11:00 |     image_mode: raw\n17:11:00 |     image_size: 256\n17:11:00 |     inference: max\n17:11:00 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:11:00 |     init_opt: None\n17:11:00 |     interactive_candidates: fixed\n17:11:00 |     interactive_mode: False\n17:11:00 |     invsqrt_lr_decay_gamma: -1\n17:11:00 |     is_debug: False\n17:11:00 |     label_truncate: 72\n17:11:00 |     learn_embeddings: True\n17:11:00 |     learn_positional_embeddings: True\n17:11:00 |     learningrate: 5e-05\n17:11:00 |     load_from_pretrained_ranker: True\n17:11:00 |     log_every_n_secs: 10.0\n17:11:00 |     log_every_n_steps: 50\n17:11:00 |     log_keep_fields: all\n17:11:00 |     loglevel: info\n17:11:00 |     lr_scheduler: reduceonplateau\n17:11:00 |     lr_scheduler_decay: 0.5\n17:11:00 |     lr_scheduler_patience: 3\n17:11:00 |     max_train_steps: -1\n17:11:00 |     max_train_time: 7200.0\n17:11:00 |     memory_attention: sqrt\n17:11:00 |     metrics: default\n17:11:00 |     model: transformer/classifier\n17:11:00 |     model_file: /tmp/model5\n17:11:00 |     model_parallel: False\n17:11:00 |     momentum: 0\n17:11:00 |     multitask_weights: [1]\n17:11:00 |     mutators: None\n17:11:00 |     n_decoder_layers: -1\n17:11:00 |     n_encoder_layers: -1\n17:11:00 |     n_heads: 12\n17:11:00 |     n_layers: 12\n17:11:00 |     n_positions: 1024\n17:11:00 |     n_segments: 2\n17:11:00 |     nesterov: True\n17:11:00 |     no_cuda: False\n17:11:00 |     normalize_sent_emb: False\n17:11:00 |     num_epochs: -1\n17:11:00 |     num_examples: -1\n17:11:00 |     num_workers: 0\n17:11:00 |     nus: [0.7]\n17:11:00 |     optimizer: adamax\n17:11:00 |     output_scaling: 0.06\n17:11:00 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n17:11:00 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:11:00 |     person_tokens: False\n17:11:00 |     print_scores: False\n17:11:00 |     rank_candidates: False\n17:11:00 |     rank_top_k: -1\n17:11:00 |     reduction_type: mean\n17:11:00 |     ref_class: None\n17:11:00 |     relu_dropout: 0.0\n17:11:00 |     repeat_blocking_heuristic: True\n17:11:00 |     report_filename: \n17:11:00 |     return_cand_scores: False\n17:11:00 |     save_after_valid: True\n17:11:00 |     save_every_n_secs: -1\n17:11:00 |     save_format: conversations\n17:11:00 |     share_encoders: False\n17:11:00 |     share_word_embeddings: False\n17:11:00 |     short_final_eval: False\n17:11:00 |     special_tok_lst: None\n17:11:00 |     split_lines: False\n17:11:00 |     starttime: Dec03_17-09\n17:11:00 |     task: fromfile:parlaiformat\n17:11:00 |     tensorboard_log: False\n17:11:00 |     tensorboard_logdir: None\n17:11:00 |     text_truncate: 360\n17:11:00 |     threshold: 0.5\n17:11:00 |     topk: 5\n17:11:00 |     train_predict: False\n17:11:00 |     truncate: 1024\n17:11:00 |     update_classifier_head_only: False\n17:11:00 |     update_freq: 1\n17:11:00 |     use_memories: False\n17:11:00 |     use_reply: none\n17:11:00 |     validation_cutoff: 1.0\n17:11:00 |     validation_every_n_epochs: -1\n17:11:00 |     validation_every_n_secs: 20.0\n17:11:00 |     validation_every_n_steps: -1\n17:11:00 |     validation_max_exs: -1\n17:11:00 |     validation_metric: accuracy\n17:11:00 |     validation_metric_mode: max\n17:11:00 |     validation_patience: 30\n17:11:00 |     validation_share_agent: False\n17:11:00 |     variant: xlm\n17:11:00 |     verbose: False\n17:11:00 |     wandb_entity: None\n17:11:00 |     wandb_log: False\n17:11:00 |     wandb_name: None\n17:11:00 |     wandb_project: None\n17:11:00 |     warmup_rate: 0.0001\n17:11:00 |     warmup_updates: 1000\n17:11:00 |     weight_decay: None\n17:11:00 |     world_logs: \n17:11:00 |     wrap_memory_encoder: False\n17:11:00 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:11:00 | creating task(s): fromfile:parlaiformat\n17:11:00 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:11:00 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-a.txt\n17:11:06 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1750 1.75e-10               .1791                 .1748   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1837            .1709              .1753   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1667 11.79 551.8   515       0          0 37.33  200 .1750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.98 .8879 5.204e-06 239.2 223.2       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    104  791 738.3        .1749\u001b[0m\n17:11:06 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1750 1.75e-10               .1791                 .1748   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1837            .1709              .1753   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1667 11.79 551.8   515       0          0 37.33  200 .1750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.98 .8879 5.204e-06 239.2 223.2       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    104  791 738.3        .1749\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:11:08.166047Z","iopub.execute_input":"2022-12-03T17:11:08.166411Z","iopub.status.idle":"2022-12-03T17:11:34.254133Z","shell.execute_reply.started":"2022-12-03T17:11:08.166374Z","shell.execute_reply":"2022-12-03T17:11:34.252833Z"},"scrolled":true,"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"17:11:15 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_valid.txt)\u001b[0m\n17:11:15 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:11:15 | Using CUDA\n17:11:15 | loading dictionary from /tmp/model5.dict\n17:11:15 | num words = 54944\n17:11:19 | Loading existing model parameters from /tmp/model5\n17:11:25 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:11:26 | Opt:\n17:11:26 |     activation: gelu\n17:11:26 |     adafactor_eps: '[1e-30, 0.001]'\n17:11:26 |     adam_eps: 1e-08\n17:11:26 |     add_p1_after_newln: False\n17:11:26 |     aggregate_micro: False\n17:11:26 |     allow_missing_init_opts: False\n17:11:26 |     area_under_curve_class: None\n17:11:26 |     area_under_curve_digits: -1\n17:11:26 |     attention_dropout: 0.1\n17:11:26 |     batchsize: 40\n17:11:26 |     betas: '[0.9, 0.999]'\n17:11:26 |     bpe_add_prefix_space: None\n17:11:26 |     bpe_debug: False\n17:11:26 |     bpe_dropout: None\n17:11:26 |     bpe_merge: None\n17:11:26 |     bpe_vocab: None\n17:11:26 |     candidates: inline\n17:11:26 |     cap_num_predictions: 100\n17:11:26 |     checkpoint_activations: False\n17:11:26 |     class_weights: None\n17:11:26 |     classes: \"['__notok__', '__ok__']\"\n17:11:26 |     classes_from_file: None\n17:11:26 |     data_parallel: True\n17:11:26 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:11:26 |     datatype: train\n17:11:26 |     delimiter: '\\n'\n17:11:26 |     dict_class: parlai.core.dict:DictionaryAgent\n17:11:26 |     dict_endtoken: __start__\n17:11:26 |     dict_file: /tmp/model5.dict\n17:11:26 |     dict_include_test: False\n17:11:26 |     dict_include_valid: False\n17:11:26 |     dict_initpath: None\n17:11:26 |     dict_language: english\n17:11:26 |     dict_loaded: True\n17:11:26 |     dict_lower: True\n17:11:26 |     dict_max_ngram_size: -1\n17:11:26 |     dict_maxexs: -1\n17:11:26 |     dict_maxtokens: -1\n17:11:26 |     dict_minfreq: 0\n17:11:26 |     dict_nulltoken: __null__\n17:11:26 |     dict_starttoken: __start__\n17:11:26 |     dict_textfields: text,labels\n17:11:26 |     dict_tokenizer: bpe\n17:11:26 |     dict_unktoken: __unk__\n17:11:26 |     display_examples: False\n17:11:26 |     download_path: None\n17:11:26 |     dropout: 0.1\n17:11:26 |     dynamic_batching: None\n17:11:26 |     embedding_projection: random\n17:11:26 |     embedding_size: 768\n17:11:26 |     embedding_type: random\n17:11:26 |     embeddings_scale: False\n17:11:26 |     encode_candidate_vecs: True\n17:11:26 |     encode_candidate_vecs_batchsize: 256\n17:11:26 |     eval_batchsize: None\n17:11:26 |     eval_candidates: inline\n17:11:26 |     eval_dynamic_batching: None\n17:11:26 |     evaltask: None\n17:11:26 |     ffn_size: 3072\n17:11:26 |     final_extra_opt: \n17:11:26 |     fixed_candidate_vecs: reuse\n17:11:26 |     fixed_candidates_path: None\n17:11:26 |     force_fp16_tokens: True\n17:11:26 |     fp16: True\n17:11:26 |     fp16_impl: safe\n17:11:26 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-b.txt\n17:11:26 |     fromfile_datatype_extension: False\n17:11:26 |     gpu: -1\n17:11:26 |     gradient_clip: 0.1\n17:11:26 |     hide_labels: False\n17:11:26 |     history_add_global_end_token: None\n17:11:26 |     history_reversed: False\n17:11:26 |     history_size: 20\n17:11:26 |     ignore_bad_candidates: False\n17:11:26 |     ignore_labels: None\n17:11:26 |     image_cropsize: 224\n17:11:26 |     image_mode: raw\n17:11:26 |     image_size: 256\n17:11:26 |     inference: max\n17:11:26 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:11:26 |     init_opt: None\n17:11:26 |     interactive_candidates: fixed\n17:11:26 |     interactive_mode: False\n17:11:26 |     invsqrt_lr_decay_gamma: -1\n17:11:26 |     is_debug: False\n17:11:26 |     label_truncate: 72\n17:11:26 |     learn_embeddings: True\n17:11:26 |     learn_positional_embeddings: True\n17:11:26 |     learningrate: 5e-05\n17:11:26 |     load_from_pretrained_ranker: True\n17:11:26 |     log_every_n_secs: 10.0\n17:11:26 |     log_every_n_steps: 50\n17:11:26 |     log_keep_fields: all\n17:11:26 |     loglevel: info\n17:11:26 |     lr_scheduler: reduceonplateau\n17:11:26 |     lr_scheduler_decay: 0.5\n17:11:26 |     lr_scheduler_patience: 3\n17:11:26 |     max_train_steps: -1\n17:11:26 |     max_train_time: 7200.0\n17:11:26 |     memory_attention: sqrt\n17:11:26 |     metrics: default\n17:11:26 |     model: transformer/classifier\n17:11:26 |     model_file: /tmp/model5\n17:11:26 |     model_parallel: False\n17:11:26 |     momentum: 0\n17:11:26 |     multitask_weights: [1]\n17:11:26 |     mutators: None\n17:11:26 |     n_decoder_layers: -1\n17:11:26 |     n_encoder_layers: -1\n17:11:26 |     n_heads: 12\n17:11:26 |     n_layers: 12\n17:11:26 |     n_positions: 1024\n17:11:26 |     n_segments: 2\n17:11:26 |     nesterov: True\n17:11:26 |     no_cuda: False\n17:11:26 |     normalize_sent_emb: False\n17:11:26 |     num_epochs: -1\n17:11:26 |     num_examples: -1\n17:11:26 |     num_workers: 0\n17:11:26 |     nus: [0.7]\n17:11:26 |     optimizer: adamax\n17:11:26 |     output_scaling: 0.06\n17:11:26 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n17:11:26 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:11:26 |     person_tokens: False\n17:11:26 |     print_scores: False\n17:11:26 |     rank_candidates: False\n17:11:26 |     rank_top_k: -1\n17:11:26 |     reduction_type: mean\n17:11:26 |     ref_class: None\n17:11:26 |     relu_dropout: 0.0\n17:11:26 |     repeat_blocking_heuristic: True\n17:11:26 |     report_filename: \n17:11:26 |     return_cand_scores: False\n17:11:26 |     save_after_valid: True\n17:11:26 |     save_every_n_secs: -1\n17:11:26 |     save_format: conversations\n17:11:26 |     share_encoders: False\n17:11:26 |     share_word_embeddings: False\n17:11:26 |     short_final_eval: False\n17:11:26 |     special_tok_lst: None\n17:11:26 |     split_lines: False\n17:11:26 |     starttime: Dec03_17-09\n17:11:26 |     task: fromfile:parlaiformat\n17:11:26 |     tensorboard_log: False\n17:11:26 |     tensorboard_logdir: None\n17:11:26 |     text_truncate: 360\n17:11:26 |     threshold: 0.5\n17:11:26 |     topk: 5\n17:11:26 |     train_predict: False\n17:11:26 |     truncate: 1024\n17:11:26 |     update_classifier_head_only: False\n17:11:26 |     update_freq: 1\n17:11:26 |     use_memories: False\n17:11:26 |     use_reply: none\n17:11:26 |     validation_cutoff: 1.0\n17:11:26 |     validation_every_n_epochs: -1\n17:11:26 |     validation_every_n_secs: 20.0\n17:11:26 |     validation_every_n_steps: -1\n17:11:26 |     validation_max_exs: -1\n17:11:26 |     validation_metric: accuracy\n17:11:26 |     validation_metric_mode: max\n17:11:26 |     validation_patience: 30\n17:11:26 |     validation_share_agent: False\n17:11:26 |     variant: xlm\n17:11:26 |     verbose: False\n17:11:26 |     wandb_entity: None\n17:11:26 |     wandb_log: False\n17:11:26 |     wandb_name: None\n17:11:26 |     wandb_project: None\n17:11:26 |     warmup_rate: 0.0001\n17:11:26 |     warmup_updates: 1000\n17:11:26 |     weight_decay: None\n17:11:26 |     world_logs: \n17:11:26 |     wrap_memory_encoder: False\n17:11:27 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:11:27 | creating task(s): fromfile:parlaiformat\n17:11:27 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:11:27 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev1corr2type2/run5/data_train-b.txt\n17:11:32 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8250 8.25e-10               .8293                 .8252   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8333            .8205              .8247   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8163 11.79 551.8 512.7       0          0 37.16  200 .8250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.02 .5598 5.204e-06 240.8 223.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 792.6 736.4        .8250\u001b[0m\n17:11:32 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8250 8.25e-10               .8293                 .8252   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8333            .8205              .8247   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8163 11.79 551.8 512.7       0          0 37.16  200 .8250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.02 .5598 5.204e-06 240.8 223.7       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 792.6 736.4        .8250\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:11:34.256341Z","iopub.execute_input":"2022-12-03T17:11:34.256778Z","iopub.status.idle":"2022-12-03T17:11:35.434956Z","shell.execute_reply.started":"2022-12-03T17:11:34.256736Z","shell.execute_reply":"2022-12-03T17:11:35.433492Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Choose completion prev2corr1type1","metadata":{}},{"cell_type":"markdown","source":"run 1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-03T17:34:31.823313Z","iopub.execute_input":"2022-12-03T17:34:31.824347Z","iopub.status.idle":"2022-12-03T17:45:28.722144Z","shell.execute_reply.started":"2022-12-03T17:34:31.824306Z","shell.execute_reply":"2022-12-03T17:45:28.720925Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"17:34:39 | building dictionary first...\n17:34:39 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:34:39 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:34:39 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:34:39 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:34:39 | Using CUDA\n17:34:39 | loading dictionary from /tmp/model1.dict\n17:34:39 | num words = 54944\n17:34:43 | Loading existing model parameters from /tmp/model1\n17:34:51 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:34:52 | Opt:\n17:34:52 |     activation: gelu\n17:34:52 |     adafactor_eps: '[1e-30, 0.001]'\n17:34:52 |     adam_eps: 1e-08\n17:34:52 |     add_p1_after_newln: False\n17:34:52 |     aggregate_micro: False\n17:34:52 |     allow_missing_init_opts: False\n17:34:52 |     attention_dropout: 0.1\n17:34:52 |     batchsize: 20\n17:34:52 |     betas: '(0.9, 0.999)'\n17:34:52 |     bpe_add_prefix_space: None\n17:34:52 |     bpe_debug: False\n17:34:52 |     bpe_dropout: None\n17:34:52 |     bpe_merge: None\n17:34:52 |     bpe_vocab: None\n17:34:52 |     candidates: inline\n17:34:52 |     cap_num_predictions: 100\n17:34:52 |     checkpoint_activations: False\n17:34:52 |     class_weights: None\n17:34:52 |     classes: \"['__notok__', '__ok__']\"\n17:34:52 |     classes_from_file: None\n17:34:52 |     data_parallel: True\n17:34:52 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:34:52 |     datatype: train\n17:34:52 |     delimiter: '\\n'\n17:34:52 |     dict_class: parlai.core.dict:DictionaryAgent\n17:34:52 |     dict_endtoken: __start__\n17:34:52 |     dict_file: /tmp/model1.dict\n17:34:52 |     dict_include_test: False\n17:34:52 |     dict_include_valid: False\n17:34:52 |     dict_initpath: None\n17:34:52 |     dict_language: english\n17:34:52 |     dict_loaded: True\n17:34:52 |     dict_lower: True\n17:34:52 |     dict_max_ngram_size: -1\n17:34:52 |     dict_maxexs: -1\n17:34:52 |     dict_maxtokens: -1\n17:34:52 |     dict_minfreq: 0\n17:34:52 |     dict_nulltoken: __null__\n17:34:52 |     dict_starttoken: __start__\n17:34:52 |     dict_textfields: text,labels\n17:34:52 |     dict_tokenizer: bpe\n17:34:52 |     dict_unktoken: __unk__\n17:34:52 |     display_examples: False\n17:34:52 |     download_path: None\n17:34:52 |     dropout: 0.1\n17:34:52 |     dynamic_batching: None\n17:34:52 |     embedding_projection: random\n17:34:52 |     embedding_size: 768\n17:34:52 |     embedding_type: random\n17:34:52 |     embeddings_scale: False\n17:34:52 |     encode_candidate_vecs: True\n17:34:52 |     encode_candidate_vecs_batchsize: 256\n17:34:52 |     eval_batchsize: None\n17:34:52 |     eval_candidates: inline\n17:34:52 |     eval_dynamic_batching: None\n17:34:52 |     evaltask: None\n17:34:52 |     ffn_size: 3072\n17:34:52 |     final_extra_opt: \n17:34:52 |     fixed_candidate_vecs: reuse\n17:34:52 |     fixed_candidates_path: None\n17:34:52 |     force_fp16_tokens: True\n17:34:52 |     fp16: True\n17:34:52 |     fp16_impl: safe\n17:34:52 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt\n17:34:52 |     fromfile_datatype_extension: False\n17:34:52 |     gpu: -1\n17:34:52 |     gradient_clip: 0.1\n17:34:52 |     hide_labels: False\n17:34:52 |     history_add_global_end_token: None\n17:34:52 |     history_reversed: False\n17:34:52 |     history_size: 20\n17:34:52 |     ignore_bad_candidates: False\n17:34:52 |     ignore_labels: None\n17:34:52 |     image_cropsize: 224\n17:34:52 |     image_mode: raw\n17:34:52 |     image_size: 256\n17:34:52 |     inference: max\n17:34:52 |     init_model: zoo:pretrained_transformers/bi_model_huge_reddit/model\n17:34:52 |     init_opt: None\n17:34:52 |     interactive_candidates: fixed\n17:34:52 |     interactive_mode: False\n17:34:52 |     invsqrt_lr_decay_gamma: -1\n17:34:52 |     is_debug: False\n17:34:52 |     label_truncate: 72\n17:34:52 |     learn_embeddings: True\n17:34:52 |     learn_positional_embeddings: True\n17:34:52 |     learningrate: 5e-05\n17:34:52 |     load_from_checkpoint: False\n17:34:52 |     load_from_pretrained_ranker: True\n17:34:52 |     log_every_n_secs: 10.0\n17:34:52 |     log_every_n_steps: 50\n17:34:52 |     log_keep_fields: all\n17:34:52 |     loglevel: info\n17:34:52 |     lr_scheduler: reduceonplateau\n17:34:52 |     lr_scheduler_decay: 0.5\n17:34:52 |     lr_scheduler_patience: 3\n17:34:52 |     max_train_steps: -1\n17:34:52 |     max_train_time: 7200.0\n17:34:52 |     memory_attention: sqrt\n17:34:52 |     metrics: default\n17:34:52 |     model: transformer/classifier\n17:34:52 |     model_file: /tmp/model1\n17:34:52 |     model_parallel: False\n17:34:52 |     momentum: 0\n17:34:52 |     multitask_weights: [1]\n17:34:52 |     mutators: None\n17:34:52 |     n_decoder_layers: -1\n17:34:52 |     n_encoder_layers: -1\n17:34:52 |     n_heads: 12\n17:34:52 |     n_layers: 12\n17:34:52 |     n_positions: 1024\n17:34:52 |     n_segments: 2\n17:34:52 |     nesterov: True\n17:34:52 |     no_cuda: False\n17:34:52 |     normalize_sent_emb: False\n17:34:52 |     num_epochs: -1\n17:34:52 |     num_workers: 0\n17:34:52 |     nus: [0.7]\n17:34:52 |     optimizer: adamax\n17:34:52 |     output_scaling: 0.06\n17:34:52 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n17:34:52 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:34:52 |     person_tokens: False\n17:34:52 |     print_scores: False\n17:34:52 |     rank_candidates: False\n17:34:52 |     rank_top_k: -1\n17:34:52 |     reduction_type: mean\n17:34:52 |     ref_class: None\n17:34:52 |     relu_dropout: 0.0\n17:34:52 |     repeat_blocking_heuristic: True\n17:34:52 |     return_cand_scores: False\n17:34:52 |     save_after_valid: True\n17:34:52 |     save_every_n_secs: -1\n17:34:52 |     save_format: conversations\n17:34:52 |     share_encoders: False\n17:34:52 |     share_word_embeddings: False\n17:34:52 |     short_final_eval: False\n17:34:52 |     special_tok_lst: None\n17:34:52 |     split_lines: False\n17:34:52 |     starttime: Dec03_17-32\n17:34:52 |     task: fromfile:parlaiformat\n17:34:52 |     tensorboard_log: False\n17:34:52 |     tensorboard_logdir: None\n17:34:52 |     text_truncate: 360\n17:34:52 |     threshold: 0.5\n17:34:52 |     topk: 5\n17:34:52 |     train_predict: False\n17:34:52 |     truncate: 1024\n17:34:52 |     update_classifier_head_only: False\n17:34:52 |     update_freq: 1\n17:34:52 |     use_memories: False\n17:34:52 |     use_reply: none\n17:34:52 |     validation_cutoff: 1.0\n17:34:52 |     validation_every_n_epochs: -1\n17:34:52 |     validation_every_n_secs: 20.0\n17:34:52 |     validation_every_n_steps: -1\n17:34:52 |     validation_max_exs: -1\n17:34:52 |     validation_metric: accuracy\n17:34:52 |     validation_metric_mode: max\n17:34:52 |     validation_patience: 30\n17:34:52 |     validation_share_agent: False\n17:34:52 |     variant: xlm\n17:34:52 |     verbose: False\n17:34:52 |     wandb_entity: None\n17:34:52 |     wandb_log: False\n17:34:52 |     wandb_name: None\n17:34:52 |     wandb_project: None\n17:34:52 |     warmup_rate: 0.0001\n17:34:52 |     warmup_updates: 1000\n17:34:52 |     weight_decay: None\n17:34:52 |     world_logs: \n17:34:52 |     wrap_memory_encoder: False\n17:34:52 | creating task(s): fromfile:parlaiformat\n17:34:52 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt\n17:34:52 | training...\n17:35:03 | time:72s total_exs:2520 total_steps:126 epochs:105.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.23     1 264.5 500.4       0          0 37.83  380   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.871    .1189 6.016 .2522 6.304e-06 120.3 227.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  126 384.8  728 1.896            1\n\n17:35:13 | time:82s total_exs:3260 total_steps:163 epochs:135.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.57     1 271.4  1030       0          0 75.86  740   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen   loss        lr  ltpb  ltps  \\\n     1             32768  1.031    .1189 5.957 .07612 8.154e-06 119.1 451.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  163 390.6 1482 3.802            1\n\n17:35:13 | creating task(s): fromfile:parlaiformat\n17:35:13 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:35:13 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt\n17:35:13 | running eval: valid\n17:35:13 | eval completed in 0.21s\n17:35:13 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1726       0          0 128.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08086     6 .0178 8.154e-06    72 769.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    163 233.5 2496            1\n\u001b[0m\n17:35:13 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 1\u001b[0m\n17:35:13 | saving model checkpoint: /tmp/model1.checkpoint\n17:35:27 | time:97s total_exs:4020 total_steps:201 epochs:167.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.64 .5263 272.7  1028       0          0 75.38  760   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen   loss        lr  ltpb  ltps  \\\n     1             32768  .1311    .1189 5.976 .01033 1.005e-05 119.5 450.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  201 392.2 1478 3.778            1\n\n17:35:33 | time:102s total_exs:4420 total_steps:221 epochs:184.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.28     0 265.6 958.9       0          0 72.22  400   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss        lr  ltpb  ltps  \\\n     1             32768 .04109    .1189  6.08 .003492 1.105e-05 121.6 439.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  221 387.1 1398 3.626            1\n\n17:35:33 | running eval: valid\n17:35:33 | eval completed in 0.20s\n17:35:33 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1826       0          0 135.7   24   1   \n    gpu_mem  llen    loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08087     6 .002037 1.105e-05    72 814.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    221 233.5 2641            1\n\u001b[0m\n17:35:33 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 2\u001b[0m\n17:35:33 | saving model checkpoint: /tmp/model1.checkpoint\n17:35:48 | time:117s total_exs:5180 total_steps:259 epochs:215.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.87     0 277.4  1044       0          0 75.28  760   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss        lr  ltpb  ltps  \\\n     1             32768 .02768    .1189 5.929 .002379 1.295e-05 118.6 446.3   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                  259  396 1491 3.772            1\n\n17:35:53 | time:123s total_exs:5580 total_steps:279 epochs:232.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.14     0 262.9 991.1       0          0 75.39  400   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss        lr  ltpb  ltps  \\\n     1             32768 .02253    .1189 6.035 .001933 1.395e-05 120.7   455   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  279 383.6 1446 3.786            1\n\n17:35:53 | running eval: valid\n17:35:54 | eval completed in 0.22s\n17:35:54 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1585       0          0 117.7   24   1   \n    gpu_mem  llen   loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .00126 1.395e-05    72 706.7       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    279 233.5 2292            1\n\u001b[0m\n17:35:54 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 3\u001b[0m\n17:35:54 | saving model checkpoint: /tmp/model1.checkpoint\n17:36:09 | time:138s total_exs:6320 total_steps:316 epochs:263.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.26     0 265.2   964       0          0  72.7  740   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss       lr  ltpb  ltps  \\\n     1             32768 .01954    .1189 6.019 .001682 1.58e-05 120.4 437.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  316 385.6 1402 3.643            1\n\n17:36:14 | time:143s total_exs:6720 total_steps:336 epochs:280.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 10.77     0 255.3 961.1       0          0 75.29  400   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss       lr  ltpb  ltps  \\\n     1             32768 .01778    .1189  6.11 .001518 1.68e-05 122.2   460   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  336 377.5 1421 3.781            1\n\n17:36:14 | running eval: valid\n17:36:14 | eval completed in 0.20s\n17:36:14 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1790       0          0 132.9   24   1   \n    gpu_mem  llen    loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .001012 1.68e-05    72 797.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    336 233.5 2588            1\n\u001b[0m\n17:36:14 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 4\u001b[0m\n17:36:14 | saving model checkpoint: /tmp/model1.checkpoint\n17:36:29 | time:158s total_exs:7480 total_steps:374 epochs:311.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.63     0 272.7  1029       0          0 75.44  760   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss       lr  ltpb  ltps  \\\n     1             32768 .01613    .1189 6.024 .001386 1.87e-05 120.5 454.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                  374 393.2 1483 3.78            1\n\n17:36:34 | time:164s total_exs:7880 total_steps:394 epochs:328.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.41     0 268.2 998.4       0          0 74.45  400   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss       lr  ltpb  ltps  \\\n     1             32768 .01502    .1190  6.11 .001289 1.97e-05 122.2 454.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  394 390.4 1453 3.738            1\n\n17:36:34 | running eval: valid\n17:36:35 | eval completed in 0.23s\n17:36:35 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1588       0          0   118   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0809     6 .0008787 1.97e-05    72 708.1       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    394 233.5 2297            1\n\u001b[0m\n17:36:35 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 5\u001b[0m\n17:36:35 | saving model checkpoint: /tmp/model1.checkpoint\n17:36:54 | time:184s total_exs:8640 total_steps:432 epochs:360.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.52     0 270.4  1026       0          0 75.88  760   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss       lr  ltpb  ltps  \\\n     1             32768 .01405    .1190 6.016 .001205 2.16e-05 120.3 456.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  432 390.8 1483 3.803            1\n\n17:36:55 | time:184s total_exs:8660 total_steps:433 epochs:360.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1  11.6     0   272  1080       0          0 79.36   20   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss        lr  ltpb  ltps  \\\n     1             32768 .01338    .1190   5.8 .001147 2.165e-05   116 460.5   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                  433  388 1541 4.361            1\n\n17:36:55 | running eval: valid\n17:36:55 | eval completed in 0.20s\n17:36:55 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1786       0          0 132.6   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08091     6 .0008125 2.165e-05    72   796       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    433 233.5 2582            1\n\u001b[0m\n17:36:55 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 6\u001b[0m\n17:36:55 | saving model checkpoint: /tmp/model1.checkpoint\n17:37:10 | time:200s total_exs:9420 total_steps:471 epochs:392.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.49     0 269.8  1003       0          0 74.33  760   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss        lr  ltpb  ltps  \\\n     1             32768 .01291    .1190 6.032 .001106 2.355e-05 120.6 448.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  471 390.4 1451 3.725            1\n\n17:37:15 | time:205s total_exs:9800 total_steps:490 epochs:408.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.67     0 273.5  1046       0          0 76.53  380   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen    loss       lr  ltpb  ltps  \\\n     1             32768 .01218    .1190 5.953 .001042 2.45e-05 119.1 455.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  490 392.5 1502 3.844            1\n\n17:37:15 | running eval: valid\n17:37:15 | eval completed in 0.19s\n17:37:15 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1847       0          0 137.2   24   1   \n    gpu_mem  llen    loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08092     6 .000728 2.45e-05    72 823.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    490 233.5 2671            1\n\u001b[0m\n17:37:15 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 7\u001b[0m\n17:37:15 | saving model checkpoint: /tmp/model1.checkpoint\n17:37:30 | time:220s total_exs:10560 total_steps:528 epochs:440.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.32     0 266.4  1013       0          0    76  760   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .01149    .1190 5.968 .0009818 2.64e-05 119.4 453.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  528 385.8 1466 3.809            1\n\n17:37:36 | time:225s total_exs:10960 total_steps:548 epochs:456.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.79     0 275.7  1048       0          0 76.03  400   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768  .0108    .1190  5.99 .0009229 2.74e-05 119.8 455.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  548 395.5 1504 3.818            1\n\n17:37:36 | running eval: valid\n17:37:36 | eval completed in 0.20s\n17:37:36 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1782       0          0 132.4   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08093     6 .0006513 2.74e-05    72 794.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    548 233.5 2577            1\n\u001b[0m\n17:37:36 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 8\u001b[0m\n17:37:36 | saving model checkpoint: /tmp/model1.checkpoint\n17:37:56 | time:245s total_exs:11720 total_steps:586 epochs:488.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.52     0 270.3  1027       0          0 75.99  760   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .01019    .1190 6.058 .0008702 2.93e-05 121.2 460.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  586 391.5 1487 3.808            1\n\n17:37:56 | time:245s total_exs:11740 total_steps:587 epochs:489.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.15     0   263  1020       0          0 77.55   20   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .009779    .1190   5.9 .0008346 2.935e-05   118   \n    ltps  ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n   457.6       0          0                  587  381 1478 4.252            1\n\n17:37:56 | running eval: valid\n17:37:56 | eval completed in 0.20s\n17:37:56 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1794       0          0 133.2   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08093     6 .0006028 2.935e-05    72 799.7       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    587 233.5 2594            1\n\u001b[0m\n17:37:56 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 9\u001b[0m\n17:37:56 | saving model checkpoint: /tmp/model1.checkpoint\n17:38:11 | time:261s total_exs:12500 total_steps:625 epochs:520.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.68     0 273.6  1016       0          0 74.29  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .009383    .1190 5.984 .0008003 3.125e-05 119.7   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   444.5       0          0                  625 393.2 1461 3.723            1\n\n17:38:16 | time:266s total_exs:12880 total_steps:644 epochs:536.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.76     0 275.2  1030       0          0 74.86  380   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .00882    .1190 5.937 .0007512 3.22e-05 118.7 444.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                  644 393.9 1475 3.76            1\n\n17:38:16 | running eval: valid\n17:38:17 | eval completed in 0.20s\n17:38:17 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1824       0          0 135.4   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08094     6 .0005376 3.22e-05    72 812.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    644 233.5 2637            1\n\u001b[0m\n17:38:17 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 10\u001b[0m\n17:38:17 | saving model checkpoint: /tmp/model1.checkpoint\n17:38:32 | time:281s total_exs:13640 total_steps:682 epochs:568.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.26     0 265.1 997.7       0          0 75.27  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .008325    .1190 5.987 .0007088 3.41e-05 119.7 450.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  682 384.8 1448 3.772            1\n\n17:38:37 | time:286s total_exs:14040 total_steps:702 epochs:585.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.77     0 275.3  1043       0          0  75.8  400   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .007816    .1190     6 .0006648 3.51e-05   120 454.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  702 395.3 1498 3.807            1\n\n17:38:37 | running eval: valid\n17:38:37 | eval completed in 0.20s\n17:38:37 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1825       0          0 135.5   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08095     6 .0004774 3.51e-05    72 813.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    702 233.5 2638            1\n\u001b[0m\n17:38:37 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 11\u001b[0m\n17:38:37 | saving model checkpoint: /tmp/model1.checkpoint\n17:38:57 | time:307s total_exs:14800 total_steps:740 epochs:616.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.54     0 270.8  1034       0          0 76.36  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss      lr  ltpb  ltps  \\\n     1             32768 .007358    .1190 6.021 .0006258 3.7e-05 120.4 459.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  740 391.2 1494 3.827            1\n\n17:38:57 | running eval: valid\n17:38:57 | eval completed in 0.20s\n17:38:57 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1790       0          0   133   24   1   \n    gpu_mem  llen     loss      lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08096     6 .0004409 3.7e-05    72 798.1       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    740 233.5 2589            1\n\u001b[0m\n17:38:57 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 12\u001b[0m\n17:38:57 | saving model checkpoint: /tmp/model1.checkpoint\n17:39:17 | time:327s total_exs:15540 total_steps:777 epochs:647.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.35     0   267 981.4       0          0 73.52  740   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .006794    .1190 6.046 .0005774 3.885e-05 120.9   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   444.5       0          0                  777 387.9 1426 3.684            1\n\n17:39:18 | time:327s total_exs:15560 total_steps:778 epochs:648.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1  10.5     0   250 973.4       0          0 77.85   20   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .006517    .1190   5.6 .0005533 3.89e-05   112 436.1   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                  778  362 1410 4.275            1\n\n17:39:18 | running eval: valid\n17:39:18 | eval completed in 0.21s\n17:39:18 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1747       0          0 129.7   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08097     6 .0004069 3.89e-05    72 778.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    778 233.5 2526            1\n\u001b[0m\n17:39:18 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 13\u001b[0m\n17:39:18 | saving model checkpoint: /tmp/model1.checkpoint\n17:39:38 | time:347s total_exs:16320 total_steps:816 epochs:680.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.38     0 267.6  1013       0          0 75.69  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .006254    .1190 6.032 .0005311 4.08e-05 120.6 456.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  816 388.2 1469 3.793            1\n\n17:39:38 | running eval: valid\n17:39:38 | eval completed in 0.19s\n17:39:38 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1842       0          0 136.8   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08098     6 .0003753 4.08e-05    72   821       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    816 233.5 2663            1\n\u001b[0m\n17:39:38 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 14\u001b[0m\n17:39:38 | saving model checkpoint: /tmp/model1.checkpoint\n17:39:58 | time:367s total_exs:17080 total_steps:854 epochs:711.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.59     0 271.9  1032       0          0  75.9  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .005761    .1190 5.982 .0004891 4.27e-05 119.6   454   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  854 391.5 1486 3.803            1\n\n17:39:59 | time:368s total_exs:17120 total_steps:856 epochs:713.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1  11.8     0   276  1059       0          0 76.71   40   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen    loss       lr  ltpb  ltps  \\\n     1             32768 .005511    .1190  6.05 .000468 4.28e-05   121 464.2   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                  856  397 1523 4.012            1\n\n17:39:59 | running eval: valid\n17:39:59 | eval completed in 0.20s\n17:39:59 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1798       0          0 133.6   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08098     6 .0003445 4.28e-05    72 801.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    856 233.5 2600            1\n\u001b[0m\n17:39:59 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 15\u001b[0m\n17:39:59 | saving model checkpoint: /tmp/model1.checkpoint\n17:40:14 | time:383s total_exs:17880 total_steps:894 epochs:745.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.68     0 273.6  1034       0          0 75.56  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .005279    .1190 5.932 .0004476 4.47e-05 118.6 448.2   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  894 392.2 1482 3.787            1\n\n17:40:19 | time:388s total_exs:18240 total_steps:912 epochs:760.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.41     0 268.2 959.4       0          0 71.55  360   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .004973    .1190 5.989 .0004216 4.56e-05 119.8 428.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  912 387.9 1388 3.594            1\n\n17:40:19 | running eval: valid\n17:40:19 | eval completed in 0.20s\n17:40:19 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1754       0          0 130.2   24   1   \n    gpu_mem  llen     loss       lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08099     6 .0003057 4.56e-05    72 781.7       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    912 233.5 2535            1\n\u001b[0m\n17:40:19 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 16\u001b[0m\n17:40:19 | saving model checkpoint: /tmp/model1.checkpoint\n17:40:34 | time:403s total_exs:19000 total_steps:950 epochs:791.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.35     0 266.9  1008       0          0  75.5  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss       lr  ltpb  ltps  \\\n     1             32768 .004686    .1191 6.024 .0003971 4.75e-05 120.5 454.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  950 387.4 1463 3.784            1\n\n17:40:39 | time:408s total_exs:19380 total_steps:969 epochs:807.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.54     0 270.8  1010       0          0  74.6  380   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .004397    .1191 6.011 .0003725 4.845e-05 120.2   \n    ltps  ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n   448.4       0          0                  969  391 1458 3.746            1\n\n17:40:39 | running eval: valid\n17:40:39 | eval completed in 0.20s\n17:40:39 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1834       0          0 136.2   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0810     6 .0002705 4.845e-05    72 817.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    969 233.5 2652            1\n\u001b[0m\n17:40:39 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 17\u001b[0m\n17:40:39 | saving model checkpoint: /tmp/model1.checkpoint\n17:40:54 | time:423s total_exs:20120 total_steps:1006 epochs:838.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.59     0 271.8  1004       0          0 73.88  740   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .004153    .1191 6.119 .0003517 4.995e-05 122.4   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n     452       0          0                 1006 394.2 1456 3.702            1\n\n17:40:59 | time:429s total_exs:20520 total_steps:1026 epochs:855.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.26     0 265.2  1001       0          0 75.51  400   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .003903    .1191     6 .0003304 4.995e-05   120   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n     453       0          0                 1026 385.2 1454 3.792            1\n\n17:40:59 | running eval: valid\n17:41:00 | eval completed in 0.23s\n17:41:00 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1530       0          0 113.7   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08101     6 .0002396 4.995e-05    72 682.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1026 233.5 2213            1\n\u001b[0m\n17:41:00 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 18\u001b[0m\n17:41:00 | saving model checkpoint: /tmp/model1.checkpoint\n17:41:15 | time:444s total_exs:21280 total_steps:1064 epochs:886.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.32     0 266.3 999.5       0          0 75.05  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .003677    .1191 6.005 .0003112 4.995e-05 120.1   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   450.7       0          0                 1064 386.4 1450 3.761            1\n\n17:41:20 | time:449s total_exs:21660 total_steps:1083 epochs:902.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.38     0 267.5 991.6       0          0 74.13  380   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .003469    .1191 5.942 .0002934 4.995e-05 118.8   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   440.5       0          0                 1083 386.4 1432 3.723            1\n\n17:41:20 | running eval: valid\n17:41:20 | eval completed in 0.21s\n17:41:20 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1789       0          0 132.9   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08102     6 .0002136 4.995e-05    72 797.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1083 233.5 2587            1\n\u001b[0m\n17:41:20 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 19\u001b[0m\n17:41:20 | saving model checkpoint: /tmp/model1.checkpoint\n17:41:35 | time:464s total_exs:22420 total_steps:1121 epochs:934.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.59     0 271.8  1018       0          0  74.9  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .003284    .1191 5.995 .0002776 4.995e-05 119.9   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n     449       0          0                 1121 391.7 1467 3.754            1\n\n17:41:40 | time:469s total_exs:22800 total_steps:1140 epochs:950.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.67     0 273.5  1049       0          0 76.69  380   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .003105    .1191 5.947 .0002625 4.995e-05 118.9   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   456.1       0          0                 1140 392.4 1505 3.852            1\n\n17:41:40 | running eval: valid\n17:41:40 | eval completed in 0.20s\n17:41:40 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1829       0          0 135.9   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08103     6 .0001915 4.995e-05    72 815.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1140 233.5 2645            1\n\u001b[0m\n17:41:40 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 20\u001b[0m\n17:41:40 | saving model checkpoint: /tmp/model1.checkpoint\n17:41:55 | time:485s total_exs:23540 total_steps:1177 epochs:980.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.33     0 266.7 983.8       0          0 73.78  740   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen     loss        lr  ltpb  ltps  \\\n     1             32768 .00295    .1191 6.024 .0002494 4.995e-05 120.5 444.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                 1177 387.2 1428 3.697            1\n\n17:42:00 | time:490s total_exs:23920 total_steps:1196 epochs:996.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.76     0 275.2  1053       0          0 76.51  380   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen     loss        lr  ltpb  ltps  \\\n     1             32768  .0028    .1191   5.9 .0002366 4.995e-05   118 451.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                 1196 393.2 1504 3.843            1\n\n17:42:00 | running eval: valid\n17:42:01 | eval completed in 0.21s\n17:42:01 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1737       0          0   129   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08104     6 .0001728 4.995e-05    72 774.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1196 233.5 2511            1\n\u001b[0m\n17:42:01 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 21\u001b[0m\n17:42:01 | saving model checkpoint: /tmp/model1.checkpoint\n17:42:16 | time:505s total_exs:24680 total_steps:1234 epochs:1028.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.37     0 267.4  1014       0          0 75.82  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .002666    .1191 6.055 .0002252 4.995e-05 121.1   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   459.1       0          0                 1234 388.6 1473 3.802            1\n\n17:42:21 | time:510s total_exs:25060 total_steps:1253 epochs:1044.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.84     0 276.9  1022       0          0 73.85  380   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .002532    .1191 5.932 .0002138 4.995e-05 118.6   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n     438       0          0                 1253 395.5 1460 3.709            1\n\n17:42:21 | running eval: valid\n17:42:21 | eval completed in 0.20s\n17:42:21 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1776       0          0 131.9   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08106     6 .0001564 4.995e-05    72 791.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1253 233.5 2568            1\n\u001b[0m\n17:42:21 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 22\u001b[0m\n17:42:21 | saving model checkpoint: /tmp/model1.checkpoint\n17:42:41 | time:530s total_exs:25820 total_steps:1291 epochs:1075.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.36     0 267.1  1013       0          0 75.83  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen    loss        lr  ltpb  ltps  \\\n     1             32768 .002416    .1191 5.979 .000204 4.995e-05 119.6 453.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                 1291 386.7 1466  3.8            1\n\n17:42:41 | time:531s total_exs:25860 total_steps:1293 epochs:1077.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.07     0 261.5  1023       0          0 78.26   40   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .002339    .1191   6.2 .0001975 4.995e-05   124   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   485.3       0          0                 1293 385.5 1509 4.095            1\n\n17:42:41 | running eval: valid\n17:42:41 | eval completed in 0.19s\n17:42:41 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1840       0          0 136.7   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08106     6 .0001461 4.995e-05    72 820.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1293 233.5 2661            1\n\u001b[0m\n17:42:41 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 23\u001b[0m\n17:42:41 | saving model checkpoint: /tmp/model1.checkpoint\n17:42:56 | time:546s total_exs:26600 total_steps:1330 epochs:1108.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.59     0 271.8 989.4       0          0 72.82  740   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen     loss        lr  ltpb  ltps  \\\n     1             32768 .00226    .1191 5.986 .0001907 4.995e-05 119.7 435.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                 1330 391.5 1425 3.649            1\n\n17:43:02 | time:551s total_exs:26980 total_steps:1349 epochs:1124.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.48     0 269.5 996.8       0          0 73.96  380   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .002161    .1191 6.068 .0001824 4.995e-05 121.4   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   448.9       0          0                 1349 390.9 1446 3.715            1\n\n17:43:02 | running eval: valid\n17:43:02 | eval completed in 0.19s\n17:43:02 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1846       0          0 137.1   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08107     6 .0001332 4.995e-05    72 822.7       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1349 233.5 2668            1\n\u001b[0m\n17:43:02 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 24\u001b[0m\n17:43:02 | saving model checkpoint: /tmp/model1.checkpoint\n17:43:22 | time:571s total_exs:27740 total_steps:1387 epochs:1155.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.55     0   271  1026       0          0 75.71  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen    loss        lr  ltpb  ltps  \\\n     1             32768 .002063    .1191 5.987 .000174 4.995e-05 119.7 453.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                 1387 390.7 1479 3.794            1\n\n17:43:22 | time:571s total_exs:27760 total_steps:1388 epochs:1156.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.55     0   271  1001       0          0 73.87   20   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001997    .1191   5.9 .0001685 4.995e-05   118   \n    ltps  ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n   435.9       0          0                 1388  389 1437 4.036            1\n\n17:43:22 | running eval: valid\n17:43:22 | eval completed in 0.20s\n17:43:22 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1818       0          0   135   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08108     6 .0001251 4.995e-05    72 810.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1388 233.5 2629            1\n\u001b[0m\n17:43:22 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 25\u001b[0m\n17:43:22 | saving model checkpoint: /tmp/model1.checkpoint\n17:43:42 | time:592s total_exs:28500 total_steps:1425 epochs:1187.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.45     0   269  1019       0          0 75.78  740   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001941    .1191 6.014 .0001637 4.995e-05 120.3   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   455.7       0          0                 1425 389.3 1475 3.798            1\n\n17:43:42 | running eval: valid\n17:43:42 | eval completed in 0.20s\n17:43:42 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1806       0          0 134.2   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08109     6 .0001181 4.995e-05    72 805.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1425 233.5 2612            1\n\u001b[0m\n17:43:42 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 26\u001b[0m\n17:43:42 | saving model checkpoint: /tmp/model1.checkpoint\n17:44:02 | time:611s total_exs:29240 total_steps:1462 epochs:1218.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1  11.4     0   268 973.5       0          0 72.64  740   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001833    .1191 5.976 .0001545 4.995e-05 119.5   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n   434.1       0          0                 1462 387.5 1408 3.64            1\n\n17:44:03 | time:612s total_exs:29280 total_steps:1464 epochs:1220.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1  11.9     0   278  1093       0          0 78.61   40   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001773    .1191   5.9 .0001495 4.995e-05   118   \n    ltps  ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n   463.8       0          0                 1464  396 1557 4.115            1\n\n17:44:03 | running eval: valid\n17:44:03 | eval completed in 0.20s\n17:44:03 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1823       0          0 135.4   24   1   \n    gpu_mem  llen     loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08109     6 .0001112 4.995e-05    72 812.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1464 233.5 2635            1\n\u001b[0m\n17:44:03 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 27\u001b[0m\n17:44:03 | saving model checkpoint: /tmp/model1.checkpoint\n17:44:18 | time:627s total_exs:30040 total_steps:1502 epochs:1251.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.55     0   271  1016       0          0 74.97  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001727    .1192 5.997 .0001456 4.995e-05 119.9   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   449.6       0          0                 1502 390.9 1465 3.757            1\n\n17:44:23 | time:632s total_exs:30420 total_steps:1521 epochs:1267.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.09     0 261.8 999.7       0          0 76.37  380   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001654    .1192 5.984 .0001394 4.995e-05 119.7   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n     457       0          0                 1521 381.5 1457 3.836            1\n\n17:44:23 | running eval: valid\n17:44:23 | eval completed in 0.20s\n17:44:23 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1815       0          0 134.8   24   1   \n    gpu_mem  llen    loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0811     6 .000102 4.995e-05    72 809.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1521 233.5 2625            1\n\u001b[0m\n17:44:23 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 28\u001b[0m\n17:44:23 | saving model checkpoint: /tmp/model1.checkpoint\n17:44:38 | time:648s total_exs:31160 total_steps:1558 epochs:1298.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.33     0 266.5 981.9       0          0 73.68  740   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001587    .1192 5.984 .0001338 4.995e-05 119.7   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   440.9       0          0                 1558 386.2 1423 3.693            1\n\n17:44:43 | time:653s total_exs:31540 total_steps:1577 epochs:1314.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.54     0 270.9  1016       0          0 75.02  380   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001523    .1192 5.979 .0001283 4.995e-05 119.6   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   448.6       0          0                 1577 390.5 1465 3.768            1\n\n17:44:43 | running eval: valid\n17:44:44 | eval completed in 0.20s\n17:44:44 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1778       0          0   132   24   1   \n    gpu_mem  llen      loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08111     6 9.392e-05 4.995e-05    72 792.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1577 233.5 2571            1\n\u001b[0m\n17:44:44 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 29\u001b[0m\n17:44:44 | saving model checkpoint: /tmp/model1.checkpoint\n17:44:58 | time:668s total_exs:32300 total_steps:1615 epochs:1345.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.71     0 274.2  1040       0          0 75.85  760   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001461    .1192 5.968 .0001231 4.995e-05 119.4   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   452.7       0          0                 1615 393.6 1493 3.801            1\n\n17:45:04 | time:673s total_exs:32660 total_steps:1633 epochs:1360.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1  11.6     0   272 948.1       0          0 69.71  360   \n    f1  fp16_loss_scalar   gnorm  gpu_mem  llen     loss        lr  ltpb  \\\n     1             32768 .001404    .1192 5.967 .0001183 4.995e-05 119.3   \n    ltps  ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n   415.9       0          0                 1633 391.3 1364 3.501            1\n\n17:45:04 | running eval: valid\n17:45:04 | eval completed in 0.20s\n17:45:04 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1828       0          0 135.8   24   1   \n    gpu_mem  llen      loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08112     6 8.664e-05 4.995e-05    72 814.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1633 233.5 2643            1\n\u001b[0m\n17:45:04 | \u001b[1mdid not beat best accuracy: 1.0 impatience: 30\u001b[0m\n17:45:04 | saving model checkpoint: /tmp/model1.checkpoint\n17:45:09 | ran out of patience! stopping training.\n17:45:17 | \u001b[33mOverriding opt[\"dict_file\"] to /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict (previously: /tmp/model1.dict)\u001b[0m\n17:45:17 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:45:17 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:45:17 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:45:17 | Using CUDA\n17:45:17 | loading dictionary from /tmp/model1.dict\n17:45:17 | num words = 54944\n17:45:22 | Loading existing model parameters from /tmp/model1\n17:45:25 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:45:26 | creating task(s): fromfile:parlaiformat\n17:45:26 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:45:26 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt\n17:45:26 | running eval: valid\n17:45:26 | eval completed in 0.21s\n17:45:26 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1759       0          0 130.7   24   1   \n    gpu_mem  llen      loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1296     6 8.664e-05 4.995e-05    72 784.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1633 233.5 2544            1\n\u001b[0m\n17:45:26 | creating task(s): fromfile:parlaiformat\n17:45:26 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:45:26 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt\n17:45:26 | running eval: test\n17:45:27 | eval completed in 0.20s\n17:45:27 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1801       0          0 133.8   24   1   \n    gpu_mem  llen      loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1296     6 8.664e-05 4.995e-05    72 802.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                   1633 233.5 2604            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:45:28.726033Z","iopub.execute_input":"2022-12-03T17:45:28.726440Z","iopub.status.idle":"2022-12-03T17:45:58.798916Z","shell.execute_reply.started":"2022-12-03T17:45:28.726397Z","shell.execute_reply":"2022-12-03T17:45:58.797732Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"17:45:38 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt)\u001b[0m\n17:45:38 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:45:38 | Using CUDA\n17:45:38 | loading dictionary from /tmp/model1.dict\n17:45:38 | num words = 54944\n17:45:42 | Loading existing model parameters from /tmp/model1\n17:45:49 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:45:50 | Opt:\n17:45:50 |     activation: gelu\n17:45:50 |     adafactor_eps: '[1e-30, 0.001]'\n17:45:50 |     adam_eps: 1e-08\n17:45:50 |     add_p1_after_newln: False\n17:45:50 |     aggregate_micro: False\n17:45:50 |     allow_missing_init_opts: False\n17:45:50 |     area_under_curve_class: None\n17:45:50 |     area_under_curve_digits: -1\n17:45:50 |     attention_dropout: 0.1\n17:45:50 |     batchsize: 40\n17:45:50 |     betas: '[0.9, 0.999]'\n17:45:50 |     bpe_add_prefix_space: None\n17:45:50 |     bpe_debug: False\n17:45:50 |     bpe_dropout: None\n17:45:50 |     bpe_merge: None\n17:45:50 |     bpe_vocab: None\n17:45:50 |     candidates: inline\n17:45:50 |     cap_num_predictions: 100\n17:45:50 |     checkpoint_activations: False\n17:45:50 |     class_weights: None\n17:45:50 |     classes: \"['__notok__', '__ok__']\"\n17:45:50 |     classes_from_file: None\n17:45:50 |     data_parallel: True\n17:45:50 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:45:50 |     datatype: train\n17:45:50 |     delimiter: '\\n'\n17:45:50 |     dict_class: parlai.core.dict:DictionaryAgent\n17:45:50 |     dict_endtoken: __start__\n17:45:50 |     dict_file: /tmp/model1.dict\n17:45:50 |     dict_include_test: False\n17:45:50 |     dict_include_valid: False\n17:45:50 |     dict_initpath: None\n17:45:50 |     dict_language: english\n17:45:50 |     dict_loaded: True\n17:45:50 |     dict_lower: True\n17:45:50 |     dict_max_ngram_size: -1\n17:45:50 |     dict_maxexs: -1\n17:45:50 |     dict_maxtokens: -1\n17:45:50 |     dict_minfreq: 0\n17:45:50 |     dict_nulltoken: __null__\n17:45:50 |     dict_starttoken: __start__\n17:45:50 |     dict_textfields: text,labels\n17:45:50 |     dict_tokenizer: bpe\n17:45:50 |     dict_unktoken: __unk__\n17:45:50 |     display_examples: False\n17:45:50 |     download_path: None\n17:45:50 |     dropout: 0.1\n17:45:50 |     dynamic_batching: None\n17:45:50 |     embedding_projection: random\n17:45:50 |     embedding_size: 768\n17:45:50 |     embedding_type: random\n17:45:50 |     embeddings_scale: False\n17:45:50 |     encode_candidate_vecs: True\n17:45:50 |     encode_candidate_vecs_batchsize: 256\n17:45:50 |     eval_batchsize: None\n17:45:50 |     eval_candidates: inline\n17:45:50 |     eval_dynamic_batching: None\n17:45:50 |     evaltask: None\n17:45:50 |     ffn_size: 3072\n17:45:50 |     final_extra_opt: \n17:45:50 |     fixed_candidate_vecs: reuse\n17:45:50 |     fixed_candidates_path: None\n17:45:50 |     force_fp16_tokens: True\n17:45:50 |     fp16: True\n17:45:50 |     fp16_impl: safe\n17:45:50 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-a.txt\n17:45:50 |     fromfile_datatype_extension: False\n17:45:50 |     gpu: -1\n17:45:50 |     gradient_clip: 0.1\n17:45:50 |     hide_labels: False\n17:45:50 |     history_add_global_end_token: None\n17:45:50 |     history_reversed: False\n17:45:50 |     history_size: 20\n17:45:50 |     ignore_bad_candidates: False\n17:45:50 |     ignore_labels: None\n17:45:50 |     image_cropsize: 224\n17:45:50 |     image_mode: raw\n17:45:50 |     image_size: 256\n17:45:50 |     inference: max\n17:45:50 |     init_model: zoo:pretrained_transformers/bi_model_huge_reddit/model\n17:45:50 |     init_opt: None\n17:45:50 |     interactive_candidates: fixed\n17:45:50 |     interactive_mode: False\n17:45:50 |     invsqrt_lr_decay_gamma: -1\n17:45:50 |     is_debug: False\n17:45:50 |     label_truncate: 72\n17:45:50 |     learn_embeddings: True\n17:45:50 |     learn_positional_embeddings: True\n17:45:50 |     learningrate: 5e-05\n17:45:50 |     load_from_pretrained_ranker: True\n17:45:50 |     log_every_n_secs: 10.0\n17:45:50 |     log_every_n_steps: 50\n17:45:50 |     log_keep_fields: all\n17:45:50 |     loglevel: info\n17:45:50 |     lr_scheduler: reduceonplateau\n17:45:50 |     lr_scheduler_decay: 0.5\n17:45:50 |     lr_scheduler_patience: 3\n17:45:50 |     max_train_steps: -1\n17:45:50 |     max_train_time: 7200.0\n17:45:50 |     memory_attention: sqrt\n17:45:50 |     metrics: default\n17:45:50 |     model: transformer/classifier\n17:45:50 |     model_file: /tmp/model1\n17:45:50 |     model_parallel: False\n17:45:50 |     momentum: 0\n17:45:50 |     multitask_weights: [1]\n17:45:50 |     mutators: None\n17:45:50 |     n_decoder_layers: -1\n17:45:50 |     n_encoder_layers: -1\n17:45:50 |     n_heads: 12\n17:45:50 |     n_layers: 12\n17:45:50 |     n_positions: 1024\n17:45:50 |     n_segments: 2\n17:45:50 |     nesterov: True\n17:45:50 |     no_cuda: False\n17:45:50 |     normalize_sent_emb: False\n17:45:50 |     num_epochs: -1\n17:45:50 |     num_examples: -1\n17:45:50 |     num_workers: 0\n17:45:50 |     nus: [0.7]\n17:45:50 |     optimizer: adamax\n17:45:50 |     output_scaling: 0.06\n17:45:50 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n17:45:50 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:45:50 |     person_tokens: False\n17:45:50 |     print_scores: False\n17:45:50 |     rank_candidates: False\n17:45:50 |     rank_top_k: -1\n17:45:50 |     reduction_type: mean\n17:45:50 |     ref_class: None\n17:45:50 |     relu_dropout: 0.0\n17:45:50 |     repeat_blocking_heuristic: True\n17:45:50 |     report_filename: \n17:45:50 |     return_cand_scores: False\n17:45:50 |     save_after_valid: True\n17:45:50 |     save_every_n_secs: -1\n17:45:50 |     save_format: conversations\n17:45:50 |     share_encoders: False\n17:45:50 |     share_word_embeddings: False\n17:45:50 |     short_final_eval: False\n17:45:50 |     special_tok_lst: None\n17:45:50 |     split_lines: False\n17:45:50 |     starttime: Dec03_17-32\n17:45:50 |     task: fromfile:parlaiformat\n17:45:50 |     tensorboard_log: False\n17:45:50 |     tensorboard_logdir: None\n17:45:50 |     text_truncate: 360\n17:45:50 |     threshold: 0.5\n17:45:50 |     topk: 5\n17:45:50 |     train_predict: False\n17:45:50 |     truncate: 1024\n17:45:50 |     update_classifier_head_only: False\n17:45:50 |     update_freq: 1\n17:45:50 |     use_memories: False\n17:45:50 |     use_reply: none\n17:45:50 |     validation_cutoff: 1.0\n17:45:50 |     validation_every_n_epochs: -1\n17:45:50 |     validation_every_n_secs: 20.0\n17:45:50 |     validation_every_n_steps: -1\n17:45:50 |     validation_max_exs: -1\n17:45:50 |     validation_metric: accuracy\n17:45:50 |     validation_metric_mode: max\n17:45:50 |     validation_patience: 30\n17:45:50 |     validation_share_agent: False\n17:45:50 |     variant: xlm\n17:45:50 |     verbose: False\n17:45:50 |     wandb_entity: None\n17:45:50 |     wandb_log: False\n17:45:50 |     wandb_name: None\n17:45:50 |     wandb_project: None\n17:45:50 |     warmup_rate: 0.0001\n17:45:50 |     warmup_updates: 1000\n17:45:50 |     weight_decay: None\n17:45:50 |     world_logs: \n17:45:50 |     wrap_memory_encoder: False\n17:45:51 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:45:51 | creating task(s): fromfile:parlaiformat\n17:45:51 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:45:51 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-a.txt\n17:45:56 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .0850 8.5e-11               .1073                 .1028   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1122           .06154             .06452   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                 .05882 11.13 525.2 515.9       0          0 39.29  200 .0850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.98  8.38 4.995e-05 239.2   235       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                   1633 764.4 750.8       .08397\u001b[0m\n17:45:56 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .0850 8.5e-11               .1073                 .1028   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1122           .06154             .06452   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                 .05882 11.13 525.2 515.9       0          0 39.29  200 .0850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  5.98  8.38 4.995e-05 239.2   235       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                   1633 764.4 750.8       .08397\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:45:58.800863Z","iopub.execute_input":"2022-12-03T17:45:58.801252Z","iopub.status.idle":"2022-12-03T17:46:25.342515Z","shell.execute_reply.started":"2022-12-03T17:45:58.801221Z","shell.execute_reply":"2022-12-03T17:46:25.341346Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"17:46:06 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_valid.txt)\u001b[0m\n17:46:06 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:46:06 | Using CUDA\n17:46:06 | loading dictionary from /tmp/model1.dict\n17:46:06 | num words = 54944\n17:46:10 | Loading existing model parameters from /tmp/model1\n17:46:16 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:46:17 | Opt:\n17:46:17 |     activation: gelu\n17:46:17 |     adafactor_eps: '[1e-30, 0.001]'\n17:46:17 |     adam_eps: 1e-08\n17:46:17 |     add_p1_after_newln: False\n17:46:17 |     aggregate_micro: False\n17:46:17 |     allow_missing_init_opts: False\n17:46:17 |     area_under_curve_class: None\n17:46:17 |     area_under_curve_digits: -1\n17:46:17 |     attention_dropout: 0.1\n17:46:17 |     batchsize: 40\n17:46:17 |     betas: '[0.9, 0.999]'\n17:46:17 |     bpe_add_prefix_space: None\n17:46:17 |     bpe_debug: False\n17:46:17 |     bpe_dropout: None\n17:46:17 |     bpe_merge: None\n17:46:17 |     bpe_vocab: None\n17:46:17 |     candidates: inline\n17:46:17 |     cap_num_predictions: 100\n17:46:17 |     checkpoint_activations: False\n17:46:17 |     class_weights: None\n17:46:17 |     classes: \"['__notok__', '__ok__']\"\n17:46:17 |     classes_from_file: None\n17:46:17 |     data_parallel: True\n17:46:17 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:46:17 |     datatype: train\n17:46:17 |     delimiter: '\\n'\n17:46:17 |     dict_class: parlai.core.dict:DictionaryAgent\n17:46:17 |     dict_endtoken: __start__\n17:46:17 |     dict_file: /tmp/model1.dict\n17:46:17 |     dict_include_test: False\n17:46:17 |     dict_include_valid: False\n17:46:17 |     dict_initpath: None\n17:46:17 |     dict_language: english\n17:46:17 |     dict_loaded: True\n17:46:17 |     dict_lower: True\n17:46:17 |     dict_max_ngram_size: -1\n17:46:17 |     dict_maxexs: -1\n17:46:17 |     dict_maxtokens: -1\n17:46:17 |     dict_minfreq: 0\n17:46:17 |     dict_nulltoken: __null__\n17:46:17 |     dict_starttoken: __start__\n17:46:17 |     dict_textfields: text,labels\n17:46:17 |     dict_tokenizer: bpe\n17:46:17 |     dict_unktoken: __unk__\n17:46:17 |     display_examples: False\n17:46:17 |     download_path: None\n17:46:17 |     dropout: 0.1\n17:46:17 |     dynamic_batching: None\n17:46:17 |     embedding_projection: random\n17:46:17 |     embedding_size: 768\n17:46:17 |     embedding_type: random\n17:46:17 |     embeddings_scale: False\n17:46:17 |     encode_candidate_vecs: True\n17:46:17 |     encode_candidate_vecs_batchsize: 256\n17:46:17 |     eval_batchsize: None\n17:46:17 |     eval_candidates: inline\n17:46:17 |     eval_dynamic_batching: None\n17:46:17 |     evaltask: None\n17:46:17 |     ffn_size: 3072\n17:46:17 |     final_extra_opt: \n17:46:17 |     fixed_candidate_vecs: reuse\n17:46:17 |     fixed_candidates_path: None\n17:46:17 |     force_fp16_tokens: True\n17:46:17 |     fp16: True\n17:46:17 |     fp16_impl: safe\n17:46:17 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-b.txt\n17:46:17 |     fromfile_datatype_extension: False\n17:46:17 |     gpu: -1\n17:46:17 |     gradient_clip: 0.1\n17:46:17 |     hide_labels: False\n17:46:17 |     history_add_global_end_token: None\n17:46:17 |     history_reversed: False\n17:46:17 |     history_size: 20\n17:46:17 |     ignore_bad_candidates: False\n17:46:17 |     ignore_labels: None\n17:46:17 |     image_cropsize: 224\n17:46:17 |     image_mode: raw\n17:46:17 |     image_size: 256\n17:46:17 |     inference: max\n17:46:17 |     init_model: zoo:pretrained_transformers/bi_model_huge_reddit/model\n17:46:17 |     init_opt: None\n17:46:17 |     interactive_candidates: fixed\n17:46:17 |     interactive_mode: False\n17:46:17 |     invsqrt_lr_decay_gamma: -1\n17:46:17 |     is_debug: False\n17:46:17 |     label_truncate: 72\n17:46:17 |     learn_embeddings: True\n17:46:17 |     learn_positional_embeddings: True\n17:46:17 |     learningrate: 5e-05\n17:46:17 |     load_from_pretrained_ranker: True\n17:46:17 |     log_every_n_secs: 10.0\n17:46:17 |     log_every_n_steps: 50\n17:46:17 |     log_keep_fields: all\n17:46:17 |     loglevel: info\n17:46:17 |     lr_scheduler: reduceonplateau\n17:46:17 |     lr_scheduler_decay: 0.5\n17:46:17 |     lr_scheduler_patience: 3\n17:46:17 |     max_train_steps: -1\n17:46:17 |     max_train_time: 7200.0\n17:46:17 |     memory_attention: sqrt\n17:46:17 |     metrics: default\n17:46:17 |     model: transformer/classifier\n17:46:17 |     model_file: /tmp/model1\n17:46:17 |     model_parallel: False\n17:46:17 |     momentum: 0\n17:46:17 |     multitask_weights: [1]\n17:46:17 |     mutators: None\n17:46:17 |     n_decoder_layers: -1\n17:46:17 |     n_encoder_layers: -1\n17:46:17 |     n_heads: 12\n17:46:17 |     n_layers: 12\n17:46:17 |     n_positions: 1024\n17:46:17 |     n_segments: 2\n17:46:17 |     nesterov: True\n17:46:17 |     no_cuda: False\n17:46:17 |     normalize_sent_emb: False\n17:46:17 |     num_epochs: -1\n17:46:17 |     num_examples: -1\n17:46:17 |     num_workers: 0\n17:46:17 |     nus: [0.7]\n17:46:17 |     optimizer: adamax\n17:46:17 |     output_scaling: 0.06\n17:46:17 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n17:46:17 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:46:17 |     person_tokens: False\n17:46:17 |     print_scores: False\n17:46:17 |     rank_candidates: False\n17:46:17 |     rank_top_k: -1\n17:46:17 |     reduction_type: mean\n17:46:17 |     ref_class: None\n17:46:17 |     relu_dropout: 0.0\n17:46:17 |     repeat_blocking_heuristic: True\n17:46:17 |     report_filename: \n17:46:17 |     return_cand_scores: False\n17:46:17 |     save_after_valid: True\n17:46:17 |     save_every_n_secs: -1\n17:46:17 |     save_format: conversations\n17:46:17 |     share_encoders: False\n17:46:17 |     share_word_embeddings: False\n17:46:17 |     short_final_eval: False\n17:46:17 |     special_tok_lst: None\n17:46:17 |     split_lines: False\n17:46:17 |     starttime: Dec03_17-32\n17:46:17 |     task: fromfile:parlaiformat\n17:46:17 |     tensorboard_log: False\n17:46:17 |     tensorboard_logdir: None\n17:46:17 |     text_truncate: 360\n17:46:17 |     threshold: 0.5\n17:46:17 |     topk: 5\n17:46:17 |     train_predict: False\n17:46:17 |     truncate: 1024\n17:46:17 |     update_classifier_head_only: False\n17:46:17 |     update_freq: 1\n17:46:17 |     use_memories: False\n17:46:17 |     use_reply: none\n17:46:17 |     validation_cutoff: 1.0\n17:46:17 |     validation_every_n_epochs: -1\n17:46:17 |     validation_every_n_secs: 20.0\n17:46:17 |     validation_every_n_steps: -1\n17:46:17 |     validation_max_exs: -1\n17:46:17 |     validation_metric: accuracy\n17:46:17 |     validation_metric_mode: max\n17:46:17 |     validation_patience: 30\n17:46:17 |     validation_share_agent: False\n17:46:17 |     variant: xlm\n17:46:17 |     verbose: False\n17:46:17 |     wandb_entity: None\n17:46:17 |     wandb_log: False\n17:46:17 |     wandb_name: None\n17:46:17 |     wandb_project: None\n17:46:17 |     warmup_rate: 0.0001\n17:46:17 |     warmup_updates: 1000\n17:46:17 |     weight_decay: None\n17:46:17 |     world_logs: \n17:46:17 |     wrap_memory_encoder: False\n17:46:18 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:46:18 | creating task(s): fromfile:parlaiformat\n17:46:18 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:46:18 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run1/data_train-b.txt\n17:46:23 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .9150 9.15e-10               .9187                 .8972   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9412            .9110              .9355   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8878 11.13 525.2 481.7       0          0 36.68  200 .9150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.02 .6415 4.995e-05 240.8 220.8       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                   1633  766 702.5        .9149\u001b[0m\n17:46:23 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .9150 9.15e-10               .9187                 .8972   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9412            .9110              .9355   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8878 11.13 525.2 481.7       0          0 36.68  200 .9150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824  6.02 .6415 4.995e-05 240.8 220.8       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                   1633  766 702.5        .9149\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:46:25.346322Z","iopub.execute_input":"2022-12-03T17:46:25.346674Z","iopub.status.idle":"2022-12-03T17:46:26.517201Z","shell.execute_reply.started":"2022-12-03T17:46:25.346644Z","shell.execute_reply":"2022-12-03T17:46:26.515885Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:46:26.519823Z","iopub.execute_input":"2022-12-03T17:46:26.520222Z","iopub.status.idle":"2022-12-03T17:47:21.588285Z","shell.execute_reply.started":"2022-12-03T17:46:26.520183Z","shell.execute_reply":"2022-12-03T17:47:21.587119Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"17:46:33 | building dictionary first...\n17:46:33 | No model with opt yet at: /tmp/model2(.opt)\n17:46:33 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:46:33 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:46:33 | Using CUDA\n17:46:33 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:46:34 | num words = 54944\n17:46:38 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:46:46 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:46:46 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:46:46 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:46:46 | Opt:\n17:46:46 |     activation: gelu\n17:46:46 |     adafactor_eps: '(1e-30, 0.001)'\n17:46:46 |     adam_eps: 1e-08\n17:46:46 |     add_p1_after_newln: False\n17:46:46 |     aggregate_micro: False\n17:46:46 |     allow_missing_init_opts: False\n17:46:46 |     attention_dropout: 0.1\n17:46:46 |     batchsize: 20\n17:46:46 |     betas: '(0.9, 0.999)'\n17:46:46 |     bpe_add_prefix_space: None\n17:46:46 |     bpe_debug: False\n17:46:46 |     bpe_dropout: None\n17:46:46 |     bpe_merge: None\n17:46:46 |     bpe_vocab: None\n17:46:46 |     candidates: inline\n17:46:46 |     cap_num_predictions: 100\n17:46:46 |     checkpoint_activations: False\n17:46:46 |     class_weights: None\n17:46:46 |     classes: \"['__notok__', '__ok__']\"\n17:46:46 |     classes_from_file: None\n17:46:46 |     data_parallel: True\n17:46:46 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:46:46 |     datatype: train\n17:46:46 |     delimiter: '\\n'\n17:46:46 |     dict_class: parlai.core.dict:DictionaryAgent\n17:46:46 |     dict_endtoken: __start__\n17:46:46 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:46:46 |     dict_include_test: False\n17:46:46 |     dict_include_valid: False\n17:46:46 |     dict_initpath: None\n17:46:46 |     dict_language: english\n17:46:46 |     dict_loaded: True\n17:46:46 |     dict_lower: True\n17:46:46 |     dict_max_ngram_size: -1\n17:46:46 |     dict_maxexs: -1\n17:46:46 |     dict_maxtokens: -1\n17:46:46 |     dict_minfreq: 0\n17:46:46 |     dict_nulltoken: __null__\n17:46:46 |     dict_starttoken: __start__\n17:46:46 |     dict_textfields: text,labels\n17:46:46 |     dict_tokenizer: bpe\n17:46:46 |     dict_unktoken: __unk__\n17:46:46 |     display_examples: False\n17:46:46 |     download_path: None\n17:46:46 |     dropout: 0.1\n17:46:46 |     dynamic_batching: None\n17:46:46 |     embedding_projection: random\n17:46:46 |     embedding_size: 768\n17:46:46 |     embedding_type: random\n17:46:46 |     embeddings_scale: False\n17:46:46 |     encode_candidate_vecs: True\n17:46:46 |     encode_candidate_vecs_batchsize: 256\n17:46:46 |     eval_batchsize: None\n17:46:46 |     eval_candidates: inline\n17:46:46 |     eval_dynamic_batching: None\n17:46:46 |     evaltask: None\n17:46:46 |     ffn_size: 3072\n17:46:46 |     final_extra_opt: \n17:46:46 |     fixed_candidate_vecs: reuse\n17:46:46 |     fixed_candidates_path: None\n17:46:46 |     force_fp16_tokens: False\n17:46:46 |     fp16: True\n17:46:46 |     fp16_impl: safe\n17:46:46 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt\n17:46:46 |     fromfile_datatype_extension: False\n17:46:46 |     gpu: -1\n17:46:46 |     gradient_clip: 0.1\n17:46:46 |     hide_labels: False\n17:46:46 |     history_add_global_end_token: None\n17:46:46 |     history_reversed: False\n17:46:46 |     history_size: 20\n17:46:46 |     ignore_bad_candidates: False\n17:46:46 |     ignore_labels: None\n17:46:46 |     image_cropsize: 224\n17:46:46 |     image_mode: raw\n17:46:46 |     image_size: 256\n17:46:46 |     inference: max\n17:46:46 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:46:46 |     init_opt: None\n17:46:46 |     interactive_candidates: fixed\n17:46:46 |     interactive_mode: False\n17:46:46 |     invsqrt_lr_decay_gamma: -1\n17:46:46 |     is_debug: False\n17:46:46 |     label_truncate: 72\n17:46:46 |     learn_embeddings: True\n17:46:46 |     learn_positional_embeddings: True\n17:46:46 |     learningrate: 5e-05\n17:46:46 |     load_from_checkpoint: False\n17:46:46 |     load_from_pretrained_ranker: True\n17:46:46 |     log_every_n_secs: 10.0\n17:46:46 |     log_every_n_steps: 50\n17:46:46 |     log_keep_fields: all\n17:46:46 |     loglevel: info\n17:46:46 |     lr_scheduler: reduceonplateau\n17:46:46 |     lr_scheduler_decay: 0.5\n17:46:46 |     lr_scheduler_patience: 3\n17:46:46 |     max_train_steps: -1\n17:46:46 |     max_train_time: 7200.0\n17:46:46 |     memory_attention: sqrt\n17:46:46 |     metrics: default\n17:46:46 |     model: transformer/classifier\n17:46:46 |     model_file: /tmp/model2\n17:46:46 |     model_parallel: False\n17:46:46 |     momentum: 0\n17:46:46 |     multitask_weights: [1]\n17:46:46 |     mutators: None\n17:46:46 |     n_decoder_layers: -1\n17:46:46 |     n_encoder_layers: -1\n17:46:46 |     n_heads: 12\n17:46:46 |     n_layers: 12\n17:46:46 |     n_positions: 1024\n17:46:46 |     n_segments: 2\n17:46:46 |     nesterov: True\n17:46:46 |     no_cuda: False\n17:46:46 |     normalize_sent_emb: False\n17:46:46 |     num_epochs: -1\n17:46:46 |     num_workers: 0\n17:46:46 |     nus: (0.7,)\n17:46:46 |     optimizer: adamax\n17:46:46 |     output_scaling: 0.06\n17:46:46 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n17:46:46 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:46:46 |     person_tokens: False\n17:46:46 |     print_scores: False\n17:46:46 |     rank_candidates: False\n17:46:46 |     rank_top_k: -1\n17:46:46 |     reduction_type: mean\n17:46:46 |     ref_class: None\n17:46:46 |     relu_dropout: 0.0\n17:46:46 |     repeat_blocking_heuristic: True\n17:46:46 |     return_cand_scores: False\n17:46:46 |     save_after_valid: True\n17:46:46 |     save_every_n_secs: -1\n17:46:46 |     save_format: conversations\n17:46:46 |     share_encoders: False\n17:46:46 |     share_word_embeddings: False\n17:46:46 |     short_final_eval: False\n17:46:46 |     special_tok_lst: None\n17:46:46 |     split_lines: False\n17:46:46 |     starttime: Dec03_17-46\n17:46:46 |     task: fromfile:parlaiformat\n17:46:46 |     tensorboard_log: False\n17:46:46 |     tensorboard_logdir: None\n17:46:46 |     text_truncate: 360\n17:46:46 |     threshold: 0.5\n17:46:46 |     topk: 5\n17:46:46 |     train_predict: False\n17:46:46 |     truncate: 1024\n17:46:46 |     update_classifier_head_only: False\n17:46:46 |     update_freq: 1\n17:46:46 |     use_memories: False\n17:46:46 |     use_reply: none\n17:46:46 |     validation_cutoff: 1.0\n17:46:46 |     validation_every_n_epochs: -1\n17:46:46 |     validation_every_n_secs: 20.0\n17:46:46 |     validation_every_n_steps: -1\n17:46:46 |     validation_max_exs: -1\n17:46:46 |     validation_metric: accuracy\n17:46:46 |     validation_metric_mode: max\n17:46:46 |     validation_patience: 30\n17:46:46 |     validation_share_agent: False\n17:46:46 |     variant: xlm\n17:46:46 |     verbose: False\n17:46:46 |     wandb_entity: None\n17:46:46 |     wandb_log: False\n17:46:46 |     wandb_name: None\n17:46:46 |     wandb_project: None\n17:46:46 |     warmup_rate: 0.0001\n17:46:46 |     warmup_updates: 1000\n17:46:46 |     weight_decay: None\n17:46:46 |     world_logs: \n17:46:46 |     wrap_memory_encoder: False\n17:46:46 | creating task(s): fromfile:parlaiformat\n17:46:46 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt\n17:46:46 | training...\n17:46:56 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .5600 5.6e-10               .3973                 .7160   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2749            .6535              .5204   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8783 10.98     1 259.6 518.9       0          0 39.97  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5600             32768  2.786    .1206 6.055 .6771 1.005e-06 121.1   242   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 380.8 760.9 2.003        .5184\n\n17:47:06 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8447 8.447e-10               .8212                 .9249   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7384            .8628              .7944   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9440 11.02     1 260.3 985.9       0          0 75.74  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8447             32768  2.662    .1207 5.966 .6188 2.905e-06 119.3 451.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 379.7 1438 3.795        .8427\n\n17:47:07 | creating task(s): fromfile:parlaiformat\n17:47:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:47:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt\n17:47:07 | running eval: valid\n17:47:07 | eval completed in 0.21s\n17:47:07 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1648       0          0 126.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5487 2.905e-06    72 760.7       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2409            1\n\u001b[0m\n17:47:07 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:47:07 | saving best valid model: /tmp/model2\n17:47:07 | Saving dictionary to /tmp/model2.dict\n17:47:11 | task solved! stopping.\n17:47:11 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:47:11 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:47:11 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:47:11 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:47:11 | Using CUDA\n17:47:11 | loading dictionary from /tmp/model2.dict\n17:47:11 | num words = 54944\n17:47:16 | Loading existing model parameters from /tmp/model2\n17:47:18 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:47:19 | creating task(s): fromfile:parlaiformat\n17:47:19 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:47:19 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt\n17:47:19 | running eval: valid\n17:47:19 | eval completed in 0.22s\n17:47:19 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1621       0          0 124.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5487 2.905e-06    72 747.9       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2369            1\n\u001b[0m\n17:47:19 | creating task(s): fromfile:parlaiformat\n17:47:19 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:47:19 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt\n17:47:19 | running eval: test\n17:47:19 | eval completed in 0.21s\n17:47:19 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1615       0          0 124.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5487 2.905e-06    72 745.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2360            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:47:21.590675Z","iopub.execute_input":"2022-12-03T17:47:21.591113Z","iopub.status.idle":"2022-12-03T17:47:49.973954Z","shell.execute_reply.started":"2022-12-03T17:47:21.591070Z","shell.execute_reply":"2022-12-03T17:47:49.972751Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"17:47:29 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt)\u001b[0m\n17:47:29 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:47:29 | Using CUDA\n17:47:29 | loading dictionary from /tmp/model2.dict\n17:47:29 | num words = 54944\n17:47:33 | Loading existing model parameters from /tmp/model2\n17:47:41 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:47:42 | Opt:\n17:47:42 |     activation: gelu\n17:47:42 |     adafactor_eps: '[1e-30, 0.001]'\n17:47:42 |     adam_eps: 1e-08\n17:47:42 |     add_p1_after_newln: False\n17:47:42 |     aggregate_micro: False\n17:47:42 |     allow_missing_init_opts: False\n17:47:42 |     area_under_curve_class: None\n17:47:42 |     area_under_curve_digits: -1\n17:47:42 |     attention_dropout: 0.1\n17:47:42 |     batchsize: 40\n17:47:42 |     betas: '[0.9, 0.999]'\n17:47:42 |     bpe_add_prefix_space: None\n17:47:42 |     bpe_debug: False\n17:47:42 |     bpe_dropout: None\n17:47:42 |     bpe_merge: None\n17:47:42 |     bpe_vocab: None\n17:47:42 |     candidates: inline\n17:47:42 |     cap_num_predictions: 100\n17:47:42 |     checkpoint_activations: False\n17:47:42 |     class_weights: None\n17:47:42 |     classes: \"['__notok__', '__ok__']\"\n17:47:42 |     classes_from_file: None\n17:47:42 |     data_parallel: True\n17:47:42 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:47:42 |     datatype: train\n17:47:42 |     delimiter: '\\n'\n17:47:42 |     dict_class: parlai.core.dict:DictionaryAgent\n17:47:42 |     dict_endtoken: __start__\n17:47:42 |     dict_file: /tmp/model2.dict\n17:47:42 |     dict_include_test: False\n17:47:42 |     dict_include_valid: False\n17:47:42 |     dict_initpath: None\n17:47:42 |     dict_language: english\n17:47:42 |     dict_loaded: True\n17:47:42 |     dict_lower: True\n17:47:42 |     dict_max_ngram_size: -1\n17:47:42 |     dict_maxexs: -1\n17:47:42 |     dict_maxtokens: -1\n17:47:42 |     dict_minfreq: 0\n17:47:42 |     dict_nulltoken: __null__\n17:47:42 |     dict_starttoken: __start__\n17:47:42 |     dict_textfields: text,labels\n17:47:42 |     dict_tokenizer: bpe\n17:47:42 |     dict_unktoken: __unk__\n17:47:42 |     display_examples: False\n17:47:42 |     download_path: None\n17:47:42 |     dropout: 0.1\n17:47:42 |     dynamic_batching: None\n17:47:42 |     embedding_projection: random\n17:47:42 |     embedding_size: 768\n17:47:42 |     embedding_type: random\n17:47:42 |     embeddings_scale: False\n17:47:42 |     encode_candidate_vecs: True\n17:47:42 |     encode_candidate_vecs_batchsize: 256\n17:47:42 |     eval_batchsize: None\n17:47:42 |     eval_candidates: inline\n17:47:42 |     eval_dynamic_batching: None\n17:47:42 |     evaltask: None\n17:47:42 |     ffn_size: 3072\n17:47:42 |     final_extra_opt: \n17:47:42 |     fixed_candidate_vecs: reuse\n17:47:42 |     fixed_candidates_path: None\n17:47:42 |     force_fp16_tokens: True\n17:47:42 |     fp16: True\n17:47:42 |     fp16_impl: safe\n17:47:42 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-a.txt\n17:47:42 |     fromfile_datatype_extension: False\n17:47:42 |     gpu: -1\n17:47:42 |     gradient_clip: 0.1\n17:47:42 |     hide_labels: False\n17:47:42 |     history_add_global_end_token: None\n17:47:42 |     history_reversed: False\n17:47:42 |     history_size: 20\n17:47:42 |     ignore_bad_candidates: False\n17:47:42 |     ignore_labels: None\n17:47:42 |     image_cropsize: 224\n17:47:42 |     image_mode: raw\n17:47:42 |     image_size: 256\n17:47:42 |     inference: max\n17:47:42 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:47:42 |     init_opt: None\n17:47:42 |     interactive_candidates: fixed\n17:47:42 |     interactive_mode: False\n17:47:42 |     invsqrt_lr_decay_gamma: -1\n17:47:42 |     is_debug: False\n17:47:42 |     label_truncate: 72\n17:47:42 |     learn_embeddings: True\n17:47:42 |     learn_positional_embeddings: True\n17:47:42 |     learningrate: 5e-05\n17:47:42 |     load_from_pretrained_ranker: True\n17:47:42 |     log_every_n_secs: 10.0\n17:47:42 |     log_every_n_steps: 50\n17:47:42 |     log_keep_fields: all\n17:47:42 |     loglevel: info\n17:47:42 |     lr_scheduler: reduceonplateau\n17:47:42 |     lr_scheduler_decay: 0.5\n17:47:42 |     lr_scheduler_patience: 3\n17:47:42 |     max_train_steps: -1\n17:47:42 |     max_train_time: 7200.0\n17:47:42 |     memory_attention: sqrt\n17:47:42 |     metrics: default\n17:47:42 |     model: transformer/classifier\n17:47:42 |     model_file: /tmp/model2\n17:47:42 |     model_parallel: False\n17:47:42 |     momentum: 0\n17:47:42 |     multitask_weights: [1]\n17:47:42 |     mutators: None\n17:47:42 |     n_decoder_layers: -1\n17:47:42 |     n_encoder_layers: -1\n17:47:42 |     n_heads: 12\n17:47:42 |     n_layers: 12\n17:47:42 |     n_positions: 1024\n17:47:42 |     n_segments: 2\n17:47:42 |     nesterov: True\n17:47:42 |     no_cuda: False\n17:47:42 |     normalize_sent_emb: False\n17:47:42 |     num_epochs: -1\n17:47:42 |     num_examples: -1\n17:47:42 |     num_workers: 0\n17:47:42 |     nus: [0.7]\n17:47:42 |     optimizer: adamax\n17:47:42 |     output_scaling: 0.06\n17:47:42 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n17:47:42 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:47:42 |     person_tokens: False\n17:47:42 |     print_scores: False\n17:47:42 |     rank_candidates: False\n17:47:42 |     rank_top_k: -1\n17:47:42 |     reduction_type: mean\n17:47:42 |     ref_class: None\n17:47:42 |     relu_dropout: 0.0\n17:47:42 |     repeat_blocking_heuristic: True\n17:47:42 |     report_filename: \n17:47:42 |     return_cand_scores: False\n17:47:42 |     save_after_valid: True\n17:47:42 |     save_every_n_secs: -1\n17:47:42 |     save_format: conversations\n17:47:42 |     share_encoders: False\n17:47:42 |     share_word_embeddings: False\n17:47:42 |     short_final_eval: False\n17:47:42 |     special_tok_lst: None\n17:47:42 |     split_lines: False\n17:47:42 |     starttime: Dec03_17-46\n17:47:42 |     task: fromfile:parlaiformat\n17:47:42 |     tensorboard_log: False\n17:47:42 |     tensorboard_logdir: None\n17:47:42 |     text_truncate: 360\n17:47:42 |     threshold: 0.5\n17:47:42 |     topk: 5\n17:47:42 |     train_predict: False\n17:47:42 |     truncate: 1024\n17:47:42 |     update_classifier_head_only: False\n17:47:42 |     update_freq: 1\n17:47:42 |     use_memories: False\n17:47:42 |     use_reply: none\n17:47:42 |     validation_cutoff: 1.0\n17:47:42 |     validation_every_n_epochs: -1\n17:47:42 |     validation_every_n_secs: 20.0\n17:47:42 |     validation_every_n_steps: -1\n17:47:42 |     validation_max_exs: -1\n17:47:42 |     validation_metric: accuracy\n17:47:42 |     validation_metric_mode: max\n17:47:42 |     validation_patience: 30\n17:47:42 |     validation_share_agent: False\n17:47:42 |     variant: xlm\n17:47:42 |     verbose: False\n17:47:42 |     wandb_entity: None\n17:47:42 |     wandb_log: False\n17:47:42 |     wandb_name: None\n17:47:42 |     wandb_project: None\n17:47:42 |     warmup_rate: 0.0001\n17:47:42 |     warmup_updates: 1000\n17:47:42 |     weight_decay: None\n17:47:42 |     world_logs: \n17:47:42 |     wrap_memory_encoder: False\n17:47:42 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:47:42 | creating task(s): fromfile:parlaiformat\n17:47:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:47:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-a.txt\n17:47:48 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .4000   4e-10               .2857                 .3380   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2474            .4828              .4341   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5437  11.7 547.8 505.3       0          0 36.89  200 .4000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.97 .7184 2.905e-06 238.8 220.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 786.6 725.6        .3872\u001b[0m\n17:47:48 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .4000   4e-10               .2857                 .3380   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2474            .4828              .4341   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5437  11.7 547.8 505.3       0          0 36.89  200 .4000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.97 .7184 2.905e-06 238.8 220.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     58 786.6 725.6        .3872\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:47:49.976096Z","iopub.execute_input":"2022-12-03T17:47:49.976591Z","iopub.status.idle":"2022-12-03T17:48:16.201269Z","shell.execute_reply.started":"2022-12-03T17:47:49.976541Z","shell.execute_reply":"2022-12-03T17:48:16.200025Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"17:47:56 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_valid.txt)\u001b[0m\n17:47:56 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:47:56 | Using CUDA\n17:47:56 | loading dictionary from /tmp/model2.dict\n17:47:57 | num words = 54944\n17:48:01 | Loading existing model parameters from /tmp/model2\n17:48:07 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:48:08 | Opt:\n17:48:08 |     activation: gelu\n17:48:08 |     adafactor_eps: '[1e-30, 0.001]'\n17:48:08 |     adam_eps: 1e-08\n17:48:08 |     add_p1_after_newln: False\n17:48:08 |     aggregate_micro: False\n17:48:08 |     allow_missing_init_opts: False\n17:48:08 |     area_under_curve_class: None\n17:48:08 |     area_under_curve_digits: -1\n17:48:08 |     attention_dropout: 0.1\n17:48:08 |     batchsize: 40\n17:48:08 |     betas: '[0.9, 0.999]'\n17:48:08 |     bpe_add_prefix_space: None\n17:48:08 |     bpe_debug: False\n17:48:08 |     bpe_dropout: None\n17:48:08 |     bpe_merge: None\n17:48:08 |     bpe_vocab: None\n17:48:08 |     candidates: inline\n17:48:08 |     cap_num_predictions: 100\n17:48:08 |     checkpoint_activations: False\n17:48:08 |     class_weights: None\n17:48:08 |     classes: \"['__notok__', '__ok__']\"\n17:48:08 |     classes_from_file: None\n17:48:08 |     data_parallel: True\n17:48:08 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:48:08 |     datatype: train\n17:48:08 |     delimiter: '\\n'\n17:48:08 |     dict_class: parlai.core.dict:DictionaryAgent\n17:48:08 |     dict_endtoken: __start__\n17:48:08 |     dict_file: /tmp/model2.dict\n17:48:08 |     dict_include_test: False\n17:48:08 |     dict_include_valid: False\n17:48:08 |     dict_initpath: None\n17:48:08 |     dict_language: english\n17:48:08 |     dict_loaded: True\n17:48:08 |     dict_lower: True\n17:48:08 |     dict_max_ngram_size: -1\n17:48:08 |     dict_maxexs: -1\n17:48:08 |     dict_maxtokens: -1\n17:48:08 |     dict_minfreq: 0\n17:48:08 |     dict_nulltoken: __null__\n17:48:08 |     dict_starttoken: __start__\n17:48:08 |     dict_textfields: text,labels\n17:48:08 |     dict_tokenizer: bpe\n17:48:08 |     dict_unktoken: __unk__\n17:48:08 |     display_examples: False\n17:48:08 |     download_path: None\n17:48:08 |     dropout: 0.1\n17:48:08 |     dynamic_batching: None\n17:48:08 |     embedding_projection: random\n17:48:08 |     embedding_size: 768\n17:48:08 |     embedding_type: random\n17:48:08 |     embeddings_scale: False\n17:48:08 |     encode_candidate_vecs: True\n17:48:08 |     encode_candidate_vecs_batchsize: 256\n17:48:08 |     eval_batchsize: None\n17:48:08 |     eval_candidates: inline\n17:48:08 |     eval_dynamic_batching: None\n17:48:08 |     evaltask: None\n17:48:08 |     ffn_size: 3072\n17:48:08 |     final_extra_opt: \n17:48:08 |     fixed_candidate_vecs: reuse\n17:48:08 |     fixed_candidates_path: None\n17:48:08 |     force_fp16_tokens: True\n17:48:08 |     fp16: True\n17:48:08 |     fp16_impl: safe\n17:48:08 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-b.txt\n17:48:08 |     fromfile_datatype_extension: False\n17:48:08 |     gpu: -1\n17:48:08 |     gradient_clip: 0.1\n17:48:08 |     hide_labels: False\n17:48:08 |     history_add_global_end_token: None\n17:48:08 |     history_reversed: False\n17:48:08 |     history_size: 20\n17:48:08 |     ignore_bad_candidates: False\n17:48:08 |     ignore_labels: None\n17:48:08 |     image_cropsize: 224\n17:48:08 |     image_mode: raw\n17:48:08 |     image_size: 256\n17:48:08 |     inference: max\n17:48:08 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:48:08 |     init_opt: None\n17:48:08 |     interactive_candidates: fixed\n17:48:08 |     interactive_mode: False\n17:48:08 |     invsqrt_lr_decay_gamma: -1\n17:48:08 |     is_debug: False\n17:48:08 |     label_truncate: 72\n17:48:08 |     learn_embeddings: True\n17:48:08 |     learn_positional_embeddings: True\n17:48:08 |     learningrate: 5e-05\n17:48:08 |     load_from_pretrained_ranker: True\n17:48:08 |     log_every_n_secs: 10.0\n17:48:08 |     log_every_n_steps: 50\n17:48:08 |     log_keep_fields: all\n17:48:08 |     loglevel: info\n17:48:08 |     lr_scheduler: reduceonplateau\n17:48:08 |     lr_scheduler_decay: 0.5\n17:48:08 |     lr_scheduler_patience: 3\n17:48:08 |     max_train_steps: -1\n17:48:08 |     max_train_time: 7200.0\n17:48:08 |     memory_attention: sqrt\n17:48:08 |     metrics: default\n17:48:08 |     model: transformer/classifier\n17:48:08 |     model_file: /tmp/model2\n17:48:08 |     model_parallel: False\n17:48:08 |     momentum: 0\n17:48:08 |     multitask_weights: [1]\n17:48:08 |     mutators: None\n17:48:08 |     n_decoder_layers: -1\n17:48:08 |     n_encoder_layers: -1\n17:48:08 |     n_heads: 12\n17:48:08 |     n_layers: 12\n17:48:08 |     n_positions: 1024\n17:48:08 |     n_segments: 2\n17:48:08 |     nesterov: True\n17:48:08 |     no_cuda: False\n17:48:08 |     normalize_sent_emb: False\n17:48:08 |     num_epochs: -1\n17:48:08 |     num_examples: -1\n17:48:08 |     num_workers: 0\n17:48:08 |     nus: [0.7]\n17:48:08 |     optimizer: adamax\n17:48:08 |     output_scaling: 0.06\n17:48:08 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n17:48:08 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:48:08 |     person_tokens: False\n17:48:08 |     print_scores: False\n17:48:08 |     rank_candidates: False\n17:48:08 |     rank_top_k: -1\n17:48:08 |     reduction_type: mean\n17:48:08 |     ref_class: None\n17:48:08 |     relu_dropout: 0.0\n17:48:08 |     repeat_blocking_heuristic: True\n17:48:08 |     report_filename: \n17:48:08 |     return_cand_scores: False\n17:48:08 |     save_after_valid: True\n17:48:08 |     save_every_n_secs: -1\n17:48:08 |     save_format: conversations\n17:48:08 |     share_encoders: False\n17:48:08 |     share_word_embeddings: False\n17:48:08 |     short_final_eval: False\n17:48:08 |     special_tok_lst: None\n17:48:08 |     split_lines: False\n17:48:08 |     starttime: Dec03_17-46\n17:48:08 |     task: fromfile:parlaiformat\n17:48:08 |     tensorboard_log: False\n17:48:08 |     tensorboard_logdir: None\n17:48:08 |     text_truncate: 360\n17:48:08 |     threshold: 0.5\n17:48:08 |     topk: 5\n17:48:08 |     train_predict: False\n17:48:08 |     truncate: 1024\n17:48:08 |     update_classifier_head_only: False\n17:48:08 |     update_freq: 1\n17:48:08 |     use_memories: False\n17:48:08 |     use_reply: none\n17:48:08 |     validation_cutoff: 1.0\n17:48:08 |     validation_every_n_epochs: -1\n17:48:08 |     validation_every_n_secs: 20.0\n17:48:08 |     validation_every_n_steps: -1\n17:48:08 |     validation_max_exs: -1\n17:48:08 |     validation_metric: accuracy\n17:48:08 |     validation_metric_mode: max\n17:48:08 |     validation_patience: 30\n17:48:08 |     validation_share_agent: False\n17:48:08 |     variant: xlm\n17:48:08 |     verbose: False\n17:48:08 |     wandb_entity: None\n17:48:08 |     wandb_log: False\n17:48:08 |     wandb_name: None\n17:48:08 |     wandb_project: None\n17:48:08 |     warmup_rate: 0.0001\n17:48:08 |     warmup_updates: 1000\n17:48:08 |     weight_decay: None\n17:48:08 |     world_logs: \n17:48:08 |     wrap_memory_encoder: False\n17:48:08 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:48:08 | creating task(s): fromfile:parlaiformat\n17:48:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:48:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run2/data_train-b.txt\n17:48:14 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6000   6e-10               .5402                 .6620   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4563            .6460              .5659   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7526  11.7 547.8 481.3       0          0 35.14  200 .6000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.03 .6749 2.905e-06 241.2 211.9       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  789 693.3        .5915\u001b[0m\n17:48:14 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6000   6e-10               .5402                 .6620   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4563            .6460              .5659   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7526  11.7 547.8 481.3       0          0 35.14  200 .6000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.03 .6749 2.905e-06 241.2 211.9       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  789 693.3        .5915\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:48:16.203311Z","iopub.execute_input":"2022-12-03T17:48:16.203738Z","iopub.status.idle":"2022-12-03T17:48:17.309981Z","shell.execute_reply.started":"2022-12-03T17:48:16.203697Z","shell.execute_reply":"2022-12-03T17:48:17.308596Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:48:17.311921Z","iopub.execute_input":"2022-12-03T17:48:17.312350Z","iopub.status.idle":"2022-12-03T17:49:34.268888Z","shell.execute_reply.started":"2022-12-03T17:48:17.312307Z","shell.execute_reply":"2022-12-03T17:49:34.267622Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"17:48:24 | building dictionary first...\n17:48:24 | No model with opt yet at: /tmp/model3(.opt)\n17:48:24 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:48:24 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:48:24 | Using CUDA\n17:48:24 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:48:24 | num words = 54944\n17:48:28 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:48:35 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:48:35 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:48:35 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:48:35 | Opt:\n17:48:35 |     activation: gelu\n17:48:35 |     adafactor_eps: '(1e-30, 0.001)'\n17:48:35 |     adam_eps: 1e-08\n17:48:35 |     add_p1_after_newln: False\n17:48:35 |     aggregate_micro: False\n17:48:35 |     allow_missing_init_opts: False\n17:48:35 |     attention_dropout: 0.1\n17:48:35 |     batchsize: 20\n17:48:35 |     betas: '(0.9, 0.999)'\n17:48:35 |     bpe_add_prefix_space: None\n17:48:35 |     bpe_debug: False\n17:48:35 |     bpe_dropout: None\n17:48:35 |     bpe_merge: None\n17:48:35 |     bpe_vocab: None\n17:48:35 |     candidates: inline\n17:48:35 |     cap_num_predictions: 100\n17:48:35 |     checkpoint_activations: False\n17:48:35 |     class_weights: None\n17:48:35 |     classes: \"['__notok__', '__ok__']\"\n17:48:35 |     classes_from_file: None\n17:48:35 |     data_parallel: True\n17:48:35 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:48:35 |     datatype: train\n17:48:35 |     delimiter: '\\n'\n17:48:35 |     dict_class: parlai.core.dict:DictionaryAgent\n17:48:35 |     dict_endtoken: __start__\n17:48:35 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:48:35 |     dict_include_test: False\n17:48:35 |     dict_include_valid: False\n17:48:35 |     dict_initpath: None\n17:48:35 |     dict_language: english\n17:48:35 |     dict_loaded: True\n17:48:35 |     dict_lower: True\n17:48:35 |     dict_max_ngram_size: -1\n17:48:35 |     dict_maxexs: -1\n17:48:35 |     dict_maxtokens: -1\n17:48:35 |     dict_minfreq: 0\n17:48:35 |     dict_nulltoken: __null__\n17:48:35 |     dict_starttoken: __start__\n17:48:35 |     dict_textfields: text,labels\n17:48:35 |     dict_tokenizer: bpe\n17:48:35 |     dict_unktoken: __unk__\n17:48:35 |     display_examples: False\n17:48:35 |     download_path: None\n17:48:35 |     dropout: 0.1\n17:48:35 |     dynamic_batching: None\n17:48:35 |     embedding_projection: random\n17:48:35 |     embedding_size: 768\n17:48:35 |     embedding_type: random\n17:48:35 |     embeddings_scale: False\n17:48:35 |     encode_candidate_vecs: True\n17:48:35 |     encode_candidate_vecs_batchsize: 256\n17:48:35 |     eval_batchsize: None\n17:48:35 |     eval_candidates: inline\n17:48:35 |     eval_dynamic_batching: None\n17:48:35 |     evaltask: None\n17:48:35 |     ffn_size: 3072\n17:48:35 |     final_extra_opt: \n17:48:35 |     fixed_candidate_vecs: reuse\n17:48:35 |     fixed_candidates_path: None\n17:48:35 |     force_fp16_tokens: False\n17:48:35 |     fp16: True\n17:48:35 |     fp16_impl: safe\n17:48:35 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt\n17:48:35 |     fromfile_datatype_extension: False\n17:48:35 |     gpu: -1\n17:48:35 |     gradient_clip: 0.1\n17:48:35 |     hide_labels: False\n17:48:35 |     history_add_global_end_token: None\n17:48:35 |     history_reversed: False\n17:48:35 |     history_size: 20\n17:48:35 |     ignore_bad_candidates: False\n17:48:35 |     ignore_labels: None\n17:48:35 |     image_cropsize: 224\n17:48:35 |     image_mode: raw\n17:48:35 |     image_size: 256\n17:48:35 |     inference: max\n17:48:35 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:48:35 |     init_opt: None\n17:48:35 |     interactive_candidates: fixed\n17:48:35 |     interactive_mode: False\n17:48:35 |     invsqrt_lr_decay_gamma: -1\n17:48:35 |     is_debug: False\n17:48:35 |     label_truncate: 72\n17:48:35 |     learn_embeddings: True\n17:48:35 |     learn_positional_embeddings: True\n17:48:35 |     learningrate: 5e-05\n17:48:35 |     load_from_checkpoint: False\n17:48:35 |     load_from_pretrained_ranker: True\n17:48:35 |     log_every_n_secs: 10.0\n17:48:35 |     log_every_n_steps: 50\n17:48:35 |     log_keep_fields: all\n17:48:35 |     loglevel: info\n17:48:35 |     lr_scheduler: reduceonplateau\n17:48:35 |     lr_scheduler_decay: 0.5\n17:48:35 |     lr_scheduler_patience: 3\n17:48:35 |     max_train_steps: -1\n17:48:35 |     max_train_time: 7200.0\n17:48:35 |     memory_attention: sqrt\n17:48:35 |     metrics: default\n17:48:35 |     model: transformer/classifier\n17:48:35 |     model_file: /tmp/model3\n17:48:35 |     model_parallel: False\n17:48:35 |     momentum: 0\n17:48:35 |     multitask_weights: [1]\n17:48:35 |     mutators: None\n17:48:35 |     n_decoder_layers: -1\n17:48:35 |     n_encoder_layers: -1\n17:48:35 |     n_heads: 12\n17:48:35 |     n_layers: 12\n17:48:35 |     n_positions: 1024\n17:48:35 |     n_segments: 2\n17:48:35 |     nesterov: True\n17:48:35 |     no_cuda: False\n17:48:35 |     normalize_sent_emb: False\n17:48:35 |     num_epochs: -1\n17:48:35 |     num_workers: 0\n17:48:35 |     nus: (0.7,)\n17:48:35 |     optimizer: adamax\n17:48:35 |     output_scaling: 0.06\n17:48:35 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n17:48:35 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:48:35 |     person_tokens: False\n17:48:35 |     print_scores: False\n17:48:35 |     rank_candidates: False\n17:48:35 |     rank_top_k: -1\n17:48:35 |     reduction_type: mean\n17:48:35 |     ref_class: None\n17:48:35 |     relu_dropout: 0.0\n17:48:35 |     repeat_blocking_heuristic: True\n17:48:35 |     return_cand_scores: False\n17:48:35 |     save_after_valid: True\n17:48:35 |     save_every_n_secs: -1\n17:48:35 |     save_format: conversations\n17:48:35 |     share_encoders: False\n17:48:35 |     share_word_embeddings: False\n17:48:35 |     short_final_eval: False\n17:48:35 |     special_tok_lst: None\n17:48:35 |     split_lines: False\n17:48:35 |     starttime: Dec03_17-48\n17:48:35 |     task: fromfile:parlaiformat\n17:48:35 |     tensorboard_log: False\n17:48:35 |     tensorboard_logdir: None\n17:48:35 |     text_truncate: 360\n17:48:35 |     threshold: 0.5\n17:48:35 |     topk: 5\n17:48:35 |     train_predict: False\n17:48:35 |     truncate: 1024\n17:48:35 |     update_classifier_head_only: False\n17:48:35 |     update_freq: 1\n17:48:35 |     use_memories: False\n17:48:35 |     use_reply: none\n17:48:35 |     validation_cutoff: 1.0\n17:48:35 |     validation_every_n_epochs: -1\n17:48:35 |     validation_every_n_secs: 20.0\n17:48:35 |     validation_every_n_steps: -1\n17:48:35 |     validation_max_exs: -1\n17:48:35 |     validation_metric: accuracy\n17:48:35 |     validation_metric_mode: max\n17:48:35 |     validation_patience: 30\n17:48:35 |     validation_share_agent: False\n17:48:35 |     variant: xlm\n17:48:35 |     verbose: False\n17:48:35 |     wandb_entity: None\n17:48:35 |     wandb_log: False\n17:48:35 |     wandb_name: None\n17:48:35 |     wandb_project: None\n17:48:35 |     warmup_rate: 0.0001\n17:48:35 |     warmup_updates: 1000\n17:48:35 |     weight_decay: None\n17:48:35 |     world_logs: \n17:48:35 |     wrap_memory_encoder: False\n17:48:35 | creating task(s): fromfile:parlaiformat\n17:48:35 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt\n17:48:35 | training...\n17:48:46 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5325 5.325e-10               .3297                 .5610   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2335            .6411              .5252   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8227 11.85     1 276.9 541.9       0          0 39.14  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5325             32768  2.694    .1206 5.985 .6862 1.005e-06 119.7 234.2   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 396.6 776.2 1.961        .4877\n\n17:48:55 | time:20s total_exs:1120 total_steps:56 epochs:46.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8431 8.431e-10               .8198                 .9312   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7322            .8610              .7883   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9485 11.79     1 275.7  1037       0          0 75.21  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8431             32768  3.087    .1207 5.975 .6245 2.805e-06 119.5 449.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   56 395.2 1486 3.769        .8409\n\n17:48:55 | creating task(s): fromfile:parlaiformat\n17:48:55 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:48:55 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt\n17:48:55 | running eval: valid\n17:48:56 | eval completed in 0.20s\n17:48:56 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9565                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9167            .9600              .9231   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                      1 11.71 164.5  1792       0          0 130.7   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5534 2.805e-06    72 784.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     56 236.5 2576        .9583\n\u001b[0m\n17:48:56 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n17:48:56 | saving best valid model: /tmp/model3\n17:48:56 | Saving dictionary to /tmp/model3.dict\n17:48:59 | saving model checkpoint: /tmp/model3.checkpoint\n17:48:59 | Saving dictionary to /tmp/model3.checkpoint.dict\n17:49:16 | time:41s total_exs:1800 total_steps:90 epochs:75.00\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9912 9.912e-10               .9908                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9818            .9915              .9831   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.47     1 269.4   909       0          0 67.47  680   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9912             32768  3.219    .1207 5.971 .4637 4.505e-06 119.4 402.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   90 388.9 1312 3.381        .9912\n\n17:49:19 | time:44s total_exs:2040 total_steps:102 epochs:85.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.72     1 274.3  1051       0          0 76.64  240   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  3.023    .1207   6.1 .2986 5.104e-06   122 467.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                  102 396.3 1519 3.86            1\n\n17:49:19 | running eval: valid\n17:49:19 | eval completed in 0.20s\n17:49:19 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1777       0          0 129.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08091     6 .2253 5.104e-06    72 777.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    102 236.5 2555            1\n\u001b[0m\n17:49:19 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n17:49:19 | saving best valid model: /tmp/model3\n17:49:24 | task solved! stopping.\n17:49:24 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:49:24 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:49:24 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:49:24 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:49:24 | Using CUDA\n17:49:24 | loading dictionary from /tmp/model3.dict\n17:49:24 | num words = 54944\n17:49:29 | Loading existing model parameters from /tmp/model3\n17:49:30 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:49:32 | creating task(s): fromfile:parlaiformat\n17:49:32 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:49:32 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt\n17:49:32 | running eval: valid\n17:49:32 | eval completed in 0.21s\n17:49:32 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1743       0          0 127.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2253 5.104e-06    72 762.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    102 236.5 2506            1\n\u001b[0m\n17:49:32 | creating task(s): fromfile:parlaiformat\n17:49:32 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:49:32 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt\n17:49:32 | running eval: test\n17:49:32 | eval completed in 0.21s\n17:49:32 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1760       0          0 128.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2253 5.104e-06    72 770.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    102 236.5 2531            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:49:34.273769Z","iopub.execute_input":"2022-12-03T17:49:34.274073Z","iopub.status.idle":"2022-12-03T17:50:01.547529Z","shell.execute_reply.started":"2022-12-03T17:49:34.274046Z","shell.execute_reply":"2022-12-03T17:50:01.546271Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"17:49:41 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt)\u001b[0m\n17:49:41 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:49:41 | Using CUDA\n17:49:41 | loading dictionary from /tmp/model3.dict\n17:49:41 | num words = 54944\n17:49:45 | Loading existing model parameters from /tmp/model3\n17:49:52 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:49:53 | Opt:\n17:49:53 |     activation: gelu\n17:49:53 |     adafactor_eps: '[1e-30, 0.001]'\n17:49:53 |     adam_eps: 1e-08\n17:49:53 |     add_p1_after_newln: False\n17:49:53 |     aggregate_micro: False\n17:49:53 |     allow_missing_init_opts: False\n17:49:53 |     area_under_curve_class: None\n17:49:53 |     area_under_curve_digits: -1\n17:49:53 |     attention_dropout: 0.1\n17:49:53 |     batchsize: 40\n17:49:53 |     betas: '[0.9, 0.999]'\n17:49:53 |     bpe_add_prefix_space: None\n17:49:53 |     bpe_debug: False\n17:49:53 |     bpe_dropout: None\n17:49:53 |     bpe_merge: None\n17:49:53 |     bpe_vocab: None\n17:49:53 |     candidates: inline\n17:49:53 |     cap_num_predictions: 100\n17:49:53 |     checkpoint_activations: False\n17:49:53 |     class_weights: None\n17:49:53 |     classes: \"['__notok__', '__ok__']\"\n17:49:53 |     classes_from_file: None\n17:49:53 |     data_parallel: True\n17:49:53 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:49:53 |     datatype: train\n17:49:53 |     delimiter: '\\n'\n17:49:53 |     dict_class: parlai.core.dict:DictionaryAgent\n17:49:53 |     dict_endtoken: __start__\n17:49:53 |     dict_file: /tmp/model3.dict\n17:49:53 |     dict_include_test: False\n17:49:53 |     dict_include_valid: False\n17:49:53 |     dict_initpath: None\n17:49:53 |     dict_language: english\n17:49:53 |     dict_loaded: True\n17:49:53 |     dict_lower: True\n17:49:53 |     dict_max_ngram_size: -1\n17:49:53 |     dict_maxexs: -1\n17:49:53 |     dict_maxtokens: -1\n17:49:53 |     dict_minfreq: 0\n17:49:53 |     dict_nulltoken: __null__\n17:49:53 |     dict_starttoken: __start__\n17:49:53 |     dict_textfields: text,labels\n17:49:53 |     dict_tokenizer: bpe\n17:49:53 |     dict_unktoken: __unk__\n17:49:53 |     display_examples: False\n17:49:53 |     download_path: None\n17:49:53 |     dropout: 0.1\n17:49:53 |     dynamic_batching: None\n17:49:53 |     embedding_projection: random\n17:49:53 |     embedding_size: 768\n17:49:53 |     embedding_type: random\n17:49:53 |     embeddings_scale: False\n17:49:53 |     encode_candidate_vecs: True\n17:49:53 |     encode_candidate_vecs_batchsize: 256\n17:49:53 |     eval_batchsize: None\n17:49:53 |     eval_candidates: inline\n17:49:53 |     eval_dynamic_batching: None\n17:49:53 |     evaltask: None\n17:49:53 |     ffn_size: 3072\n17:49:53 |     final_extra_opt: \n17:49:53 |     fixed_candidate_vecs: reuse\n17:49:53 |     fixed_candidates_path: None\n17:49:53 |     force_fp16_tokens: True\n17:49:53 |     fp16: True\n17:49:53 |     fp16_impl: safe\n17:49:53 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-a.txt\n17:49:53 |     fromfile_datatype_extension: False\n17:49:53 |     gpu: -1\n17:49:53 |     gradient_clip: 0.1\n17:49:53 |     hide_labels: False\n17:49:53 |     history_add_global_end_token: None\n17:49:53 |     history_reversed: False\n17:49:53 |     history_size: 20\n17:49:53 |     ignore_bad_candidates: False\n17:49:53 |     ignore_labels: None\n17:49:53 |     image_cropsize: 224\n17:49:53 |     image_mode: raw\n17:49:53 |     image_size: 256\n17:49:53 |     inference: max\n17:49:53 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:49:53 |     init_opt: None\n17:49:53 |     interactive_candidates: fixed\n17:49:53 |     interactive_mode: False\n17:49:53 |     invsqrt_lr_decay_gamma: -1\n17:49:53 |     is_debug: False\n17:49:53 |     label_truncate: 72\n17:49:53 |     learn_embeddings: True\n17:49:53 |     learn_positional_embeddings: True\n17:49:53 |     learningrate: 5e-05\n17:49:53 |     load_from_pretrained_ranker: True\n17:49:53 |     log_every_n_secs: 10.0\n17:49:53 |     log_every_n_steps: 50\n17:49:53 |     log_keep_fields: all\n17:49:53 |     loglevel: info\n17:49:53 |     lr_scheduler: reduceonplateau\n17:49:53 |     lr_scheduler_decay: 0.5\n17:49:53 |     lr_scheduler_patience: 3\n17:49:53 |     max_train_steps: -1\n17:49:53 |     max_train_time: 7200.0\n17:49:53 |     memory_attention: sqrt\n17:49:53 |     metrics: default\n17:49:53 |     model: transformer/classifier\n17:49:53 |     model_file: /tmp/model3\n17:49:53 |     model_parallel: False\n17:49:53 |     momentum: 0\n17:49:53 |     multitask_weights: [1]\n17:49:53 |     mutators: None\n17:49:53 |     n_decoder_layers: -1\n17:49:53 |     n_encoder_layers: -1\n17:49:53 |     n_heads: 12\n17:49:53 |     n_layers: 12\n17:49:53 |     n_positions: 1024\n17:49:53 |     n_segments: 2\n17:49:53 |     nesterov: True\n17:49:53 |     no_cuda: False\n17:49:53 |     normalize_sent_emb: False\n17:49:53 |     num_epochs: -1\n17:49:53 |     num_examples: -1\n17:49:53 |     num_workers: 0\n17:49:53 |     nus: [0.7]\n17:49:53 |     optimizer: adamax\n17:49:53 |     output_scaling: 0.06\n17:49:53 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n17:49:53 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:49:53 |     person_tokens: False\n17:49:53 |     print_scores: False\n17:49:53 |     rank_candidates: False\n17:49:53 |     rank_top_k: -1\n17:49:53 |     reduction_type: mean\n17:49:53 |     ref_class: None\n17:49:53 |     relu_dropout: 0.0\n17:49:53 |     repeat_blocking_heuristic: True\n17:49:53 |     report_filename: \n17:49:53 |     return_cand_scores: False\n17:49:53 |     save_after_valid: True\n17:49:53 |     save_every_n_secs: -1\n17:49:53 |     save_format: conversations\n17:49:53 |     share_encoders: False\n17:49:53 |     share_word_embeddings: False\n17:49:53 |     short_final_eval: False\n17:49:53 |     special_tok_lst: None\n17:49:53 |     split_lines: False\n17:49:53 |     starttime: Dec03_17-48\n17:49:53 |     task: fromfile:parlaiformat\n17:49:53 |     tensorboard_log: False\n17:49:53 |     tensorboard_logdir: None\n17:49:53 |     text_truncate: 360\n17:49:53 |     threshold: 0.5\n17:49:53 |     topk: 5\n17:49:53 |     train_predict: False\n17:49:53 |     truncate: 1024\n17:49:53 |     update_classifier_head_only: False\n17:49:53 |     update_freq: 1\n17:49:53 |     use_memories: False\n17:49:53 |     use_reply: none\n17:49:53 |     validation_cutoff: 1.0\n17:49:53 |     validation_every_n_epochs: -1\n17:49:53 |     validation_every_n_secs: 20.0\n17:49:53 |     validation_every_n_steps: -1\n17:49:53 |     validation_max_exs: -1\n17:49:53 |     validation_metric: accuracy\n17:49:53 |     validation_metric_mode: max\n17:49:53 |     validation_patience: 30\n17:49:53 |     validation_share_agent: False\n17:49:53 |     variant: xlm\n17:49:53 |     verbose: False\n17:49:53 |     wandb_entity: None\n17:49:53 |     wandb_log: False\n17:49:53 |     wandb_name: None\n17:49:53 |     wandb_project: None\n17:49:53 |     warmup_rate: 0.0001\n17:49:53 |     warmup_updates: 1000\n17:49:53 |     weight_decay: None\n17:49:53 |     world_logs: \n17:49:53 |     wrap_memory_encoder: False\n17:49:54 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:49:54 | creating task(s): fromfile:parlaiformat\n17:49:54 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:49:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-a.txt\n17:49:59 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1050 1.05e-10              .07254                .07527   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .0700            .1353              .1308   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1400 11.47   539 500.9       0          0 37.17  200 .1050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 1.105 5.104e-06   240   223       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    102  779 723.9        .1039\u001b[0m\n17:49:59 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1050 1.05e-10              .07254                .07527   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .0700            .1353              .1308   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1400 11.47   539 500.9       0          0 37.17  200 .1050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 1.105 5.104e-06   240   223       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    102  779 723.9        .1039\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:50:01.549641Z","iopub.execute_input":"2022-12-03T17:50:01.550001Z","iopub.status.idle":"2022-12-03T17:50:27.878968Z","shell.execute_reply.started":"2022-12-03T17:50:01.549972Z","shell.execute_reply":"2022-12-03T17:50:27.877713Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"17:50:08 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_valid.txt)\u001b[0m\n17:50:08 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:50:08 | Using CUDA\n17:50:08 | loading dictionary from /tmp/model3.dict\n17:50:08 | num words = 54944\n17:50:13 | Loading existing model parameters from /tmp/model3\n17:50:19 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:50:20 | Opt:\n17:50:20 |     activation: gelu\n17:50:20 |     adafactor_eps: '[1e-30, 0.001]'\n17:50:20 |     adam_eps: 1e-08\n17:50:20 |     add_p1_after_newln: False\n17:50:20 |     aggregate_micro: False\n17:50:20 |     allow_missing_init_opts: False\n17:50:20 |     area_under_curve_class: None\n17:50:20 |     area_under_curve_digits: -1\n17:50:20 |     attention_dropout: 0.1\n17:50:20 |     batchsize: 40\n17:50:20 |     betas: '[0.9, 0.999]'\n17:50:20 |     bpe_add_prefix_space: None\n17:50:20 |     bpe_debug: False\n17:50:20 |     bpe_dropout: None\n17:50:20 |     bpe_merge: None\n17:50:20 |     bpe_vocab: None\n17:50:20 |     candidates: inline\n17:50:20 |     cap_num_predictions: 100\n17:50:20 |     checkpoint_activations: False\n17:50:20 |     class_weights: None\n17:50:20 |     classes: \"['__notok__', '__ok__']\"\n17:50:20 |     classes_from_file: None\n17:50:20 |     data_parallel: True\n17:50:20 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:50:20 |     datatype: train\n17:50:20 |     delimiter: '\\n'\n17:50:20 |     dict_class: parlai.core.dict:DictionaryAgent\n17:50:20 |     dict_endtoken: __start__\n17:50:20 |     dict_file: /tmp/model3.dict\n17:50:20 |     dict_include_test: False\n17:50:20 |     dict_include_valid: False\n17:50:20 |     dict_initpath: None\n17:50:20 |     dict_language: english\n17:50:20 |     dict_loaded: True\n17:50:20 |     dict_lower: True\n17:50:20 |     dict_max_ngram_size: -1\n17:50:20 |     dict_maxexs: -1\n17:50:20 |     dict_maxtokens: -1\n17:50:20 |     dict_minfreq: 0\n17:50:20 |     dict_nulltoken: __null__\n17:50:20 |     dict_starttoken: __start__\n17:50:20 |     dict_textfields: text,labels\n17:50:20 |     dict_tokenizer: bpe\n17:50:20 |     dict_unktoken: __unk__\n17:50:20 |     display_examples: False\n17:50:20 |     download_path: None\n17:50:20 |     dropout: 0.1\n17:50:20 |     dynamic_batching: None\n17:50:20 |     embedding_projection: random\n17:50:20 |     embedding_size: 768\n17:50:20 |     embedding_type: random\n17:50:20 |     embeddings_scale: False\n17:50:20 |     encode_candidate_vecs: True\n17:50:20 |     encode_candidate_vecs_batchsize: 256\n17:50:20 |     eval_batchsize: None\n17:50:20 |     eval_candidates: inline\n17:50:20 |     eval_dynamic_batching: None\n17:50:20 |     evaltask: None\n17:50:20 |     ffn_size: 3072\n17:50:20 |     final_extra_opt: \n17:50:20 |     fixed_candidate_vecs: reuse\n17:50:20 |     fixed_candidates_path: None\n17:50:20 |     force_fp16_tokens: True\n17:50:20 |     fp16: True\n17:50:20 |     fp16_impl: safe\n17:50:20 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-b.txt\n17:50:20 |     fromfile_datatype_extension: False\n17:50:20 |     gpu: -1\n17:50:20 |     gradient_clip: 0.1\n17:50:20 |     hide_labels: False\n17:50:20 |     history_add_global_end_token: None\n17:50:20 |     history_reversed: False\n17:50:20 |     history_size: 20\n17:50:20 |     ignore_bad_candidates: False\n17:50:20 |     ignore_labels: None\n17:50:20 |     image_cropsize: 224\n17:50:20 |     image_mode: raw\n17:50:20 |     image_size: 256\n17:50:20 |     inference: max\n17:50:20 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:50:20 |     init_opt: None\n17:50:20 |     interactive_candidates: fixed\n17:50:20 |     interactive_mode: False\n17:50:20 |     invsqrt_lr_decay_gamma: -1\n17:50:20 |     is_debug: False\n17:50:20 |     label_truncate: 72\n17:50:20 |     learn_embeddings: True\n17:50:20 |     learn_positional_embeddings: True\n17:50:20 |     learningrate: 5e-05\n17:50:20 |     load_from_pretrained_ranker: True\n17:50:20 |     log_every_n_secs: 10.0\n17:50:20 |     log_every_n_steps: 50\n17:50:20 |     log_keep_fields: all\n17:50:20 |     loglevel: info\n17:50:20 |     lr_scheduler: reduceonplateau\n17:50:20 |     lr_scheduler_decay: 0.5\n17:50:20 |     lr_scheduler_patience: 3\n17:50:20 |     max_train_steps: -1\n17:50:20 |     max_train_time: 7200.0\n17:50:20 |     memory_attention: sqrt\n17:50:20 |     metrics: default\n17:50:20 |     model: transformer/classifier\n17:50:20 |     model_file: /tmp/model3\n17:50:20 |     model_parallel: False\n17:50:20 |     momentum: 0\n17:50:20 |     multitask_weights: [1]\n17:50:20 |     mutators: None\n17:50:20 |     n_decoder_layers: -1\n17:50:20 |     n_encoder_layers: -1\n17:50:20 |     n_heads: 12\n17:50:20 |     n_layers: 12\n17:50:20 |     n_positions: 1024\n17:50:20 |     n_segments: 2\n17:50:20 |     nesterov: True\n17:50:20 |     no_cuda: False\n17:50:20 |     normalize_sent_emb: False\n17:50:20 |     num_epochs: -1\n17:50:20 |     num_examples: -1\n17:50:20 |     num_workers: 0\n17:50:20 |     nus: [0.7]\n17:50:20 |     optimizer: adamax\n17:50:20 |     output_scaling: 0.06\n17:50:20 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n17:50:20 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:50:20 |     person_tokens: False\n17:50:20 |     print_scores: False\n17:50:20 |     rank_candidates: False\n17:50:20 |     rank_top_k: -1\n17:50:20 |     reduction_type: mean\n17:50:20 |     ref_class: None\n17:50:20 |     relu_dropout: 0.0\n17:50:20 |     repeat_blocking_heuristic: True\n17:50:20 |     report_filename: \n17:50:20 |     return_cand_scores: False\n17:50:20 |     save_after_valid: True\n17:50:20 |     save_every_n_secs: -1\n17:50:20 |     save_format: conversations\n17:50:20 |     share_encoders: False\n17:50:20 |     share_word_embeddings: False\n17:50:20 |     short_final_eval: False\n17:50:20 |     special_tok_lst: None\n17:50:20 |     split_lines: False\n17:50:20 |     starttime: Dec03_17-48\n17:50:20 |     task: fromfile:parlaiformat\n17:50:20 |     tensorboard_log: False\n17:50:20 |     tensorboard_logdir: None\n17:50:20 |     text_truncate: 360\n17:50:20 |     threshold: 0.5\n17:50:20 |     topk: 5\n17:50:20 |     train_predict: False\n17:50:20 |     truncate: 1024\n17:50:20 |     update_classifier_head_only: False\n17:50:20 |     update_freq: 1\n17:50:20 |     use_memories: False\n17:50:20 |     use_reply: none\n17:50:20 |     validation_cutoff: 1.0\n17:50:20 |     validation_every_n_epochs: -1\n17:50:20 |     validation_every_n_secs: 20.0\n17:50:20 |     validation_every_n_steps: -1\n17:50:20 |     validation_max_exs: -1\n17:50:20 |     validation_metric: accuracy\n17:50:20 |     validation_metric_mode: max\n17:50:20 |     validation_patience: 30\n17:50:20 |     validation_share_agent: False\n17:50:20 |     variant: xlm\n17:50:20 |     verbose: False\n17:50:20 |     wandb_entity: None\n17:50:20 |     wandb_log: False\n17:50:20 |     wandb_name: None\n17:50:20 |     wandb_project: None\n17:50:20 |     warmup_rate: 0.0001\n17:50:20 |     warmup_updates: 1000\n17:50:20 |     weight_decay: None\n17:50:20 |     world_logs: \n17:50:20 |     wrap_memory_encoder: False\n17:50:20 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:50:20 | creating task(s): fromfile:parlaiformat\n17:50:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:50:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run3/data_train-b.txt\n17:50:26 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8950 8.95e-10               .8912                 .9247   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8600            .8986              .8692   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9300 11.47   539 513.1       0          0 38.07  200 .8950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .4412 5.104e-06   240 228.4       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    102  779 741.5        .8949\u001b[0m\n17:50:26 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8950 8.95e-10               .8912                 .9247   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8600            .8986              .8692   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9300 11.47   539 513.1       0          0 38.07  200 .8950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .4412 5.104e-06   240 228.4       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                    102  779 741.5        .8949\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:50:27.880881Z","iopub.execute_input":"2022-12-03T17:50:27.881262Z","iopub.status.idle":"2022-12-03T17:50:29.067044Z","shell.execute_reply.started":"2022-12-03T17:50:27.881229Z","shell.execute_reply":"2022-12-03T17:50:29.065690Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:50:29.069102Z","iopub.execute_input":"2022-12-03T17:50:29.069515Z","iopub.status.idle":"2022-12-03T17:51:21.400320Z","shell.execute_reply.started":"2022-12-03T17:50:29.069459Z","shell.execute_reply":"2022-12-03T17:51:21.399132Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"17:50:36 | building dictionary first...\n17:50:36 | No model with opt yet at: /tmp/model4(.opt)\n17:50:36 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:50:36 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:50:36 | Using CUDA\n17:50:36 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:50:36 | num words = 54944\n17:50:40 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:50:46 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:50:46 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:50:46 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:50:46 | Opt:\n17:50:46 |     activation: gelu\n17:50:46 |     adafactor_eps: '(1e-30, 0.001)'\n17:50:46 |     adam_eps: 1e-08\n17:50:46 |     add_p1_after_newln: False\n17:50:46 |     aggregate_micro: False\n17:50:46 |     allow_missing_init_opts: False\n17:50:46 |     attention_dropout: 0.1\n17:50:46 |     batchsize: 20\n17:50:46 |     betas: '(0.9, 0.999)'\n17:50:46 |     bpe_add_prefix_space: None\n17:50:46 |     bpe_debug: False\n17:50:46 |     bpe_dropout: None\n17:50:46 |     bpe_merge: None\n17:50:46 |     bpe_vocab: None\n17:50:46 |     candidates: inline\n17:50:46 |     cap_num_predictions: 100\n17:50:46 |     checkpoint_activations: False\n17:50:46 |     class_weights: None\n17:50:46 |     classes: \"['__notok__', '__ok__']\"\n17:50:46 |     classes_from_file: None\n17:50:46 |     data_parallel: True\n17:50:46 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:50:46 |     datatype: train\n17:50:46 |     delimiter: '\\n'\n17:50:46 |     dict_class: parlai.core.dict:DictionaryAgent\n17:50:46 |     dict_endtoken: __start__\n17:50:46 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:50:46 |     dict_include_test: False\n17:50:46 |     dict_include_valid: False\n17:50:46 |     dict_initpath: None\n17:50:46 |     dict_language: english\n17:50:46 |     dict_loaded: True\n17:50:46 |     dict_lower: True\n17:50:46 |     dict_max_ngram_size: -1\n17:50:46 |     dict_maxexs: -1\n17:50:46 |     dict_maxtokens: -1\n17:50:46 |     dict_minfreq: 0\n17:50:46 |     dict_nulltoken: __null__\n17:50:46 |     dict_starttoken: __start__\n17:50:46 |     dict_textfields: text,labels\n17:50:46 |     dict_tokenizer: bpe\n17:50:46 |     dict_unktoken: __unk__\n17:50:46 |     display_examples: False\n17:50:46 |     download_path: None\n17:50:46 |     dropout: 0.1\n17:50:46 |     dynamic_batching: None\n17:50:46 |     embedding_projection: random\n17:50:46 |     embedding_size: 768\n17:50:46 |     embedding_type: random\n17:50:46 |     embeddings_scale: False\n17:50:46 |     encode_candidate_vecs: True\n17:50:46 |     encode_candidate_vecs_batchsize: 256\n17:50:46 |     eval_batchsize: None\n17:50:46 |     eval_candidates: inline\n17:50:46 |     eval_dynamic_batching: None\n17:50:46 |     evaltask: None\n17:50:46 |     ffn_size: 3072\n17:50:46 |     final_extra_opt: \n17:50:46 |     fixed_candidate_vecs: reuse\n17:50:46 |     fixed_candidates_path: None\n17:50:46 |     force_fp16_tokens: False\n17:50:46 |     fp16: True\n17:50:46 |     fp16_impl: safe\n17:50:46 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt\n17:50:46 |     fromfile_datatype_extension: False\n17:50:46 |     gpu: -1\n17:50:46 |     gradient_clip: 0.1\n17:50:46 |     hide_labels: False\n17:50:46 |     history_add_global_end_token: None\n17:50:46 |     history_reversed: False\n17:50:46 |     history_size: 20\n17:50:46 |     ignore_bad_candidates: False\n17:50:46 |     ignore_labels: None\n17:50:46 |     image_cropsize: 224\n17:50:46 |     image_mode: raw\n17:50:46 |     image_size: 256\n17:50:46 |     inference: max\n17:50:46 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:50:46 |     init_opt: None\n17:50:46 |     interactive_candidates: fixed\n17:50:46 |     interactive_mode: False\n17:50:46 |     invsqrt_lr_decay_gamma: -1\n17:50:46 |     is_debug: False\n17:50:46 |     label_truncate: 72\n17:50:46 |     learn_embeddings: True\n17:50:46 |     learn_positional_embeddings: True\n17:50:46 |     learningrate: 5e-05\n17:50:46 |     load_from_checkpoint: False\n17:50:46 |     load_from_pretrained_ranker: True\n17:50:46 |     log_every_n_secs: 10.0\n17:50:46 |     log_every_n_steps: 50\n17:50:46 |     log_keep_fields: all\n17:50:46 |     loglevel: info\n17:50:46 |     lr_scheduler: reduceonplateau\n17:50:46 |     lr_scheduler_decay: 0.5\n17:50:46 |     lr_scheduler_patience: 3\n17:50:46 |     max_train_steps: -1\n17:50:46 |     max_train_time: 7200.0\n17:50:46 |     memory_attention: sqrt\n17:50:46 |     metrics: default\n17:50:46 |     model: transformer/classifier\n17:50:46 |     model_file: /tmp/model4\n17:50:46 |     model_parallel: False\n17:50:46 |     momentum: 0\n17:50:46 |     multitask_weights: [1]\n17:50:46 |     mutators: None\n17:50:46 |     n_decoder_layers: -1\n17:50:46 |     n_encoder_layers: -1\n17:50:46 |     n_heads: 12\n17:50:46 |     n_layers: 12\n17:50:46 |     n_positions: 1024\n17:50:46 |     n_segments: 2\n17:50:46 |     nesterov: True\n17:50:46 |     no_cuda: False\n17:50:46 |     normalize_sent_emb: False\n17:50:46 |     num_epochs: -1\n17:50:46 |     num_workers: 0\n17:50:46 |     nus: (0.7,)\n17:50:46 |     optimizer: adamax\n17:50:46 |     output_scaling: 0.06\n17:50:46 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n17:50:46 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:50:46 |     person_tokens: False\n17:50:46 |     print_scores: False\n17:50:46 |     rank_candidates: False\n17:50:46 |     rank_top_k: -1\n17:50:46 |     reduction_type: mean\n17:50:46 |     ref_class: None\n17:50:46 |     relu_dropout: 0.0\n17:50:46 |     repeat_blocking_heuristic: True\n17:50:46 |     return_cand_scores: False\n17:50:46 |     save_after_valid: True\n17:50:46 |     save_every_n_secs: -1\n17:50:46 |     save_format: conversations\n17:50:46 |     share_encoders: False\n17:50:46 |     share_word_embeddings: False\n17:50:46 |     short_final_eval: False\n17:50:46 |     special_tok_lst: None\n17:50:46 |     split_lines: False\n17:50:46 |     starttime: Dec03_17-50\n17:50:46 |     task: fromfile:parlaiformat\n17:50:46 |     tensorboard_log: False\n17:50:46 |     tensorboard_logdir: None\n17:50:46 |     text_truncate: 360\n17:50:46 |     threshold: 0.5\n17:50:46 |     topk: 5\n17:50:46 |     train_predict: False\n17:50:46 |     truncate: 1024\n17:50:46 |     update_classifier_head_only: False\n17:50:46 |     update_freq: 1\n17:50:46 |     use_memories: False\n17:50:46 |     use_reply: none\n17:50:46 |     validation_cutoff: 1.0\n17:50:46 |     validation_every_n_epochs: -1\n17:50:46 |     validation_every_n_secs: 20.0\n17:50:46 |     validation_every_n_steps: -1\n17:50:46 |     validation_max_exs: -1\n17:50:46 |     validation_metric: accuracy\n17:50:46 |     validation_metric_mode: max\n17:50:46 |     validation_patience: 30\n17:50:46 |     validation_share_agent: False\n17:50:46 |     variant: xlm\n17:50:46 |     verbose: False\n17:50:46 |     wandb_entity: None\n17:50:46 |     wandb_log: False\n17:50:46 |     wandb_name: None\n17:50:46 |     wandb_project: None\n17:50:46 |     warmup_rate: 0.0001\n17:50:46 |     warmup_updates: 1000\n17:50:46 |     weight_decay: None\n17:50:46 |     world_logs: \n17:50:46 |     wrap_memory_encoder: False\n17:50:46 | creating task(s): fromfile:parlaiformat\n17:50:46 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt\n17:50:46 | training...\n17:50:57 | time:10s total_exs:380 total_steps:19 epochs:15.83\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5158 5.158e-10               .2333                 .5490   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1481            .6462              .5106   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8796 12.44     1 288.8 544.8       0          0 37.73  380   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5158             32768  2.678    .1189 5.995 .6903 9.549e-07 119.9 226.2   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   19 408.7  771 1.891        .4408\n\n17:51:07 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8487 8.487e-10               .8260                 .9479   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7319            .8661              .7881   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9612 12.08     1 281.7  1079       0          0 76.62  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8487             32768  2.822    .1189 5.982 .6335 2.855e-06 119.6 458.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                   57 401.3 1537 3.84        .8464\n\n17:51:07 | creating task(s): fromfile:parlaiformat\n17:51:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:51:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt\n17:51:07 | running eval: valid\n17:51:07 | eval completed in 0.19s\n17:51:07 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1929       0          0 137.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5666 2.855e-06    72 824.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2753            1\n\u001b[0m\n17:51:07 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:51:07 | saving best valid model: /tmp/model4\n17:51:07 | Saving dictionary to /tmp/model4.dict\n17:51:10 | task solved! stopping.\n17:51:10 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:51:10 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:51:10 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:51:10 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:51:10 | Using CUDA\n17:51:10 | loading dictionary from /tmp/model4.dict\n17:51:11 | num words = 54944\n17:51:16 | Loading existing model parameters from /tmp/model4\n17:51:17 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:51:18 | creating task(s): fromfile:parlaiformat\n17:51:18 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:51:18 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt\n17:51:19 | running eval: valid\n17:51:19 | eval completed in 0.21s\n17:51:19 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1854       0          0 131.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5666 2.855e-06    72   792       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2646            1\n\u001b[0m\n17:51:19 | creating task(s): fromfile:parlaiformat\n17:51:19 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:51:19 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt\n17:51:19 | running eval: test\n17:51:19 | eval completed in 0.19s\n17:51:19 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1921       0          0 136.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5666 2.855e-06    72 820.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2741            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:51:21.403503Z","iopub.execute_input":"2022-12-03T17:51:21.403846Z","iopub.status.idle":"2022-12-03T17:51:49.382826Z","shell.execute_reply.started":"2022-12-03T17:51:21.403813Z","shell.execute_reply":"2022-12-03T17:51:49.381561Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"17:51:28 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt)\u001b[0m\n17:51:28 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:51:28 | Using CUDA\n17:51:28 | loading dictionary from /tmp/model4.dict\n17:51:28 | num words = 54944\n17:51:33 | Loading existing model parameters from /tmp/model4\n17:51:40 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:51:41 | Opt:\n17:51:41 |     activation: gelu\n17:51:41 |     adafactor_eps: '[1e-30, 0.001]'\n17:51:41 |     adam_eps: 1e-08\n17:51:41 |     add_p1_after_newln: False\n17:51:41 |     aggregate_micro: False\n17:51:41 |     allow_missing_init_opts: False\n17:51:41 |     area_under_curve_class: None\n17:51:41 |     area_under_curve_digits: -1\n17:51:41 |     attention_dropout: 0.1\n17:51:41 |     batchsize: 40\n17:51:41 |     betas: '[0.9, 0.999]'\n17:51:41 |     bpe_add_prefix_space: None\n17:51:41 |     bpe_debug: False\n17:51:41 |     bpe_dropout: None\n17:51:41 |     bpe_merge: None\n17:51:41 |     bpe_vocab: None\n17:51:41 |     candidates: inline\n17:51:41 |     cap_num_predictions: 100\n17:51:41 |     checkpoint_activations: False\n17:51:41 |     class_weights: None\n17:51:41 |     classes: \"['__notok__', '__ok__']\"\n17:51:41 |     classes_from_file: None\n17:51:41 |     data_parallel: True\n17:51:41 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:51:41 |     datatype: train\n17:51:41 |     delimiter: '\\n'\n17:51:41 |     dict_class: parlai.core.dict:DictionaryAgent\n17:51:41 |     dict_endtoken: __start__\n17:51:41 |     dict_file: /tmp/model4.dict\n17:51:41 |     dict_include_test: False\n17:51:41 |     dict_include_valid: False\n17:51:41 |     dict_initpath: None\n17:51:41 |     dict_language: english\n17:51:41 |     dict_loaded: True\n17:51:41 |     dict_lower: True\n17:51:41 |     dict_max_ngram_size: -1\n17:51:41 |     dict_maxexs: -1\n17:51:41 |     dict_maxtokens: -1\n17:51:41 |     dict_minfreq: 0\n17:51:41 |     dict_nulltoken: __null__\n17:51:41 |     dict_starttoken: __start__\n17:51:41 |     dict_textfields: text,labels\n17:51:41 |     dict_tokenizer: bpe\n17:51:41 |     dict_unktoken: __unk__\n17:51:41 |     display_examples: False\n17:51:41 |     download_path: None\n17:51:41 |     dropout: 0.1\n17:51:41 |     dynamic_batching: None\n17:51:41 |     embedding_projection: random\n17:51:41 |     embedding_size: 768\n17:51:41 |     embedding_type: random\n17:51:41 |     embeddings_scale: False\n17:51:41 |     encode_candidate_vecs: True\n17:51:41 |     encode_candidate_vecs_batchsize: 256\n17:51:41 |     eval_batchsize: None\n17:51:41 |     eval_candidates: inline\n17:51:41 |     eval_dynamic_batching: None\n17:51:41 |     evaltask: None\n17:51:41 |     ffn_size: 3072\n17:51:41 |     final_extra_opt: \n17:51:41 |     fixed_candidate_vecs: reuse\n17:51:41 |     fixed_candidates_path: None\n17:51:41 |     force_fp16_tokens: True\n17:51:41 |     fp16: True\n17:51:41 |     fp16_impl: safe\n17:51:41 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-a.txt\n17:51:41 |     fromfile_datatype_extension: False\n17:51:41 |     gpu: -1\n17:51:41 |     gradient_clip: 0.1\n17:51:41 |     hide_labels: False\n17:51:41 |     history_add_global_end_token: None\n17:51:41 |     history_reversed: False\n17:51:41 |     history_size: 20\n17:51:41 |     ignore_bad_candidates: False\n17:51:41 |     ignore_labels: None\n17:51:41 |     image_cropsize: 224\n17:51:41 |     image_mode: raw\n17:51:41 |     image_size: 256\n17:51:41 |     inference: max\n17:51:41 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:51:41 |     init_opt: None\n17:51:41 |     interactive_candidates: fixed\n17:51:41 |     interactive_mode: False\n17:51:41 |     invsqrt_lr_decay_gamma: -1\n17:51:41 |     is_debug: False\n17:51:41 |     label_truncate: 72\n17:51:41 |     learn_embeddings: True\n17:51:41 |     learn_positional_embeddings: True\n17:51:41 |     learningrate: 5e-05\n17:51:41 |     load_from_pretrained_ranker: True\n17:51:41 |     log_every_n_secs: 10.0\n17:51:41 |     log_every_n_steps: 50\n17:51:41 |     log_keep_fields: all\n17:51:41 |     loglevel: info\n17:51:41 |     lr_scheduler: reduceonplateau\n17:51:41 |     lr_scheduler_decay: 0.5\n17:51:41 |     lr_scheduler_patience: 3\n17:51:41 |     max_train_steps: -1\n17:51:41 |     max_train_time: 7200.0\n17:51:41 |     memory_attention: sqrt\n17:51:41 |     metrics: default\n17:51:41 |     model: transformer/classifier\n17:51:41 |     model_file: /tmp/model4\n17:51:41 |     model_parallel: False\n17:51:41 |     momentum: 0\n17:51:41 |     multitask_weights: [1]\n17:51:41 |     mutators: None\n17:51:41 |     n_decoder_layers: -1\n17:51:41 |     n_encoder_layers: -1\n17:51:41 |     n_heads: 12\n17:51:41 |     n_layers: 12\n17:51:41 |     n_positions: 1024\n17:51:41 |     n_segments: 2\n17:51:41 |     nesterov: True\n17:51:41 |     no_cuda: False\n17:51:41 |     normalize_sent_emb: False\n17:51:41 |     num_epochs: -1\n17:51:41 |     num_examples: -1\n17:51:41 |     num_workers: 0\n17:51:41 |     nus: [0.7]\n17:51:41 |     optimizer: adamax\n17:51:41 |     output_scaling: 0.06\n17:51:41 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n17:51:41 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:51:41 |     person_tokens: False\n17:51:41 |     print_scores: False\n17:51:41 |     rank_candidates: False\n17:51:41 |     rank_top_k: -1\n17:51:41 |     reduction_type: mean\n17:51:41 |     ref_class: None\n17:51:41 |     relu_dropout: 0.0\n17:51:41 |     repeat_blocking_heuristic: True\n17:51:41 |     report_filename: \n17:51:41 |     return_cand_scores: False\n17:51:41 |     save_after_valid: True\n17:51:41 |     save_every_n_secs: -1\n17:51:41 |     save_format: conversations\n17:51:41 |     share_encoders: False\n17:51:41 |     share_word_embeddings: False\n17:51:41 |     short_final_eval: False\n17:51:41 |     special_tok_lst: None\n17:51:41 |     split_lines: False\n17:51:41 |     starttime: Dec03_17-50\n17:51:41 |     task: fromfile:parlaiformat\n17:51:41 |     tensorboard_log: False\n17:51:41 |     tensorboard_logdir: None\n17:51:41 |     text_truncate: 360\n17:51:41 |     threshold: 0.5\n17:51:41 |     topk: 5\n17:51:41 |     train_predict: False\n17:51:41 |     truncate: 1024\n17:51:41 |     update_classifier_head_only: False\n17:51:41 |     update_freq: 1\n17:51:41 |     use_memories: False\n17:51:41 |     use_reply: none\n17:51:41 |     validation_cutoff: 1.0\n17:51:41 |     validation_every_n_epochs: -1\n17:51:41 |     validation_every_n_secs: 20.0\n17:51:41 |     validation_every_n_steps: -1\n17:51:41 |     validation_max_exs: -1\n17:51:41 |     validation_metric: accuracy\n17:51:41 |     validation_metric_mode: max\n17:51:41 |     validation_patience: 30\n17:51:41 |     validation_share_agent: False\n17:51:41 |     variant: xlm\n17:51:41 |     verbose: False\n17:51:41 |     wandb_entity: None\n17:51:41 |     wandb_log: False\n17:51:41 |     wandb_name: None\n17:51:41 |     wandb_project: None\n17:51:41 |     warmup_rate: 0.0001\n17:51:41 |     warmup_updates: 1000\n17:51:41 |     weight_decay: None\n17:51:41 |     world_logs: \n17:51:41 |     wrap_memory_encoder: False\n17:51:42 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:51:42 | creating task(s): fromfile:parlaiformat\n17:51:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:51:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-a.txt\n17:51:47 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1950 1.95e-10               .0800                .09211   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                    .07071            .2844              .2581   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3168 11.46 538.2 513.5       0          0 38.17  200 .1950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .7647 2.855e-06 239.6 228.6       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 777.8 742.2        .1832\u001b[0m\n17:51:47 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1950 1.95e-10               .0800                .09211   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                    .07071            .2844              .2581   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3168 11.46 538.2 513.5       0          0 38.17  200 .1950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .7647 2.855e-06 239.6 228.6       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 777.8 742.2        .1832\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:51:49.385066Z","iopub.execute_input":"2022-12-03T17:51:49.385530Z","iopub.status.idle":"2022-12-03T17:52:15.654606Z","shell.execute_reply.started":"2022-12-03T17:51:49.385462Z","shell.execute_reply":"2022-12-03T17:52:15.653421Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"17:51:56 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_valid.txt)\u001b[0m\n17:51:56 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:51:56 | Using CUDA\n17:51:56 | loading dictionary from /tmp/model4.dict\n17:51:56 | num words = 54944\n17:52:01 | Loading existing model parameters from /tmp/model4\n17:52:06 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:52:08 | Opt:\n17:52:08 |     activation: gelu\n17:52:08 |     adafactor_eps: '[1e-30, 0.001]'\n17:52:08 |     adam_eps: 1e-08\n17:52:08 |     add_p1_after_newln: False\n17:52:08 |     aggregate_micro: False\n17:52:08 |     allow_missing_init_opts: False\n17:52:08 |     area_under_curve_class: None\n17:52:08 |     area_under_curve_digits: -1\n17:52:08 |     attention_dropout: 0.1\n17:52:08 |     batchsize: 40\n17:52:08 |     betas: '[0.9, 0.999]'\n17:52:08 |     bpe_add_prefix_space: None\n17:52:08 |     bpe_debug: False\n17:52:08 |     bpe_dropout: None\n17:52:08 |     bpe_merge: None\n17:52:08 |     bpe_vocab: None\n17:52:08 |     candidates: inline\n17:52:08 |     cap_num_predictions: 100\n17:52:08 |     checkpoint_activations: False\n17:52:08 |     class_weights: None\n17:52:08 |     classes: \"['__notok__', '__ok__']\"\n17:52:08 |     classes_from_file: None\n17:52:08 |     data_parallel: True\n17:52:08 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:52:08 |     datatype: train\n17:52:08 |     delimiter: '\\n'\n17:52:08 |     dict_class: parlai.core.dict:DictionaryAgent\n17:52:08 |     dict_endtoken: __start__\n17:52:08 |     dict_file: /tmp/model4.dict\n17:52:08 |     dict_include_test: False\n17:52:08 |     dict_include_valid: False\n17:52:08 |     dict_initpath: None\n17:52:08 |     dict_language: english\n17:52:08 |     dict_loaded: True\n17:52:08 |     dict_lower: True\n17:52:08 |     dict_max_ngram_size: -1\n17:52:08 |     dict_maxexs: -1\n17:52:08 |     dict_maxtokens: -1\n17:52:08 |     dict_minfreq: 0\n17:52:08 |     dict_nulltoken: __null__\n17:52:08 |     dict_starttoken: __start__\n17:52:08 |     dict_textfields: text,labels\n17:52:08 |     dict_tokenizer: bpe\n17:52:08 |     dict_unktoken: __unk__\n17:52:08 |     display_examples: False\n17:52:08 |     download_path: None\n17:52:08 |     dropout: 0.1\n17:52:08 |     dynamic_batching: None\n17:52:08 |     embedding_projection: random\n17:52:08 |     embedding_size: 768\n17:52:08 |     embedding_type: random\n17:52:08 |     embeddings_scale: False\n17:52:08 |     encode_candidate_vecs: True\n17:52:08 |     encode_candidate_vecs_batchsize: 256\n17:52:08 |     eval_batchsize: None\n17:52:08 |     eval_candidates: inline\n17:52:08 |     eval_dynamic_batching: None\n17:52:08 |     evaltask: None\n17:52:08 |     ffn_size: 3072\n17:52:08 |     final_extra_opt: \n17:52:08 |     fixed_candidate_vecs: reuse\n17:52:08 |     fixed_candidates_path: None\n17:52:08 |     force_fp16_tokens: True\n17:52:08 |     fp16: True\n17:52:08 |     fp16_impl: safe\n17:52:08 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-b.txt\n17:52:08 |     fromfile_datatype_extension: False\n17:52:08 |     gpu: -1\n17:52:08 |     gradient_clip: 0.1\n17:52:08 |     hide_labels: False\n17:52:08 |     history_add_global_end_token: None\n17:52:08 |     history_reversed: False\n17:52:08 |     history_size: 20\n17:52:08 |     ignore_bad_candidates: False\n17:52:08 |     ignore_labels: None\n17:52:08 |     image_cropsize: 224\n17:52:08 |     image_mode: raw\n17:52:08 |     image_size: 256\n17:52:08 |     inference: max\n17:52:08 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:52:08 |     init_opt: None\n17:52:08 |     interactive_candidates: fixed\n17:52:08 |     interactive_mode: False\n17:52:08 |     invsqrt_lr_decay_gamma: -1\n17:52:08 |     is_debug: False\n17:52:08 |     label_truncate: 72\n17:52:08 |     learn_embeddings: True\n17:52:08 |     learn_positional_embeddings: True\n17:52:08 |     learningrate: 5e-05\n17:52:08 |     load_from_pretrained_ranker: True\n17:52:08 |     log_every_n_secs: 10.0\n17:52:08 |     log_every_n_steps: 50\n17:52:08 |     log_keep_fields: all\n17:52:08 |     loglevel: info\n17:52:08 |     lr_scheduler: reduceonplateau\n17:52:08 |     lr_scheduler_decay: 0.5\n17:52:08 |     lr_scheduler_patience: 3\n17:52:08 |     max_train_steps: -1\n17:52:08 |     max_train_time: 7200.0\n17:52:08 |     memory_attention: sqrt\n17:52:08 |     metrics: default\n17:52:08 |     model: transformer/classifier\n17:52:08 |     model_file: /tmp/model4\n17:52:08 |     model_parallel: False\n17:52:08 |     momentum: 0\n17:52:08 |     multitask_weights: [1]\n17:52:08 |     mutators: None\n17:52:08 |     n_decoder_layers: -1\n17:52:08 |     n_encoder_layers: -1\n17:52:08 |     n_heads: 12\n17:52:08 |     n_layers: 12\n17:52:08 |     n_positions: 1024\n17:52:08 |     n_segments: 2\n17:52:08 |     nesterov: True\n17:52:08 |     no_cuda: False\n17:52:08 |     normalize_sent_emb: False\n17:52:08 |     num_epochs: -1\n17:52:08 |     num_examples: -1\n17:52:08 |     num_workers: 0\n17:52:08 |     nus: [0.7]\n17:52:08 |     optimizer: adamax\n17:52:08 |     output_scaling: 0.06\n17:52:08 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n17:52:08 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:52:08 |     person_tokens: False\n17:52:08 |     print_scores: False\n17:52:08 |     rank_candidates: False\n17:52:08 |     rank_top_k: -1\n17:52:08 |     reduction_type: mean\n17:52:08 |     ref_class: None\n17:52:08 |     relu_dropout: 0.0\n17:52:08 |     repeat_blocking_heuristic: True\n17:52:08 |     report_filename: \n17:52:08 |     return_cand_scores: False\n17:52:08 |     save_after_valid: True\n17:52:08 |     save_every_n_secs: -1\n17:52:08 |     save_format: conversations\n17:52:08 |     share_encoders: False\n17:52:08 |     share_word_embeddings: False\n17:52:08 |     short_final_eval: False\n17:52:08 |     special_tok_lst: None\n17:52:08 |     split_lines: False\n17:52:08 |     starttime: Dec03_17-50\n17:52:08 |     task: fromfile:parlaiformat\n17:52:08 |     tensorboard_log: False\n17:52:08 |     tensorboard_logdir: None\n17:52:08 |     text_truncate: 360\n17:52:08 |     threshold: 0.5\n17:52:08 |     topk: 5\n17:52:08 |     train_predict: False\n17:52:08 |     truncate: 1024\n17:52:08 |     update_classifier_head_only: False\n17:52:08 |     update_freq: 1\n17:52:08 |     use_memories: False\n17:52:08 |     use_reply: none\n17:52:08 |     validation_cutoff: 1.0\n17:52:08 |     validation_every_n_epochs: -1\n17:52:08 |     validation_every_n_secs: 20.0\n17:52:08 |     validation_every_n_steps: -1\n17:52:08 |     validation_max_exs: -1\n17:52:08 |     validation_metric: accuracy\n17:52:08 |     validation_metric_mode: max\n17:52:08 |     validation_patience: 30\n17:52:08 |     validation_share_agent: False\n17:52:08 |     variant: xlm\n17:52:08 |     verbose: False\n17:52:08 |     wandb_entity: None\n17:52:08 |     wandb_log: False\n17:52:08 |     wandb_name: None\n17:52:08 |     wandb_project: None\n17:52:08 |     warmup_rate: 0.0001\n17:52:08 |     warmup_updates: 1000\n17:52:08 |     weight_decay: None\n17:52:08 |     world_logs: \n17:52:08 |     wrap_memory_encoder: False\n17:52:08 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:52:08 | creating task(s): fromfile:parlaiformat\n17:52:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:52:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run4/data_train-b.txt\n17:52:14 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8050 8.05e-10               .7797                 .9079   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6832            .8251              .7419   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9293 11.46 538.2 499.2       0          0  37.1  200 .8050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .6320 2.855e-06 240.4   223       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 778.6 722.1        .8022\u001b[0m\n17:52:14 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8050 8.05e-10               .7797                 .9079   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6832            .8251              .7419   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9293 11.46 538.2 499.2       0          0  37.1  200 .8050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .6320 2.855e-06 240.4   223       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 778.6 722.1        .8022\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:52:15.656714Z","iopub.execute_input":"2022-12-03T17:52:15.657099Z","iopub.status.idle":"2022-12-03T17:52:16.746315Z","shell.execute_reply.started":"2022-12-03T17:52:15.657061Z","shell.execute_reply":"2022-12-03T17:52:16.744994Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:52:16.748173Z","iopub.execute_input":"2022-12-03T17:52:16.748619Z","iopub.status.idle":"2022-12-03T17:53:10.021640Z","shell.execute_reply.started":"2022-12-03T17:52:16.748577Z","shell.execute_reply":"2022-12-03T17:53:10.020351Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"17:52:24 | building dictionary first...\n17:52:24 | No model with opt yet at: /tmp/model5(.opt)\n17:52:24 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:52:24 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:52:24 | Using CUDA\n17:52:24 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:52:24 | num words = 54944\n17:52:28 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:52:34 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:52:34 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:52:34 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:52:34 | Opt:\n17:52:34 |     activation: gelu\n17:52:34 |     adafactor_eps: '(1e-30, 0.001)'\n17:52:34 |     adam_eps: 1e-08\n17:52:34 |     add_p1_after_newln: False\n17:52:34 |     aggregate_micro: False\n17:52:34 |     allow_missing_init_opts: False\n17:52:34 |     attention_dropout: 0.1\n17:52:34 |     batchsize: 20\n17:52:34 |     betas: '(0.9, 0.999)'\n17:52:34 |     bpe_add_prefix_space: None\n17:52:34 |     bpe_debug: False\n17:52:34 |     bpe_dropout: None\n17:52:34 |     bpe_merge: None\n17:52:34 |     bpe_vocab: None\n17:52:34 |     candidates: inline\n17:52:34 |     cap_num_predictions: 100\n17:52:34 |     checkpoint_activations: False\n17:52:34 |     class_weights: None\n17:52:34 |     classes: \"['__notok__', '__ok__']\"\n17:52:34 |     classes_from_file: None\n17:52:34 |     data_parallel: True\n17:52:34 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:52:34 |     datatype: train\n17:52:34 |     delimiter: '\\n'\n17:52:34 |     dict_class: parlai.core.dict:DictionaryAgent\n17:52:34 |     dict_endtoken: __start__\n17:52:34 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:52:34 |     dict_include_test: False\n17:52:34 |     dict_include_valid: False\n17:52:34 |     dict_initpath: None\n17:52:34 |     dict_language: english\n17:52:34 |     dict_loaded: True\n17:52:34 |     dict_lower: True\n17:52:34 |     dict_max_ngram_size: -1\n17:52:34 |     dict_maxexs: -1\n17:52:34 |     dict_maxtokens: -1\n17:52:34 |     dict_minfreq: 0\n17:52:34 |     dict_nulltoken: __null__\n17:52:34 |     dict_starttoken: __start__\n17:52:34 |     dict_textfields: text,labels\n17:52:34 |     dict_tokenizer: bpe\n17:52:34 |     dict_unktoken: __unk__\n17:52:34 |     display_examples: False\n17:52:34 |     download_path: None\n17:52:34 |     dropout: 0.1\n17:52:34 |     dynamic_batching: None\n17:52:34 |     embedding_projection: random\n17:52:34 |     embedding_size: 768\n17:52:34 |     embedding_type: random\n17:52:34 |     embeddings_scale: False\n17:52:34 |     encode_candidate_vecs: True\n17:52:34 |     encode_candidate_vecs_batchsize: 256\n17:52:34 |     eval_batchsize: None\n17:52:34 |     eval_candidates: inline\n17:52:34 |     eval_dynamic_batching: None\n17:52:34 |     evaltask: None\n17:52:34 |     ffn_size: 3072\n17:52:34 |     final_extra_opt: \n17:52:34 |     fixed_candidate_vecs: reuse\n17:52:34 |     fixed_candidates_path: None\n17:52:34 |     force_fp16_tokens: False\n17:52:34 |     fp16: True\n17:52:34 |     fp16_impl: safe\n17:52:34 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt\n17:52:34 |     fromfile_datatype_extension: False\n17:52:34 |     gpu: -1\n17:52:34 |     gradient_clip: 0.1\n17:52:34 |     hide_labels: False\n17:52:34 |     history_add_global_end_token: None\n17:52:34 |     history_reversed: False\n17:52:34 |     history_size: 20\n17:52:34 |     ignore_bad_candidates: False\n17:52:34 |     ignore_labels: None\n17:52:34 |     image_cropsize: 224\n17:52:34 |     image_mode: raw\n17:52:34 |     image_size: 256\n17:52:34 |     inference: max\n17:52:34 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:52:34 |     init_opt: None\n17:52:34 |     interactive_candidates: fixed\n17:52:34 |     interactive_mode: False\n17:52:34 |     invsqrt_lr_decay_gamma: -1\n17:52:34 |     is_debug: False\n17:52:34 |     label_truncate: 72\n17:52:34 |     learn_embeddings: True\n17:52:34 |     learn_positional_embeddings: True\n17:52:34 |     learningrate: 5e-05\n17:52:34 |     load_from_checkpoint: False\n17:52:34 |     load_from_pretrained_ranker: True\n17:52:34 |     log_every_n_secs: 10.0\n17:52:34 |     log_every_n_steps: 50\n17:52:34 |     log_keep_fields: all\n17:52:34 |     loglevel: info\n17:52:34 |     lr_scheduler: reduceonplateau\n17:52:34 |     lr_scheduler_decay: 0.5\n17:52:34 |     lr_scheduler_patience: 3\n17:52:34 |     max_train_steps: -1\n17:52:34 |     max_train_time: 7200.0\n17:52:34 |     memory_attention: sqrt\n17:52:34 |     metrics: default\n17:52:34 |     model: transformer/classifier\n17:52:34 |     model_file: /tmp/model5\n17:52:34 |     model_parallel: False\n17:52:34 |     momentum: 0\n17:52:34 |     multitask_weights: [1]\n17:52:34 |     mutators: None\n17:52:34 |     n_decoder_layers: -1\n17:52:34 |     n_encoder_layers: -1\n17:52:34 |     n_heads: 12\n17:52:34 |     n_layers: 12\n17:52:34 |     n_positions: 1024\n17:52:34 |     n_segments: 2\n17:52:34 |     nesterov: True\n17:52:34 |     no_cuda: False\n17:52:34 |     normalize_sent_emb: False\n17:52:34 |     num_epochs: -1\n17:52:34 |     num_workers: 0\n17:52:34 |     nus: (0.7,)\n17:52:34 |     optimizer: adamax\n17:52:34 |     output_scaling: 0.06\n17:52:34 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n17:52:34 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:52:34 |     person_tokens: False\n17:52:34 |     print_scores: False\n17:52:34 |     rank_candidates: False\n17:52:34 |     rank_top_k: -1\n17:52:34 |     reduction_type: mean\n17:52:34 |     ref_class: None\n17:52:34 |     relu_dropout: 0.0\n17:52:34 |     repeat_blocking_heuristic: True\n17:52:34 |     return_cand_scores: False\n17:52:34 |     save_after_valid: True\n17:52:34 |     save_every_n_secs: -1\n17:52:34 |     save_format: conversations\n17:52:34 |     share_encoders: False\n17:52:34 |     share_word_embeddings: False\n17:52:34 |     short_final_eval: False\n17:52:34 |     special_tok_lst: None\n17:52:34 |     split_lines: False\n17:52:34 |     starttime: Dec03_17-52\n17:52:34 |     task: fromfile:parlaiformat\n17:52:34 |     tensorboard_log: False\n17:52:34 |     tensorboard_logdir: None\n17:52:34 |     text_truncate: 360\n17:52:34 |     threshold: 0.5\n17:52:34 |     topk: 5\n17:52:34 |     train_predict: False\n17:52:34 |     truncate: 1024\n17:52:34 |     update_classifier_head_only: False\n17:52:34 |     update_freq: 1\n17:52:34 |     use_memories: False\n17:52:34 |     use_reply: none\n17:52:34 |     validation_cutoff: 1.0\n17:52:34 |     validation_every_n_epochs: -1\n17:52:34 |     validation_every_n_secs: 20.0\n17:52:34 |     validation_every_n_steps: -1\n17:52:34 |     validation_max_exs: -1\n17:52:34 |     validation_metric: accuracy\n17:52:34 |     validation_metric_mode: max\n17:52:34 |     validation_patience: 30\n17:52:34 |     validation_share_agent: False\n17:52:34 |     variant: xlm\n17:52:34 |     verbose: False\n17:52:34 |     wandb_entity: None\n17:52:34 |     wandb_log: False\n17:52:34 |     wandb_name: None\n17:52:34 |     wandb_project: None\n17:52:34 |     warmup_rate: 0.0001\n17:52:34 |     warmup_updates: 1000\n17:52:34 |     weight_decay: None\n17:52:34 |     world_logs: \n17:52:34 |     wrap_memory_encoder: False\n17:52:34 | creating task(s): fromfile:parlaiformat\n17:52:34 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt\n17:52:35 | training...\n17:52:45 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4476 4.476e-10               .3927                 .4190   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3695            .4934              .4689   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5207 10.43     1 248.6 518.1       0          0 41.68  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4476             32768  2.771    .1206 5.967 .7113 1.055e-06 119.3 248.7   \n    ltrunc  ltrunclen  total_train_updates  tpb   tps   ups  weighted_f1  \n         0          0                   21  368 766.8 2.089        .4447\n\n17:52:55 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7500 7.5e-10               .7301                 .8056   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6675            .7672              .7098   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8347 10.81     1 256.1 995.7       0          0 77.75  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7500             32768  2.749    .1207 6.013 .6573 2.955e-06 120.3 467.6   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   59 376.4 1463 3.894        .7484\n\n17:52:55 | creating task(s): fromfile:parlaiformat\n17:52:55 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:52:55 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt\n17:52:55 | running eval: valid\n17:52:55 | eval completed in 0.25s\n17:52:55 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1382       0          0 109.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5857 2.955e-06    72 654.7       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  224 2037            1\n\u001b[0m\n17:52:55 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:52:55 | saving best valid model: /tmp/model5\n17:52:55 | Saving dictionary to /tmp/model5.dict\n17:52:59 | task solved! stopping.\n17:52:59 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:52:59 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:52:59 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:52:59 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:52:59 | Using CUDA\n17:52:59 | loading dictionary from /tmp/model5.dict\n17:52:59 | num words = 54944\n17:53:04 | Loading existing model parameters from /tmp/model5\n17:53:06 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:53:07 | creating task(s): fromfile:parlaiformat\n17:53:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:53:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt\n17:53:07 | running eval: valid\n17:53:08 | eval completed in 0.34s\n17:53:08 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1020       0          0  80.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5857 2.955e-06    72 483.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  224 1504            1\n\u001b[0m\n17:53:08 | creating task(s): fromfile:parlaiformat\n17:53:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:53:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt\n17:53:08 | running eval: test\n17:53:08 | eval completed in 0.20s\n17:53:08 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1705       0          0 134.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5857 2.955e-06    72 807.6       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  224 2513            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:53:10.024212Z","iopub.execute_input":"2022-12-03T17:53:10.024545Z","iopub.status.idle":"2022-12-03T17:53:38.116164Z","shell.execute_reply.started":"2022-12-03T17:53:10.024511Z","shell.execute_reply":"2022-12-03T17:53:38.114913Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"17:53:17 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt)\u001b[0m\n17:53:17 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:53:17 | Using CUDA\n17:53:17 | loading dictionary from /tmp/model5.dict\n17:53:17 | num words = 54944\n17:53:21 | Loading existing model parameters from /tmp/model5\n17:53:29 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:53:30 | Opt:\n17:53:30 |     activation: gelu\n17:53:30 |     adafactor_eps: '[1e-30, 0.001]'\n17:53:30 |     adam_eps: 1e-08\n17:53:30 |     add_p1_after_newln: False\n17:53:30 |     aggregate_micro: False\n17:53:30 |     allow_missing_init_opts: False\n17:53:30 |     area_under_curve_class: None\n17:53:30 |     area_under_curve_digits: -1\n17:53:30 |     attention_dropout: 0.1\n17:53:30 |     batchsize: 40\n17:53:30 |     betas: '[0.9, 0.999]'\n17:53:30 |     bpe_add_prefix_space: None\n17:53:30 |     bpe_debug: False\n17:53:30 |     bpe_dropout: None\n17:53:30 |     bpe_merge: None\n17:53:30 |     bpe_vocab: None\n17:53:30 |     candidates: inline\n17:53:30 |     cap_num_predictions: 100\n17:53:30 |     checkpoint_activations: False\n17:53:30 |     class_weights: None\n17:53:30 |     classes: \"['__notok__', '__ok__']\"\n17:53:30 |     classes_from_file: None\n17:53:30 |     data_parallel: True\n17:53:30 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:53:30 |     datatype: train\n17:53:30 |     delimiter: '\\n'\n17:53:30 |     dict_class: parlai.core.dict:DictionaryAgent\n17:53:30 |     dict_endtoken: __start__\n17:53:30 |     dict_file: /tmp/model5.dict\n17:53:30 |     dict_include_test: False\n17:53:30 |     dict_include_valid: False\n17:53:30 |     dict_initpath: None\n17:53:30 |     dict_language: english\n17:53:30 |     dict_loaded: True\n17:53:30 |     dict_lower: True\n17:53:30 |     dict_max_ngram_size: -1\n17:53:30 |     dict_maxexs: -1\n17:53:30 |     dict_maxtokens: -1\n17:53:30 |     dict_minfreq: 0\n17:53:30 |     dict_nulltoken: __null__\n17:53:30 |     dict_starttoken: __start__\n17:53:30 |     dict_textfields: text,labels\n17:53:30 |     dict_tokenizer: bpe\n17:53:30 |     dict_unktoken: __unk__\n17:53:30 |     display_examples: False\n17:53:30 |     download_path: None\n17:53:30 |     dropout: 0.1\n17:53:30 |     dynamic_batching: None\n17:53:30 |     embedding_projection: random\n17:53:30 |     embedding_size: 768\n17:53:30 |     embedding_type: random\n17:53:30 |     embeddings_scale: False\n17:53:30 |     encode_candidate_vecs: True\n17:53:30 |     encode_candidate_vecs_batchsize: 256\n17:53:30 |     eval_batchsize: None\n17:53:30 |     eval_candidates: inline\n17:53:30 |     eval_dynamic_batching: None\n17:53:30 |     evaltask: None\n17:53:30 |     ffn_size: 3072\n17:53:30 |     final_extra_opt: \n17:53:30 |     fixed_candidate_vecs: reuse\n17:53:30 |     fixed_candidates_path: None\n17:53:30 |     force_fp16_tokens: True\n17:53:30 |     fp16: True\n17:53:30 |     fp16_impl: safe\n17:53:30 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-a.txt\n17:53:30 |     fromfile_datatype_extension: False\n17:53:30 |     gpu: -1\n17:53:30 |     gradient_clip: 0.1\n17:53:30 |     hide_labels: False\n17:53:30 |     history_add_global_end_token: None\n17:53:30 |     history_reversed: False\n17:53:30 |     history_size: 20\n17:53:30 |     ignore_bad_candidates: False\n17:53:30 |     ignore_labels: None\n17:53:30 |     image_cropsize: 224\n17:53:30 |     image_mode: raw\n17:53:30 |     image_size: 256\n17:53:30 |     inference: max\n17:53:30 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:53:30 |     init_opt: None\n17:53:30 |     interactive_candidates: fixed\n17:53:30 |     interactive_mode: False\n17:53:30 |     invsqrt_lr_decay_gamma: -1\n17:53:30 |     is_debug: False\n17:53:30 |     label_truncate: 72\n17:53:30 |     learn_embeddings: True\n17:53:30 |     learn_positional_embeddings: True\n17:53:30 |     learningrate: 5e-05\n17:53:30 |     load_from_pretrained_ranker: True\n17:53:30 |     log_every_n_secs: 10.0\n17:53:30 |     log_every_n_steps: 50\n17:53:30 |     log_keep_fields: all\n17:53:30 |     loglevel: info\n17:53:30 |     lr_scheduler: reduceonplateau\n17:53:30 |     lr_scheduler_decay: 0.5\n17:53:30 |     lr_scheduler_patience: 3\n17:53:30 |     max_train_steps: -1\n17:53:30 |     max_train_time: 7200.0\n17:53:30 |     memory_attention: sqrt\n17:53:30 |     metrics: default\n17:53:30 |     model: transformer/classifier\n17:53:30 |     model_file: /tmp/model5\n17:53:30 |     model_parallel: False\n17:53:30 |     momentum: 0\n17:53:30 |     multitask_weights: [1]\n17:53:30 |     mutators: None\n17:53:30 |     n_decoder_layers: -1\n17:53:30 |     n_encoder_layers: -1\n17:53:30 |     n_heads: 12\n17:53:30 |     n_layers: 12\n17:53:30 |     n_positions: 1024\n17:53:30 |     n_segments: 2\n17:53:30 |     nesterov: True\n17:53:30 |     no_cuda: False\n17:53:30 |     normalize_sent_emb: False\n17:53:30 |     num_epochs: -1\n17:53:30 |     num_examples: -1\n17:53:30 |     num_workers: 0\n17:53:30 |     nus: [0.7]\n17:53:30 |     optimizer: adamax\n17:53:30 |     output_scaling: 0.06\n17:53:30 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n17:53:30 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:53:30 |     person_tokens: False\n17:53:30 |     print_scores: False\n17:53:30 |     rank_candidates: False\n17:53:30 |     rank_top_k: -1\n17:53:30 |     reduction_type: mean\n17:53:30 |     ref_class: None\n17:53:30 |     relu_dropout: 0.0\n17:53:30 |     repeat_blocking_heuristic: True\n17:53:30 |     report_filename: \n17:53:30 |     return_cand_scores: False\n17:53:30 |     save_after_valid: True\n17:53:30 |     save_every_n_secs: -1\n17:53:30 |     save_format: conversations\n17:53:30 |     share_encoders: False\n17:53:30 |     share_word_embeddings: False\n17:53:30 |     short_final_eval: False\n17:53:30 |     special_tok_lst: None\n17:53:30 |     split_lines: False\n17:53:30 |     starttime: Dec03_17-52\n17:53:30 |     task: fromfile:parlaiformat\n17:53:30 |     tensorboard_log: False\n17:53:30 |     tensorboard_logdir: None\n17:53:30 |     text_truncate: 360\n17:53:30 |     threshold: 0.5\n17:53:30 |     topk: 5\n17:53:30 |     train_predict: False\n17:53:30 |     truncate: 1024\n17:53:30 |     update_classifier_head_only: False\n17:53:30 |     update_freq: 1\n17:53:30 |     use_memories: False\n17:53:30 |     use_reply: none\n17:53:30 |     validation_cutoff: 1.0\n17:53:30 |     validation_every_n_epochs: -1\n17:53:30 |     validation_every_n_secs: 20.0\n17:53:30 |     validation_every_n_steps: -1\n17:53:30 |     validation_max_exs: -1\n17:53:30 |     validation_metric: accuracy\n17:53:30 |     validation_metric_mode: max\n17:53:30 |     validation_patience: 30\n17:53:30 |     validation_share_agent: False\n17:53:30 |     variant: xlm\n17:53:30 |     verbose: False\n17:53:30 |     wandb_entity: None\n17:53:30 |     wandb_log: False\n17:53:30 |     wandb_name: None\n17:53:30 |     wandb_project: None\n17:53:30 |     warmup_rate: 0.0001\n17:53:30 |     warmup_updates: 1000\n17:53:30 |     weight_decay: None\n17:53:30 |     world_logs: \n17:53:30 |     wrap_memory_encoder: False\n17:53:31 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:53:31 | creating task(s): fromfile:parlaiformat\n17:53:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:53:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-a.txt\n17:53:36 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3500 3.5e-10               .3434                 .3400   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3469            .3564              .3600   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3529 11.79 551.8   536       0          0 38.85  200 .3500   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.98 .7205 2.955e-06 239.2 232.4       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     59  791 768.4        .3501\u001b[0m\n17:53:36 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .3500 3.5e-10               .3434                 .3400   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3469            .3564              .3600   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3529 11.79 551.8   536       0          0 38.85  200 .3500   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.98 .7205 2.955e-06 239.2 232.4       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     59  791 768.4        .3501\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:53:38.119022Z","iopub.execute_input":"2022-12-03T17:53:38.119443Z","iopub.status.idle":"2022-12-03T17:54:04.554742Z","shell.execute_reply.started":"2022-12-03T17:53:38.119400Z","shell.execute_reply":"2022-12-03T17:54:04.553548Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"17:53:45 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_valid.txt)\u001b[0m\n17:53:45 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:53:45 | Using CUDA\n17:53:45 | loading dictionary from /tmp/model5.dict\n17:53:45 | num words = 54944\n17:53:49 | Loading existing model parameters from /tmp/model5\n17:53:55 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:53:56 | Opt:\n17:53:56 |     activation: gelu\n17:53:56 |     adafactor_eps: '[1e-30, 0.001]'\n17:53:56 |     adam_eps: 1e-08\n17:53:56 |     add_p1_after_newln: False\n17:53:56 |     aggregate_micro: False\n17:53:56 |     allow_missing_init_opts: False\n17:53:56 |     area_under_curve_class: None\n17:53:56 |     area_under_curve_digits: -1\n17:53:56 |     attention_dropout: 0.1\n17:53:56 |     batchsize: 40\n17:53:56 |     betas: '[0.9, 0.999]'\n17:53:56 |     bpe_add_prefix_space: None\n17:53:56 |     bpe_debug: False\n17:53:56 |     bpe_dropout: None\n17:53:56 |     bpe_merge: None\n17:53:56 |     bpe_vocab: None\n17:53:56 |     candidates: inline\n17:53:56 |     cap_num_predictions: 100\n17:53:56 |     checkpoint_activations: False\n17:53:56 |     class_weights: None\n17:53:56 |     classes: \"['__notok__', '__ok__']\"\n17:53:56 |     classes_from_file: None\n17:53:56 |     data_parallel: True\n17:53:56 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:53:56 |     datatype: train\n17:53:56 |     delimiter: '\\n'\n17:53:56 |     dict_class: parlai.core.dict:DictionaryAgent\n17:53:56 |     dict_endtoken: __start__\n17:53:56 |     dict_file: /tmp/model5.dict\n17:53:56 |     dict_include_test: False\n17:53:56 |     dict_include_valid: False\n17:53:56 |     dict_initpath: None\n17:53:56 |     dict_language: english\n17:53:56 |     dict_loaded: True\n17:53:56 |     dict_lower: True\n17:53:56 |     dict_max_ngram_size: -1\n17:53:56 |     dict_maxexs: -1\n17:53:56 |     dict_maxtokens: -1\n17:53:56 |     dict_minfreq: 0\n17:53:56 |     dict_nulltoken: __null__\n17:53:56 |     dict_starttoken: __start__\n17:53:56 |     dict_textfields: text,labels\n17:53:56 |     dict_tokenizer: bpe\n17:53:56 |     dict_unktoken: __unk__\n17:53:56 |     display_examples: False\n17:53:56 |     download_path: None\n17:53:56 |     dropout: 0.1\n17:53:56 |     dynamic_batching: None\n17:53:56 |     embedding_projection: random\n17:53:56 |     embedding_size: 768\n17:53:56 |     embedding_type: random\n17:53:56 |     embeddings_scale: False\n17:53:56 |     encode_candidate_vecs: True\n17:53:56 |     encode_candidate_vecs_batchsize: 256\n17:53:56 |     eval_batchsize: None\n17:53:56 |     eval_candidates: inline\n17:53:56 |     eval_dynamic_batching: None\n17:53:56 |     evaltask: None\n17:53:56 |     ffn_size: 3072\n17:53:56 |     final_extra_opt: \n17:53:56 |     fixed_candidate_vecs: reuse\n17:53:56 |     fixed_candidates_path: None\n17:53:56 |     force_fp16_tokens: True\n17:53:56 |     fp16: True\n17:53:56 |     fp16_impl: safe\n17:53:56 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-b.txt\n17:53:56 |     fromfile_datatype_extension: False\n17:53:56 |     gpu: -1\n17:53:56 |     gradient_clip: 0.1\n17:53:56 |     hide_labels: False\n17:53:56 |     history_add_global_end_token: None\n17:53:56 |     history_reversed: False\n17:53:56 |     history_size: 20\n17:53:56 |     ignore_bad_candidates: False\n17:53:56 |     ignore_labels: None\n17:53:56 |     image_cropsize: 224\n17:53:56 |     image_mode: raw\n17:53:56 |     image_size: 256\n17:53:56 |     inference: max\n17:53:56 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:53:56 |     init_opt: None\n17:53:56 |     interactive_candidates: fixed\n17:53:56 |     interactive_mode: False\n17:53:56 |     invsqrt_lr_decay_gamma: -1\n17:53:56 |     is_debug: False\n17:53:56 |     label_truncate: 72\n17:53:56 |     learn_embeddings: True\n17:53:56 |     learn_positional_embeddings: True\n17:53:56 |     learningrate: 5e-05\n17:53:56 |     load_from_pretrained_ranker: True\n17:53:56 |     log_every_n_secs: 10.0\n17:53:56 |     log_every_n_steps: 50\n17:53:56 |     log_keep_fields: all\n17:53:56 |     loglevel: info\n17:53:56 |     lr_scheduler: reduceonplateau\n17:53:56 |     lr_scheduler_decay: 0.5\n17:53:56 |     lr_scheduler_patience: 3\n17:53:56 |     max_train_steps: -1\n17:53:56 |     max_train_time: 7200.0\n17:53:56 |     memory_attention: sqrt\n17:53:56 |     metrics: default\n17:53:56 |     model: transformer/classifier\n17:53:56 |     model_file: /tmp/model5\n17:53:56 |     model_parallel: False\n17:53:56 |     momentum: 0\n17:53:56 |     multitask_weights: [1]\n17:53:56 |     mutators: None\n17:53:56 |     n_decoder_layers: -1\n17:53:56 |     n_encoder_layers: -1\n17:53:56 |     n_heads: 12\n17:53:56 |     n_layers: 12\n17:53:56 |     n_positions: 1024\n17:53:56 |     n_segments: 2\n17:53:56 |     nesterov: True\n17:53:56 |     no_cuda: False\n17:53:56 |     normalize_sent_emb: False\n17:53:56 |     num_epochs: -1\n17:53:56 |     num_examples: -1\n17:53:56 |     num_workers: 0\n17:53:56 |     nus: [0.7]\n17:53:56 |     optimizer: adamax\n17:53:56 |     output_scaling: 0.06\n17:53:56 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n17:53:56 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:53:56 |     person_tokens: False\n17:53:56 |     print_scores: False\n17:53:56 |     rank_candidates: False\n17:53:56 |     rank_top_k: -1\n17:53:56 |     reduction_type: mean\n17:53:56 |     ref_class: None\n17:53:56 |     relu_dropout: 0.0\n17:53:56 |     repeat_blocking_heuristic: True\n17:53:56 |     report_filename: \n17:53:56 |     return_cand_scores: False\n17:53:56 |     save_after_valid: True\n17:53:56 |     save_every_n_secs: -1\n17:53:56 |     save_format: conversations\n17:53:56 |     share_encoders: False\n17:53:56 |     share_word_embeddings: False\n17:53:56 |     short_final_eval: False\n17:53:56 |     special_tok_lst: None\n17:53:56 |     split_lines: False\n17:53:56 |     starttime: Dec03_17-52\n17:53:56 |     task: fromfile:parlaiformat\n17:53:56 |     tensorboard_log: False\n17:53:56 |     tensorboard_logdir: None\n17:53:56 |     text_truncate: 360\n17:53:56 |     threshold: 0.5\n17:53:56 |     topk: 5\n17:53:56 |     train_predict: False\n17:53:56 |     truncate: 1024\n17:53:56 |     update_classifier_head_only: False\n17:53:56 |     update_freq: 1\n17:53:56 |     use_memories: False\n17:53:56 |     use_reply: none\n17:53:56 |     validation_cutoff: 1.0\n17:53:56 |     validation_every_n_epochs: -1\n17:53:56 |     validation_every_n_secs: 20.0\n17:53:56 |     validation_every_n_steps: -1\n17:53:56 |     validation_max_exs: -1\n17:53:56 |     validation_metric: accuracy\n17:53:56 |     validation_metric_mode: max\n17:53:56 |     validation_patience: 30\n17:53:56 |     validation_share_agent: False\n17:53:56 |     variant: xlm\n17:53:56 |     verbose: False\n17:53:56 |     wandb_entity: None\n17:53:56 |     wandb_log: False\n17:53:56 |     wandb_name: None\n17:53:56 |     wandb_project: None\n17:53:56 |     warmup_rate: 0.0001\n17:53:56 |     warmup_updates: 1000\n17:53:56 |     weight_decay: None\n17:53:56 |     world_logs: \n17:53:56 |     wrap_memory_encoder: False\n17:53:56 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:53:56 | creating task(s): fromfile:parlaiformat\n17:53:56 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:53:56 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type1/run5/data_train-b.txt\n17:54:02 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6500 6.5e-10               .6535                 .6600   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6471            .6465              .6400   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6531 11.79 551.8 479.2       0          0 34.74  200 .6500   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.02 .6706 2.955e-06 240.8 209.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 792.6 688.3        .6500\u001b[0m\n17:54:02 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .6500 6.5e-10               .6535                 .6600   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6471            .6465              .6400   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6531 11.79 551.8 479.2       0          0 34.74  200 .6500   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.02 .6706 2.955e-06 240.8 209.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 792.6 688.3        .6500\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:54:04.556373Z","iopub.execute_input":"2022-12-03T17:54:04.556771Z","iopub.status.idle":"2022-12-03T17:54:05.645893Z","shell.execute_reply.started":"2022-12-03T17:54:04.556729Z","shell.execute_reply":"2022-12-03T17:54:05.644587Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"Choose completion prev2corr1type2","metadata":{}},{"cell_type":"markdown","source":"run 1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:54:05.649127Z","iopub.execute_input":"2022-12-03T17:54:05.650064Z","iopub.status.idle":"2022-12-03T17:55:22.519505Z","shell.execute_reply.started":"2022-12-03T17:54:05.650022Z","shell.execute_reply":"2022-12-03T17:55:22.518276Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"17:54:12 | building dictionary first...\n17:54:12 | No model with opt yet at: /tmp/model1(.opt)\n17:54:12 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:54:12 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:54:12 | Using CUDA\n17:54:12 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:54:13 | num words = 54944\n17:54:17 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:54:23 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:54:23 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:54:23 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:54:23 | Opt:\n17:54:23 |     activation: gelu\n17:54:23 |     adafactor_eps: '(1e-30, 0.001)'\n17:54:23 |     adam_eps: 1e-08\n17:54:23 |     add_p1_after_newln: False\n17:54:23 |     aggregate_micro: False\n17:54:23 |     allow_missing_init_opts: False\n17:54:23 |     attention_dropout: 0.1\n17:54:23 |     batchsize: 20\n17:54:23 |     betas: '(0.9, 0.999)'\n17:54:23 |     bpe_add_prefix_space: None\n17:54:23 |     bpe_debug: False\n17:54:23 |     bpe_dropout: None\n17:54:23 |     bpe_merge: None\n17:54:23 |     bpe_vocab: None\n17:54:23 |     candidates: inline\n17:54:23 |     cap_num_predictions: 100\n17:54:23 |     checkpoint_activations: False\n17:54:23 |     class_weights: None\n17:54:23 |     classes: \"['__notok__', '__ok__']\"\n17:54:23 |     classes_from_file: None\n17:54:23 |     data_parallel: True\n17:54:23 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:54:23 |     datatype: train\n17:54:23 |     delimiter: '\\n'\n17:54:23 |     dict_class: parlai.core.dict:DictionaryAgent\n17:54:23 |     dict_endtoken: __start__\n17:54:23 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:54:23 |     dict_include_test: False\n17:54:23 |     dict_include_valid: False\n17:54:23 |     dict_initpath: None\n17:54:23 |     dict_language: english\n17:54:23 |     dict_loaded: True\n17:54:23 |     dict_lower: True\n17:54:23 |     dict_max_ngram_size: -1\n17:54:23 |     dict_maxexs: -1\n17:54:23 |     dict_maxtokens: -1\n17:54:23 |     dict_minfreq: 0\n17:54:23 |     dict_nulltoken: __null__\n17:54:23 |     dict_starttoken: __start__\n17:54:23 |     dict_textfields: text,labels\n17:54:23 |     dict_tokenizer: bpe\n17:54:23 |     dict_unktoken: __unk__\n17:54:23 |     display_examples: False\n17:54:23 |     download_path: None\n17:54:23 |     dropout: 0.1\n17:54:23 |     dynamic_batching: None\n17:54:23 |     embedding_projection: random\n17:54:23 |     embedding_size: 768\n17:54:23 |     embedding_type: random\n17:54:23 |     embeddings_scale: False\n17:54:23 |     encode_candidate_vecs: True\n17:54:23 |     encode_candidate_vecs_batchsize: 256\n17:54:23 |     eval_batchsize: None\n17:54:23 |     eval_candidates: inline\n17:54:23 |     eval_dynamic_batching: None\n17:54:23 |     evaltask: None\n17:54:23 |     ffn_size: 3072\n17:54:23 |     final_extra_opt: \n17:54:23 |     fixed_candidate_vecs: reuse\n17:54:23 |     fixed_candidates_path: None\n17:54:23 |     force_fp16_tokens: False\n17:54:23 |     fp16: True\n17:54:23 |     fp16_impl: safe\n17:54:23 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt\n17:54:23 |     fromfile_datatype_extension: False\n17:54:23 |     gpu: -1\n17:54:23 |     gradient_clip: 0.1\n17:54:23 |     hide_labels: False\n17:54:23 |     history_add_global_end_token: None\n17:54:23 |     history_reversed: False\n17:54:23 |     history_size: 20\n17:54:23 |     ignore_bad_candidates: False\n17:54:23 |     ignore_labels: None\n17:54:23 |     image_cropsize: 224\n17:54:23 |     image_mode: raw\n17:54:23 |     image_size: 256\n17:54:23 |     inference: max\n17:54:23 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:54:23 |     init_opt: None\n17:54:23 |     interactive_candidates: fixed\n17:54:23 |     interactive_mode: False\n17:54:23 |     invsqrt_lr_decay_gamma: -1\n17:54:23 |     is_debug: False\n17:54:23 |     label_truncate: 72\n17:54:23 |     learn_embeddings: True\n17:54:23 |     learn_positional_embeddings: True\n17:54:23 |     learningrate: 5e-05\n17:54:23 |     load_from_checkpoint: False\n17:54:23 |     load_from_pretrained_ranker: True\n17:54:23 |     log_every_n_secs: 10.0\n17:54:23 |     log_every_n_steps: 50\n17:54:23 |     log_keep_fields: all\n17:54:23 |     loglevel: info\n17:54:23 |     lr_scheduler: reduceonplateau\n17:54:23 |     lr_scheduler_decay: 0.5\n17:54:23 |     lr_scheduler_patience: 3\n17:54:23 |     max_train_steps: -1\n17:54:23 |     max_train_time: 7200.0\n17:54:23 |     memory_attention: sqrt\n17:54:23 |     metrics: default\n17:54:23 |     model: transformer/classifier\n17:54:23 |     model_file: /tmp/model1\n17:54:23 |     model_parallel: False\n17:54:23 |     momentum: 0\n17:54:23 |     multitask_weights: [1]\n17:54:23 |     mutators: None\n17:54:23 |     n_decoder_layers: -1\n17:54:23 |     n_encoder_layers: -1\n17:54:23 |     n_heads: 12\n17:54:23 |     n_layers: 12\n17:54:23 |     n_positions: 1024\n17:54:23 |     n_segments: 2\n17:54:23 |     nesterov: True\n17:54:23 |     no_cuda: False\n17:54:23 |     normalize_sent_emb: False\n17:54:23 |     num_epochs: -1\n17:54:23 |     num_workers: 0\n17:54:23 |     nus: (0.7,)\n17:54:23 |     optimizer: adamax\n17:54:23 |     output_scaling: 0.06\n17:54:23 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n17:54:23 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:54:23 |     person_tokens: False\n17:54:23 |     print_scores: False\n17:54:23 |     rank_candidates: False\n17:54:23 |     rank_top_k: -1\n17:54:23 |     reduction_type: mean\n17:54:23 |     ref_class: None\n17:54:23 |     relu_dropout: 0.0\n17:54:23 |     repeat_blocking_heuristic: True\n17:54:23 |     return_cand_scores: False\n17:54:23 |     save_after_valid: True\n17:54:23 |     save_every_n_secs: -1\n17:54:23 |     save_format: conversations\n17:54:23 |     share_encoders: False\n17:54:23 |     share_word_embeddings: False\n17:54:23 |     short_final_eval: False\n17:54:23 |     special_tok_lst: None\n17:54:23 |     split_lines: False\n17:54:23 |     starttime: Dec03_17-54\n17:54:23 |     task: fromfile:parlaiformat\n17:54:23 |     tensorboard_log: False\n17:54:23 |     tensorboard_logdir: None\n17:54:23 |     text_truncate: 360\n17:54:23 |     threshold: 0.5\n17:54:23 |     topk: 5\n17:54:23 |     train_predict: False\n17:54:23 |     truncate: 1024\n17:54:23 |     update_classifier_head_only: False\n17:54:23 |     update_freq: 1\n17:54:23 |     use_memories: False\n17:54:23 |     use_reply: none\n17:54:23 |     validation_cutoff: 1.0\n17:54:23 |     validation_every_n_epochs: -1\n17:54:23 |     validation_every_n_secs: 20.0\n17:54:23 |     validation_every_n_steps: -1\n17:54:23 |     validation_max_exs: -1\n17:54:23 |     validation_metric: accuracy\n17:54:23 |     validation_metric_mode: max\n17:54:23 |     validation_patience: 30\n17:54:23 |     validation_share_agent: False\n17:54:23 |     variant: xlm\n17:54:23 |     verbose: False\n17:54:23 |     wandb_entity: None\n17:54:23 |     wandb_log: False\n17:54:23 |     wandb_name: None\n17:54:23 |     wandb_project: None\n17:54:23 |     warmup_rate: 0.0001\n17:54:23 |     warmup_updates: 1000\n17:54:23 |     weight_decay: None\n17:54:23 |     world_logs: \n17:54:23 |     wrap_memory_encoder: False\n17:54:23 | creating task(s): fromfile:parlaiformat\n17:54:23 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt\n17:54:23 | training...\n17:54:33 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6325 6.325e-10               .4453                 .9516   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2906            .7252              .5740   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9848  11.7     1 274.1 541.7       0          0 39.53  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6325             32768  2.611    .1189 6.015 .6886 1.005e-06 120.3 237.8   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 394.4 779.5 1.981        .5832\n\n17:54:43 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7553 7.553e-10               .6310                 .9755   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4663            .8169              .6951   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9905 11.53     1 270.5  1055       0          0 77.98  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7553             32768  2.548    .1189 5.897 .6419 2.905e-06 117.9 459.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 388.4 1515 3.909        .7335\n\n17:54:43 | creating task(s): fromfile:parlaiformat\n17:54:43 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:54:43 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt\n17:54:43 | running eval: valid\n17:54:43 | eval completed in 0.20s\n17:54:43 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8333 8.333e-10               .8000                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6667            .8571              .7500   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                      1 11.46 161.5  1822       0          0 135.3   24 .8333   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5852 2.905e-06    72   812       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 233.5 2634        .8286\n\u001b[0m\n17:54:43 | \u001b[1;32mnew best accuracy: 0.8333\u001b[0m\n17:54:43 | saving best valid model: /tmp/model1\n17:54:43 | Saving dictionary to /tmp/model1.dict\n17:54:47 | saving model checkpoint: /tmp/model1.checkpoint\n17:54:47 | Saving dictionary to /tmp/model1.checkpoint.dict\n17:55:04 | time:41s total_exs:1880 total_steps:94 epochs:78.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9319 9.319e-10               .9322                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8731            .9317              .8721   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 10.99     1 259.9 918.9       0          0 70.72  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9319             32768  3.289    .1189 6.072 .5275 4.705e-06 121.4 429.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   94 381.3 1348 3.544        .9320\n\n17:55:07 | time:44s total_exs:2080 total_steps:104 epochs:86.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.31     1 266.3 988.4       0          0 74.23  200   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  3.243    .1189  5.96 .3786 5.204e-06 119.2 442.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  104 385.5 1431 3.743            1\n\n17:55:07 | running eval: valid\n17:55:07 | eval completed in 0.19s\n17:55:07 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1839       0          0 136.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3007 5.204e-06    72 819.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 233.5 2659            1\n\u001b[0m\n17:55:07 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.8333)\u001b[0m\n17:55:07 | saving best valid model: /tmp/model1\n17:55:12 | task solved! stopping.\n17:55:12 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:55:12 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:55:12 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:55:12 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:55:12 | Using CUDA\n17:55:12 | loading dictionary from /tmp/model1.dict\n17:55:12 | num words = 54944\n17:55:17 | Loading existing model parameters from /tmp/model1\n17:55:19 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:55:20 | creating task(s): fromfile:parlaiformat\n17:55:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:55:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt\n17:55:20 | running eval: valid\n17:55:20 | eval completed in 0.21s\n17:55:20 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1763       0          0 130.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3007 5.204e-06    72   786       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 233.5 2549            1\n\u001b[0m\n17:55:20 | creating task(s): fromfile:parlaiformat\n17:55:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:55:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt\n17:55:20 | running eval: test\n17:55:20 | eval completed in 0.20s\n17:55:20 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1821       0          0 135.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3007 5.204e-06    72 811.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 233.5 2633            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:55:22.522121Z","iopub.execute_input":"2022-12-03T17:55:22.522441Z","iopub.status.idle":"2022-12-03T17:55:51.529104Z","shell.execute_reply.started":"2022-12-03T17:55:22.522412Z","shell.execute_reply":"2022-12-03T17:55:51.527863Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"17:55:29 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt)\u001b[0m\n17:55:29 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:55:29 | Using CUDA\n17:55:29 | loading dictionary from /tmp/model1.dict\n17:55:29 | num words = 54944\n17:55:34 | Loading existing model parameters from /tmp/model1\n17:55:42 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:55:43 | Opt:\n17:55:43 |     activation: gelu\n17:55:43 |     adafactor_eps: '[1e-30, 0.001]'\n17:55:43 |     adam_eps: 1e-08\n17:55:43 |     add_p1_after_newln: False\n17:55:43 |     aggregate_micro: False\n17:55:43 |     allow_missing_init_opts: False\n17:55:43 |     area_under_curve_class: None\n17:55:43 |     area_under_curve_digits: -1\n17:55:43 |     attention_dropout: 0.1\n17:55:43 |     batchsize: 40\n17:55:43 |     betas: '[0.9, 0.999]'\n17:55:43 |     bpe_add_prefix_space: None\n17:55:43 |     bpe_debug: False\n17:55:43 |     bpe_dropout: None\n17:55:43 |     bpe_merge: None\n17:55:43 |     bpe_vocab: None\n17:55:43 |     candidates: inline\n17:55:43 |     cap_num_predictions: 100\n17:55:43 |     checkpoint_activations: False\n17:55:43 |     class_weights: None\n17:55:43 |     classes: \"['__notok__', '__ok__']\"\n17:55:43 |     classes_from_file: None\n17:55:43 |     data_parallel: True\n17:55:43 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:55:43 |     datatype: train\n17:55:43 |     delimiter: '\\n'\n17:55:43 |     dict_class: parlai.core.dict:DictionaryAgent\n17:55:43 |     dict_endtoken: __start__\n17:55:43 |     dict_file: /tmp/model1.dict\n17:55:43 |     dict_include_test: False\n17:55:43 |     dict_include_valid: False\n17:55:43 |     dict_initpath: None\n17:55:43 |     dict_language: english\n17:55:43 |     dict_loaded: True\n17:55:43 |     dict_lower: True\n17:55:43 |     dict_max_ngram_size: -1\n17:55:43 |     dict_maxexs: -1\n17:55:43 |     dict_maxtokens: -1\n17:55:43 |     dict_minfreq: 0\n17:55:43 |     dict_nulltoken: __null__\n17:55:43 |     dict_starttoken: __start__\n17:55:43 |     dict_textfields: text,labels\n17:55:43 |     dict_tokenizer: bpe\n17:55:43 |     dict_unktoken: __unk__\n17:55:43 |     display_examples: False\n17:55:43 |     download_path: None\n17:55:43 |     dropout: 0.1\n17:55:43 |     dynamic_batching: None\n17:55:43 |     embedding_projection: random\n17:55:43 |     embedding_size: 768\n17:55:43 |     embedding_type: random\n17:55:43 |     embeddings_scale: False\n17:55:43 |     encode_candidate_vecs: True\n17:55:43 |     encode_candidate_vecs_batchsize: 256\n17:55:43 |     eval_batchsize: None\n17:55:43 |     eval_candidates: inline\n17:55:43 |     eval_dynamic_batching: None\n17:55:43 |     evaltask: None\n17:55:43 |     ffn_size: 3072\n17:55:43 |     final_extra_opt: \n17:55:43 |     fixed_candidate_vecs: reuse\n17:55:43 |     fixed_candidates_path: None\n17:55:43 |     force_fp16_tokens: True\n17:55:43 |     fp16: True\n17:55:43 |     fp16_impl: safe\n17:55:43 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-a.txt\n17:55:43 |     fromfile_datatype_extension: False\n17:55:43 |     gpu: -1\n17:55:43 |     gradient_clip: 0.1\n17:55:43 |     hide_labels: False\n17:55:43 |     history_add_global_end_token: None\n17:55:43 |     history_reversed: False\n17:55:43 |     history_size: 20\n17:55:43 |     ignore_bad_candidates: False\n17:55:43 |     ignore_labels: None\n17:55:43 |     image_cropsize: 224\n17:55:43 |     image_mode: raw\n17:55:43 |     image_size: 256\n17:55:43 |     inference: max\n17:55:43 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:55:43 |     init_opt: None\n17:55:43 |     interactive_candidates: fixed\n17:55:43 |     interactive_mode: False\n17:55:43 |     invsqrt_lr_decay_gamma: -1\n17:55:43 |     is_debug: False\n17:55:43 |     label_truncate: 72\n17:55:43 |     learn_embeddings: True\n17:55:43 |     learn_positional_embeddings: True\n17:55:43 |     learningrate: 5e-05\n17:55:43 |     load_from_pretrained_ranker: True\n17:55:43 |     log_every_n_secs: 10.0\n17:55:43 |     log_every_n_steps: 50\n17:55:43 |     log_keep_fields: all\n17:55:43 |     loglevel: info\n17:55:43 |     lr_scheduler: reduceonplateau\n17:55:43 |     lr_scheduler_decay: 0.5\n17:55:43 |     lr_scheduler_patience: 3\n17:55:43 |     max_train_steps: -1\n17:55:43 |     max_train_time: 7200.0\n17:55:43 |     memory_attention: sqrt\n17:55:43 |     metrics: default\n17:55:43 |     model: transformer/classifier\n17:55:43 |     model_file: /tmp/model1\n17:55:43 |     model_parallel: False\n17:55:43 |     momentum: 0\n17:55:43 |     multitask_weights: [1]\n17:55:43 |     mutators: None\n17:55:43 |     n_decoder_layers: -1\n17:55:43 |     n_encoder_layers: -1\n17:55:43 |     n_heads: 12\n17:55:43 |     n_layers: 12\n17:55:43 |     n_positions: 1024\n17:55:43 |     n_segments: 2\n17:55:43 |     nesterov: True\n17:55:43 |     no_cuda: False\n17:55:43 |     normalize_sent_emb: False\n17:55:43 |     num_epochs: -1\n17:55:43 |     num_examples: -1\n17:55:43 |     num_workers: 0\n17:55:43 |     nus: [0.7]\n17:55:43 |     optimizer: adamax\n17:55:43 |     output_scaling: 0.06\n17:55:43 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n17:55:43 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:55:43 |     person_tokens: False\n17:55:43 |     print_scores: False\n17:55:43 |     rank_candidates: False\n17:55:43 |     rank_top_k: -1\n17:55:43 |     reduction_type: mean\n17:55:43 |     ref_class: None\n17:55:43 |     relu_dropout: 0.0\n17:55:43 |     repeat_blocking_heuristic: True\n17:55:43 |     report_filename: \n17:55:43 |     return_cand_scores: False\n17:55:43 |     save_after_valid: True\n17:55:43 |     save_every_n_secs: -1\n17:55:43 |     save_format: conversations\n17:55:43 |     share_encoders: False\n17:55:43 |     share_word_embeddings: False\n17:55:43 |     short_final_eval: False\n17:55:43 |     special_tok_lst: None\n17:55:43 |     split_lines: False\n17:55:43 |     starttime: Dec03_17-54\n17:55:43 |     task: fromfile:parlaiformat\n17:55:43 |     tensorboard_log: False\n17:55:43 |     tensorboard_logdir: None\n17:55:43 |     text_truncate: 360\n17:55:43 |     threshold: 0.5\n17:55:43 |     topk: 5\n17:55:43 |     train_predict: False\n17:55:43 |     truncate: 1024\n17:55:43 |     update_classifier_head_only: False\n17:55:43 |     update_freq: 1\n17:55:43 |     use_memories: False\n17:55:43 |     use_reply: none\n17:55:43 |     validation_cutoff: 1.0\n17:55:43 |     validation_every_n_epochs: -1\n17:55:43 |     validation_every_n_secs: 20.0\n17:55:43 |     validation_every_n_steps: -1\n17:55:43 |     validation_max_exs: -1\n17:55:43 |     validation_metric: accuracy\n17:55:43 |     validation_metric_mode: max\n17:55:43 |     validation_patience: 30\n17:55:43 |     validation_share_agent: False\n17:55:43 |     variant: xlm\n17:55:43 |     verbose: False\n17:55:43 |     wandb_entity: None\n17:55:43 |     wandb_log: False\n17:55:43 |     wandb_name: None\n17:55:43 |     wandb_project: None\n17:55:43 |     warmup_rate: 0.0001\n17:55:43 |     warmup_updates: 1000\n17:55:43 |     weight_decay: None\n17:55:43 |     world_logs: \n17:55:43 |     wrap_memory_encoder: False\n17:55:44 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:55:44 | creating task(s): fromfile:parlaiformat\n17:55:44 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:55:44 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-a.txt\n17:55:49 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1200 1.2e-10               .1776                 .1667   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1900           .05376             .05814   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .0500 11.13 525.2 509.4       0          0 38.79  200 .1200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .9713 5.204e-06   240 232.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 765.2 742.1        .1157\u001b[0m\n17:55:49 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1200 1.2e-10               .1776                 .1667   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1900           .05376             .05814   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .0500 11.13 525.2 509.4       0          0 38.79  200 .1200   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .9713 5.204e-06   240 232.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 765.2 742.1        .1157\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:55:51.530980Z","iopub.execute_input":"2022-12-03T17:55:51.531465Z","iopub.status.idle":"2022-12-03T17:56:17.704306Z","shell.execute_reply.started":"2022-12-03T17:55:51.531392Z","shell.execute_reply":"2022-12-03T17:56:17.703115Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"17:55:58 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_valid.txt)\u001b[0m\n17:55:58 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:55:58 | Using CUDA\n17:55:58 | loading dictionary from /tmp/model1.dict\n17:55:58 | num words = 54944\n17:56:03 | Loading existing model parameters from /tmp/model1\n17:56:08 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:56:10 | Opt:\n17:56:10 |     activation: gelu\n17:56:10 |     adafactor_eps: '[1e-30, 0.001]'\n17:56:10 |     adam_eps: 1e-08\n17:56:10 |     add_p1_after_newln: False\n17:56:10 |     aggregate_micro: False\n17:56:10 |     allow_missing_init_opts: False\n17:56:10 |     area_under_curve_class: None\n17:56:10 |     area_under_curve_digits: -1\n17:56:10 |     attention_dropout: 0.1\n17:56:10 |     batchsize: 40\n17:56:10 |     betas: '[0.9, 0.999]'\n17:56:10 |     bpe_add_prefix_space: None\n17:56:10 |     bpe_debug: False\n17:56:10 |     bpe_dropout: None\n17:56:10 |     bpe_merge: None\n17:56:10 |     bpe_vocab: None\n17:56:10 |     candidates: inline\n17:56:10 |     cap_num_predictions: 100\n17:56:10 |     checkpoint_activations: False\n17:56:10 |     class_weights: None\n17:56:10 |     classes: \"['__notok__', '__ok__']\"\n17:56:10 |     classes_from_file: None\n17:56:10 |     data_parallel: True\n17:56:10 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:56:10 |     datatype: train\n17:56:10 |     delimiter: '\\n'\n17:56:10 |     dict_class: parlai.core.dict:DictionaryAgent\n17:56:10 |     dict_endtoken: __start__\n17:56:10 |     dict_file: /tmp/model1.dict\n17:56:10 |     dict_include_test: False\n17:56:10 |     dict_include_valid: False\n17:56:10 |     dict_initpath: None\n17:56:10 |     dict_language: english\n17:56:10 |     dict_loaded: True\n17:56:10 |     dict_lower: True\n17:56:10 |     dict_max_ngram_size: -1\n17:56:10 |     dict_maxexs: -1\n17:56:10 |     dict_maxtokens: -1\n17:56:10 |     dict_minfreq: 0\n17:56:10 |     dict_nulltoken: __null__\n17:56:10 |     dict_starttoken: __start__\n17:56:10 |     dict_textfields: text,labels\n17:56:10 |     dict_tokenizer: bpe\n17:56:10 |     dict_unktoken: __unk__\n17:56:10 |     display_examples: False\n17:56:10 |     download_path: None\n17:56:10 |     dropout: 0.1\n17:56:10 |     dynamic_batching: None\n17:56:10 |     embedding_projection: random\n17:56:10 |     embedding_size: 768\n17:56:10 |     embedding_type: random\n17:56:10 |     embeddings_scale: False\n17:56:10 |     encode_candidate_vecs: True\n17:56:10 |     encode_candidate_vecs_batchsize: 256\n17:56:10 |     eval_batchsize: None\n17:56:10 |     eval_candidates: inline\n17:56:10 |     eval_dynamic_batching: None\n17:56:10 |     evaltask: None\n17:56:10 |     ffn_size: 3072\n17:56:10 |     final_extra_opt: \n17:56:10 |     fixed_candidate_vecs: reuse\n17:56:10 |     fixed_candidates_path: None\n17:56:10 |     force_fp16_tokens: True\n17:56:10 |     fp16: True\n17:56:10 |     fp16_impl: safe\n17:56:10 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-b.txt\n17:56:10 |     fromfile_datatype_extension: False\n17:56:10 |     gpu: -1\n17:56:10 |     gradient_clip: 0.1\n17:56:10 |     hide_labels: False\n17:56:10 |     history_add_global_end_token: None\n17:56:10 |     history_reversed: False\n17:56:10 |     history_size: 20\n17:56:10 |     ignore_bad_candidates: False\n17:56:10 |     ignore_labels: None\n17:56:10 |     image_cropsize: 224\n17:56:10 |     image_mode: raw\n17:56:10 |     image_size: 256\n17:56:10 |     inference: max\n17:56:10 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:56:10 |     init_opt: None\n17:56:10 |     interactive_candidates: fixed\n17:56:10 |     interactive_mode: False\n17:56:10 |     invsqrt_lr_decay_gamma: -1\n17:56:10 |     is_debug: False\n17:56:10 |     label_truncate: 72\n17:56:10 |     learn_embeddings: True\n17:56:10 |     learn_positional_embeddings: True\n17:56:10 |     learningrate: 5e-05\n17:56:10 |     load_from_pretrained_ranker: True\n17:56:10 |     log_every_n_secs: 10.0\n17:56:10 |     log_every_n_steps: 50\n17:56:10 |     log_keep_fields: all\n17:56:10 |     loglevel: info\n17:56:10 |     lr_scheduler: reduceonplateau\n17:56:10 |     lr_scheduler_decay: 0.5\n17:56:10 |     lr_scheduler_patience: 3\n17:56:10 |     max_train_steps: -1\n17:56:10 |     max_train_time: 7200.0\n17:56:10 |     memory_attention: sqrt\n17:56:10 |     metrics: default\n17:56:10 |     model: transformer/classifier\n17:56:10 |     model_file: /tmp/model1\n17:56:10 |     model_parallel: False\n17:56:10 |     momentum: 0\n17:56:10 |     multitask_weights: [1]\n17:56:10 |     mutators: None\n17:56:10 |     n_decoder_layers: -1\n17:56:10 |     n_encoder_layers: -1\n17:56:10 |     n_heads: 12\n17:56:10 |     n_layers: 12\n17:56:10 |     n_positions: 1024\n17:56:10 |     n_segments: 2\n17:56:10 |     nesterov: True\n17:56:10 |     no_cuda: False\n17:56:10 |     normalize_sent_emb: False\n17:56:10 |     num_epochs: -1\n17:56:10 |     num_examples: -1\n17:56:10 |     num_workers: 0\n17:56:10 |     nus: [0.7]\n17:56:10 |     optimizer: adamax\n17:56:10 |     output_scaling: 0.06\n17:56:10 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n17:56:10 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:56:10 |     person_tokens: False\n17:56:10 |     print_scores: False\n17:56:10 |     rank_candidates: False\n17:56:10 |     rank_top_k: -1\n17:56:10 |     reduction_type: mean\n17:56:10 |     ref_class: None\n17:56:10 |     relu_dropout: 0.0\n17:56:10 |     repeat_blocking_heuristic: True\n17:56:10 |     report_filename: \n17:56:10 |     return_cand_scores: False\n17:56:10 |     save_after_valid: True\n17:56:10 |     save_every_n_secs: -1\n17:56:10 |     save_format: conversations\n17:56:10 |     share_encoders: False\n17:56:10 |     share_word_embeddings: False\n17:56:10 |     short_final_eval: False\n17:56:10 |     special_tok_lst: None\n17:56:10 |     split_lines: False\n17:56:10 |     starttime: Dec03_17-54\n17:56:10 |     task: fromfile:parlaiformat\n17:56:10 |     tensorboard_log: False\n17:56:10 |     tensorboard_logdir: None\n17:56:10 |     text_truncate: 360\n17:56:10 |     threshold: 0.5\n17:56:10 |     topk: 5\n17:56:10 |     train_predict: False\n17:56:10 |     truncate: 1024\n17:56:10 |     update_classifier_head_only: False\n17:56:10 |     update_freq: 1\n17:56:10 |     use_memories: False\n17:56:10 |     use_reply: none\n17:56:10 |     validation_cutoff: 1.0\n17:56:10 |     validation_every_n_epochs: -1\n17:56:10 |     validation_every_n_secs: 20.0\n17:56:10 |     validation_every_n_steps: -1\n17:56:10 |     validation_max_exs: -1\n17:56:10 |     validation_metric: accuracy\n17:56:10 |     validation_metric_mode: max\n17:56:10 |     validation_patience: 30\n17:56:10 |     validation_share_agent: False\n17:56:10 |     variant: xlm\n17:56:10 |     verbose: False\n17:56:10 |     wandb_entity: None\n17:56:10 |     wandb_log: False\n17:56:10 |     wandb_name: None\n17:56:10 |     wandb_project: None\n17:56:10 |     warmup_rate: 0.0001\n17:56:10 |     warmup_updates: 1000\n17:56:10 |     weight_decay: None\n17:56:10 |     world_logs: \n17:56:10 |     wrap_memory_encoder: False\n17:56:10 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:56:10 | creating task(s): fromfile:parlaiformat\n17:56:10 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:56:10 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run1/data_train-b.txt\n17:56:16 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8800 8.8e-10               .8879                 .8333   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9500            .8710              .9419   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8100 11.13 525.2 497.2       0          0 37.87  200 .8800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .5027 5.204e-06   240 227.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 765.2 724.4        .8794\u001b[0m\n17:56:16 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8800 8.8e-10               .8879                 .8333   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9500            .8710              .9419   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8100 11.13 525.2 497.2       0          0 37.87  200 .8800   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .5027 5.204e-06   240 227.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 765.2 724.4        .8794\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:56:17.706173Z","iopub.execute_input":"2022-12-03T17:56:17.706595Z","iopub.status.idle":"2022-12-03T17:56:18.849857Z","shell.execute_reply.started":"2022-12-03T17:56:17.706553Z","shell.execute_reply":"2022-12-03T17:56:18.848550Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:56:18.852137Z","iopub.execute_input":"2022-12-03T17:56:18.852558Z","iopub.status.idle":"2022-12-03T17:57:13.482860Z","shell.execute_reply.started":"2022-12-03T17:56:18.852517Z","shell.execute_reply":"2022-12-03T17:57:13.481598Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"17:56:26 | building dictionary first...\n17:56:26 | No model with opt yet at: /tmp/model2(.opt)\n17:56:26 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:56:26 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:56:26 | Using CUDA\n17:56:26 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:56:26 | num words = 54944\n17:56:30 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:56:36 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:56:36 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:56:36 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:56:36 | Opt:\n17:56:36 |     activation: gelu\n17:56:36 |     adafactor_eps: '(1e-30, 0.001)'\n17:56:36 |     adam_eps: 1e-08\n17:56:36 |     add_p1_after_newln: False\n17:56:36 |     aggregate_micro: False\n17:56:36 |     allow_missing_init_opts: False\n17:56:36 |     attention_dropout: 0.1\n17:56:36 |     batchsize: 20\n17:56:36 |     betas: '(0.9, 0.999)'\n17:56:36 |     bpe_add_prefix_space: None\n17:56:36 |     bpe_debug: False\n17:56:36 |     bpe_dropout: None\n17:56:36 |     bpe_merge: None\n17:56:36 |     bpe_vocab: None\n17:56:36 |     candidates: inline\n17:56:36 |     cap_num_predictions: 100\n17:56:36 |     checkpoint_activations: False\n17:56:36 |     class_weights: None\n17:56:36 |     classes: \"['__notok__', '__ok__']\"\n17:56:36 |     classes_from_file: None\n17:56:36 |     data_parallel: True\n17:56:36 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:56:36 |     datatype: train\n17:56:36 |     delimiter: '\\n'\n17:56:36 |     dict_class: parlai.core.dict:DictionaryAgent\n17:56:36 |     dict_endtoken: __start__\n17:56:36 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:56:36 |     dict_include_test: False\n17:56:36 |     dict_include_valid: False\n17:56:36 |     dict_initpath: None\n17:56:36 |     dict_language: english\n17:56:36 |     dict_loaded: True\n17:56:36 |     dict_lower: True\n17:56:36 |     dict_max_ngram_size: -1\n17:56:36 |     dict_maxexs: -1\n17:56:36 |     dict_maxtokens: -1\n17:56:36 |     dict_minfreq: 0\n17:56:36 |     dict_nulltoken: __null__\n17:56:36 |     dict_starttoken: __start__\n17:56:36 |     dict_textfields: text,labels\n17:56:36 |     dict_tokenizer: bpe\n17:56:36 |     dict_unktoken: __unk__\n17:56:36 |     display_examples: False\n17:56:36 |     download_path: None\n17:56:36 |     dropout: 0.1\n17:56:36 |     dynamic_batching: None\n17:56:36 |     embedding_projection: random\n17:56:36 |     embedding_size: 768\n17:56:36 |     embedding_type: random\n17:56:36 |     embeddings_scale: False\n17:56:36 |     encode_candidate_vecs: True\n17:56:36 |     encode_candidate_vecs_batchsize: 256\n17:56:36 |     eval_batchsize: None\n17:56:36 |     eval_candidates: inline\n17:56:36 |     eval_dynamic_batching: None\n17:56:36 |     evaltask: None\n17:56:36 |     ffn_size: 3072\n17:56:36 |     final_extra_opt: \n17:56:36 |     fixed_candidate_vecs: reuse\n17:56:36 |     fixed_candidates_path: None\n17:56:36 |     force_fp16_tokens: False\n17:56:36 |     fp16: True\n17:56:36 |     fp16_impl: safe\n17:56:36 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt\n17:56:36 |     fromfile_datatype_extension: False\n17:56:36 |     gpu: -1\n17:56:36 |     gradient_clip: 0.1\n17:56:36 |     hide_labels: False\n17:56:36 |     history_add_global_end_token: None\n17:56:36 |     history_reversed: False\n17:56:36 |     history_size: 20\n17:56:36 |     ignore_bad_candidates: False\n17:56:36 |     ignore_labels: None\n17:56:36 |     image_cropsize: 224\n17:56:36 |     image_mode: raw\n17:56:36 |     image_size: 256\n17:56:36 |     inference: max\n17:56:36 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:56:36 |     init_opt: None\n17:56:36 |     interactive_candidates: fixed\n17:56:36 |     interactive_mode: False\n17:56:36 |     invsqrt_lr_decay_gamma: -1\n17:56:36 |     is_debug: False\n17:56:36 |     label_truncate: 72\n17:56:36 |     learn_embeddings: True\n17:56:36 |     learn_positional_embeddings: True\n17:56:36 |     learningrate: 5e-05\n17:56:36 |     load_from_checkpoint: False\n17:56:36 |     load_from_pretrained_ranker: True\n17:56:36 |     log_every_n_secs: 10.0\n17:56:36 |     log_every_n_steps: 50\n17:56:36 |     log_keep_fields: all\n17:56:36 |     loglevel: info\n17:56:36 |     lr_scheduler: reduceonplateau\n17:56:36 |     lr_scheduler_decay: 0.5\n17:56:36 |     lr_scheduler_patience: 3\n17:56:36 |     max_train_steps: -1\n17:56:36 |     max_train_time: 7200.0\n17:56:36 |     memory_attention: sqrt\n17:56:36 |     metrics: default\n17:56:36 |     model: transformer/classifier\n17:56:36 |     model_file: /tmp/model2\n17:56:36 |     model_parallel: False\n17:56:36 |     momentum: 0\n17:56:36 |     multitask_weights: [1]\n17:56:36 |     mutators: None\n17:56:36 |     n_decoder_layers: -1\n17:56:36 |     n_encoder_layers: -1\n17:56:36 |     n_heads: 12\n17:56:36 |     n_layers: 12\n17:56:36 |     n_positions: 1024\n17:56:36 |     n_segments: 2\n17:56:36 |     nesterov: True\n17:56:36 |     no_cuda: False\n17:56:36 |     normalize_sent_emb: False\n17:56:36 |     num_epochs: -1\n17:56:36 |     num_workers: 0\n17:56:36 |     nus: (0.7,)\n17:56:36 |     optimizer: adamax\n17:56:36 |     output_scaling: 0.06\n17:56:36 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n17:56:36 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:56:36 |     person_tokens: False\n17:56:36 |     print_scores: False\n17:56:36 |     rank_candidates: False\n17:56:36 |     rank_top_k: -1\n17:56:36 |     reduction_type: mean\n17:56:36 |     ref_class: None\n17:56:36 |     relu_dropout: 0.0\n17:56:36 |     repeat_blocking_heuristic: True\n17:56:36 |     return_cand_scores: False\n17:56:36 |     save_after_valid: True\n17:56:36 |     save_every_n_secs: -1\n17:56:36 |     save_format: conversations\n17:56:36 |     share_encoders: False\n17:56:36 |     share_word_embeddings: False\n17:56:36 |     short_final_eval: False\n17:56:36 |     special_tok_lst: None\n17:56:36 |     split_lines: False\n17:56:36 |     starttime: Dec03_17-56\n17:56:36 |     task: fromfile:parlaiformat\n17:56:36 |     tensorboard_log: False\n17:56:36 |     tensorboard_logdir: None\n17:56:36 |     text_truncate: 360\n17:56:36 |     threshold: 0.5\n17:56:36 |     topk: 5\n17:56:36 |     train_predict: False\n17:56:36 |     truncate: 1024\n17:56:36 |     update_classifier_head_only: False\n17:56:36 |     update_freq: 1\n17:56:36 |     use_memories: False\n17:56:36 |     use_reply: none\n17:56:36 |     validation_cutoff: 1.0\n17:56:36 |     validation_every_n_epochs: -1\n17:56:36 |     validation_every_n_secs: 20.0\n17:56:36 |     validation_every_n_steps: -1\n17:56:36 |     validation_max_exs: -1\n17:56:36 |     validation_metric: accuracy\n17:56:36 |     validation_metric_mode: max\n17:56:36 |     validation_patience: 30\n17:56:36 |     validation_share_agent: False\n17:56:36 |     variant: xlm\n17:56:36 |     verbose: False\n17:56:36 |     wandb_entity: None\n17:56:36 |     wandb_log: False\n17:56:36 |     wandb_name: None\n17:56:36 |     wandb_project: None\n17:56:36 |     warmup_rate: 0.0001\n17:56:36 |     warmup_updates: 1000\n17:56:36 |     weight_decay: None\n17:56:36 |     world_logs: \n17:56:36 |     wrap_memory_encoder: False\n17:56:36 | creating task(s): fromfile:parlaiformat\n17:56:36 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt\n17:56:36 | training...\n17:56:47 | time:10s total_exs:380 total_steps:19 epochs:15.83\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5789 5.789e-10               .5876                 .5907   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5846            .5699              .5668   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5730 10.85     1 256.9 482.8       0          0 37.58  380   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5789             32768  2.704    .1207 6.026 .6899 9.549e-07 120.5 226.5   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   19 377.5 709.2 1.883        .5790\n\n17:56:57 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8053 8.053e-10               .8047                 .8112   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7984            .8058              .7995   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8122 10.92     1 258.4  1004       0          0 77.74  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8053             32768  2.759    .1207 6.005 .6409 2.855e-06 120.1 466.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 378.5 1471 3.896        .8053\n\n17:56:57 | creating task(s): fromfile:parlaiformat\n17:56:57 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:56:57 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt\n17:56:57 | running eval: valid\n17:56:57 | eval completed in 0.19s\n17:56:57 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1792       0          0 137.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0809     6 .5732 2.855e-06    72 826.8       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  228 2619            1\n\u001b[0m\n17:56:57 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:56:57 | saving best valid model: /tmp/model2\n17:56:57 | Saving dictionary to /tmp/model2.dict\n17:57:00 | task solved! stopping.\n17:57:00 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:57:00 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:57:00 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:57:00 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:57:00 | Using CUDA\n17:57:00 | loading dictionary from /tmp/model2.dict\n17:57:01 | num words = 54944\n17:57:06 | Loading existing model parameters from /tmp/model2\n17:57:09 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:57:11 | creating task(s): fromfile:parlaiformat\n17:57:11 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:57:11 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt\n17:57:11 | running eval: valid\n17:57:11 | eval completed in 0.20s\n17:57:11 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1709       0          0 131.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5732 2.855e-06    72 788.9       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  228 2498            1\n\u001b[0m\n17:57:11 | creating task(s): fromfile:parlaiformat\n17:57:11 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:57:11 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt\n17:57:11 | running eval: test\n17:57:11 | eval completed in 0.19s\n17:57:11 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1838       0          0 141.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5732 2.855e-06    72 848.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  228 2686            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:57:13.486161Z","iopub.execute_input":"2022-12-03T17:57:13.486602Z","iopub.status.idle":"2022-12-03T17:57:43.795987Z","shell.execute_reply.started":"2022-12-03T17:57:13.486542Z","shell.execute_reply":"2022-12-03T17:57:43.794713Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"17:57:22 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt)\u001b[0m\n17:57:22 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:57:22 | Using CUDA\n17:57:22 | loading dictionary from /tmp/model2.dict\n17:57:22 | num words = 54944\n17:57:26 | Loading existing model parameters from /tmp/model2\n17:57:33 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:57:35 | Opt:\n17:57:35 |     activation: gelu\n17:57:35 |     adafactor_eps: '[1e-30, 0.001]'\n17:57:35 |     adam_eps: 1e-08\n17:57:35 |     add_p1_after_newln: False\n17:57:35 |     aggregate_micro: False\n17:57:35 |     allow_missing_init_opts: False\n17:57:35 |     area_under_curve_class: None\n17:57:35 |     area_under_curve_digits: -1\n17:57:35 |     attention_dropout: 0.1\n17:57:35 |     batchsize: 40\n17:57:35 |     betas: '[0.9, 0.999]'\n17:57:35 |     bpe_add_prefix_space: None\n17:57:35 |     bpe_debug: False\n17:57:35 |     bpe_dropout: None\n17:57:35 |     bpe_merge: None\n17:57:35 |     bpe_vocab: None\n17:57:35 |     candidates: inline\n17:57:35 |     cap_num_predictions: 100\n17:57:35 |     checkpoint_activations: False\n17:57:35 |     class_weights: None\n17:57:35 |     classes: \"['__notok__', '__ok__']\"\n17:57:35 |     classes_from_file: None\n17:57:35 |     data_parallel: True\n17:57:35 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:57:35 |     datatype: train\n17:57:35 |     delimiter: '\\n'\n17:57:35 |     dict_class: parlai.core.dict:DictionaryAgent\n17:57:35 |     dict_endtoken: __start__\n17:57:35 |     dict_file: /tmp/model2.dict\n17:57:35 |     dict_include_test: False\n17:57:35 |     dict_include_valid: False\n17:57:35 |     dict_initpath: None\n17:57:35 |     dict_language: english\n17:57:35 |     dict_loaded: True\n17:57:35 |     dict_lower: True\n17:57:35 |     dict_max_ngram_size: -1\n17:57:35 |     dict_maxexs: -1\n17:57:35 |     dict_maxtokens: -1\n17:57:35 |     dict_minfreq: 0\n17:57:35 |     dict_nulltoken: __null__\n17:57:35 |     dict_starttoken: __start__\n17:57:35 |     dict_textfields: text,labels\n17:57:35 |     dict_tokenizer: bpe\n17:57:35 |     dict_unktoken: __unk__\n17:57:35 |     display_examples: False\n17:57:35 |     download_path: None\n17:57:35 |     dropout: 0.1\n17:57:35 |     dynamic_batching: None\n17:57:35 |     embedding_projection: random\n17:57:35 |     embedding_size: 768\n17:57:35 |     embedding_type: random\n17:57:35 |     embeddings_scale: False\n17:57:35 |     encode_candidate_vecs: True\n17:57:35 |     encode_candidate_vecs_batchsize: 256\n17:57:35 |     eval_batchsize: None\n17:57:35 |     eval_candidates: inline\n17:57:35 |     eval_dynamic_batching: None\n17:57:35 |     evaltask: None\n17:57:35 |     ffn_size: 3072\n17:57:35 |     final_extra_opt: \n17:57:35 |     fixed_candidate_vecs: reuse\n17:57:35 |     fixed_candidates_path: None\n17:57:35 |     force_fp16_tokens: True\n17:57:35 |     fp16: True\n17:57:35 |     fp16_impl: safe\n17:57:35 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-a.txt\n17:57:35 |     fromfile_datatype_extension: False\n17:57:35 |     gpu: -1\n17:57:35 |     gradient_clip: 0.1\n17:57:35 |     hide_labels: False\n17:57:35 |     history_add_global_end_token: None\n17:57:35 |     history_reversed: False\n17:57:35 |     history_size: 20\n17:57:35 |     ignore_bad_candidates: False\n17:57:35 |     ignore_labels: None\n17:57:35 |     image_cropsize: 224\n17:57:35 |     image_mode: raw\n17:57:35 |     image_size: 256\n17:57:35 |     inference: max\n17:57:35 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:57:35 |     init_opt: None\n17:57:35 |     interactive_candidates: fixed\n17:57:35 |     interactive_mode: False\n17:57:35 |     invsqrt_lr_decay_gamma: -1\n17:57:35 |     is_debug: False\n17:57:35 |     label_truncate: 72\n17:57:35 |     learn_embeddings: True\n17:57:35 |     learn_positional_embeddings: True\n17:57:35 |     learningrate: 5e-05\n17:57:35 |     load_from_pretrained_ranker: True\n17:57:35 |     log_every_n_secs: 10.0\n17:57:35 |     log_every_n_steps: 50\n17:57:35 |     log_keep_fields: all\n17:57:35 |     loglevel: info\n17:57:35 |     lr_scheduler: reduceonplateau\n17:57:35 |     lr_scheduler_decay: 0.5\n17:57:35 |     lr_scheduler_patience: 3\n17:57:35 |     max_train_steps: -1\n17:57:35 |     max_train_time: 7200.0\n17:57:35 |     memory_attention: sqrt\n17:57:35 |     metrics: default\n17:57:35 |     model: transformer/classifier\n17:57:35 |     model_file: /tmp/model2\n17:57:35 |     model_parallel: False\n17:57:35 |     momentum: 0\n17:57:35 |     multitask_weights: [1]\n17:57:35 |     mutators: None\n17:57:35 |     n_decoder_layers: -1\n17:57:35 |     n_encoder_layers: -1\n17:57:35 |     n_heads: 12\n17:57:35 |     n_layers: 12\n17:57:35 |     n_positions: 1024\n17:57:35 |     n_segments: 2\n17:57:35 |     nesterov: True\n17:57:35 |     no_cuda: False\n17:57:35 |     normalize_sent_emb: False\n17:57:35 |     num_epochs: -1\n17:57:35 |     num_examples: -1\n17:57:35 |     num_workers: 0\n17:57:35 |     nus: [0.7]\n17:57:35 |     optimizer: adamax\n17:57:35 |     output_scaling: 0.06\n17:57:35 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n17:57:35 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:57:35 |     person_tokens: False\n17:57:35 |     print_scores: False\n17:57:35 |     rank_candidates: False\n17:57:35 |     rank_top_k: -1\n17:57:35 |     reduction_type: mean\n17:57:35 |     ref_class: None\n17:57:35 |     relu_dropout: 0.0\n17:57:35 |     repeat_blocking_heuristic: True\n17:57:35 |     report_filename: \n17:57:35 |     return_cand_scores: False\n17:57:35 |     save_after_valid: True\n17:57:35 |     save_every_n_secs: -1\n17:57:35 |     save_format: conversations\n17:57:35 |     share_encoders: False\n17:57:35 |     share_word_embeddings: False\n17:57:35 |     short_final_eval: False\n17:57:35 |     special_tok_lst: None\n17:57:35 |     split_lines: False\n17:57:35 |     starttime: Dec03_17-56\n17:57:35 |     task: fromfile:parlaiformat\n17:57:35 |     tensorboard_log: False\n17:57:35 |     tensorboard_logdir: None\n17:57:35 |     text_truncate: 360\n17:57:35 |     threshold: 0.5\n17:57:35 |     topk: 5\n17:57:35 |     train_predict: False\n17:57:35 |     truncate: 1024\n17:57:35 |     update_classifier_head_only: False\n17:57:35 |     update_freq: 1\n17:57:35 |     use_memories: False\n17:57:35 |     use_reply: none\n17:57:35 |     validation_cutoff: 1.0\n17:57:35 |     validation_every_n_epochs: -1\n17:57:35 |     validation_every_n_secs: 20.0\n17:57:35 |     validation_every_n_steps: -1\n17:57:35 |     validation_max_exs: -1\n17:57:35 |     validation_metric: accuracy\n17:57:35 |     validation_metric_mode: max\n17:57:35 |     validation_patience: 30\n17:57:35 |     validation_share_agent: False\n17:57:35 |     variant: xlm\n17:57:35 |     verbose: False\n17:57:35 |     wandb_entity: None\n17:57:35 |     wandb_log: False\n17:57:35 |     wandb_name: None\n17:57:35 |     wandb_project: None\n17:57:35 |     warmup_rate: 0.0001\n17:57:35 |     warmup_updates: 1000\n17:57:35 |     weight_decay: None\n17:57:35 |     world_logs: \n17:57:35 |     wrap_memory_encoder: False\n17:57:35 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:57:35 | creating task(s): fromfile:parlaiformat\n17:57:35 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:57:35 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-a.txt\n17:57:42 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3650 3.65e-10               .2983                 .3293   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2727            .4201              .3898   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4554  11.7 547.8 459.6       0          0 33.56  200 .3650   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .7231 2.855e-06 239.6   201       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 787.4 660.6        .3598\u001b[0m\n17:57:42 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3650 3.65e-10               .2983                 .3293   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2727            .4201              .3898   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4554  11.7 547.8 459.6       0          0 33.56  200 .3650   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .7231 2.855e-06 239.6   201       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 787.4 660.6        .3598\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:57:43.797824Z","iopub.execute_input":"2022-12-03T17:57:43.798203Z","iopub.status.idle":"2022-12-03T17:58:09.829874Z","shell.execute_reply.started":"2022-12-03T17:57:43.798161Z","shell.execute_reply":"2022-12-03T17:58:09.828558Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"17:57:50 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_valid.txt)\u001b[0m\n17:57:50 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:57:50 | Using CUDA\n17:57:50 | loading dictionary from /tmp/model2.dict\n17:57:50 | num words = 54944\n17:57:55 | Loading existing model parameters from /tmp/model2\n17:58:00 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:58:02 | Opt:\n17:58:02 |     activation: gelu\n17:58:02 |     adafactor_eps: '[1e-30, 0.001]'\n17:58:02 |     adam_eps: 1e-08\n17:58:02 |     add_p1_after_newln: False\n17:58:02 |     aggregate_micro: False\n17:58:02 |     allow_missing_init_opts: False\n17:58:02 |     area_under_curve_class: None\n17:58:02 |     area_under_curve_digits: -1\n17:58:02 |     attention_dropout: 0.1\n17:58:02 |     batchsize: 40\n17:58:02 |     betas: '[0.9, 0.999]'\n17:58:02 |     bpe_add_prefix_space: None\n17:58:02 |     bpe_debug: False\n17:58:02 |     bpe_dropout: None\n17:58:02 |     bpe_merge: None\n17:58:02 |     bpe_vocab: None\n17:58:02 |     candidates: inline\n17:58:02 |     cap_num_predictions: 100\n17:58:02 |     checkpoint_activations: False\n17:58:02 |     class_weights: None\n17:58:02 |     classes: \"['__notok__', '__ok__']\"\n17:58:02 |     classes_from_file: None\n17:58:02 |     data_parallel: True\n17:58:02 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:58:02 |     datatype: train\n17:58:02 |     delimiter: '\\n'\n17:58:02 |     dict_class: parlai.core.dict:DictionaryAgent\n17:58:02 |     dict_endtoken: __start__\n17:58:02 |     dict_file: /tmp/model2.dict\n17:58:02 |     dict_include_test: False\n17:58:02 |     dict_include_valid: False\n17:58:02 |     dict_initpath: None\n17:58:02 |     dict_language: english\n17:58:02 |     dict_loaded: True\n17:58:02 |     dict_lower: True\n17:58:02 |     dict_max_ngram_size: -1\n17:58:02 |     dict_maxexs: -1\n17:58:02 |     dict_maxtokens: -1\n17:58:02 |     dict_minfreq: 0\n17:58:02 |     dict_nulltoken: __null__\n17:58:02 |     dict_starttoken: __start__\n17:58:02 |     dict_textfields: text,labels\n17:58:02 |     dict_tokenizer: bpe\n17:58:02 |     dict_unktoken: __unk__\n17:58:02 |     display_examples: False\n17:58:02 |     download_path: None\n17:58:02 |     dropout: 0.1\n17:58:02 |     dynamic_batching: None\n17:58:02 |     embedding_projection: random\n17:58:02 |     embedding_size: 768\n17:58:02 |     embedding_type: random\n17:58:02 |     embeddings_scale: False\n17:58:02 |     encode_candidate_vecs: True\n17:58:02 |     encode_candidate_vecs_batchsize: 256\n17:58:02 |     eval_batchsize: None\n17:58:02 |     eval_candidates: inline\n17:58:02 |     eval_dynamic_batching: None\n17:58:02 |     evaltask: None\n17:58:02 |     ffn_size: 3072\n17:58:02 |     final_extra_opt: \n17:58:02 |     fixed_candidate_vecs: reuse\n17:58:02 |     fixed_candidates_path: None\n17:58:02 |     force_fp16_tokens: True\n17:58:02 |     fp16: True\n17:58:02 |     fp16_impl: safe\n17:58:02 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-b.txt\n17:58:02 |     fromfile_datatype_extension: False\n17:58:02 |     gpu: -1\n17:58:02 |     gradient_clip: 0.1\n17:58:02 |     hide_labels: False\n17:58:02 |     history_add_global_end_token: None\n17:58:02 |     history_reversed: False\n17:58:02 |     history_size: 20\n17:58:02 |     ignore_bad_candidates: False\n17:58:02 |     ignore_labels: None\n17:58:02 |     image_cropsize: 224\n17:58:02 |     image_mode: raw\n17:58:02 |     image_size: 256\n17:58:02 |     inference: max\n17:58:02 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:58:02 |     init_opt: None\n17:58:02 |     interactive_candidates: fixed\n17:58:02 |     interactive_mode: False\n17:58:02 |     invsqrt_lr_decay_gamma: -1\n17:58:02 |     is_debug: False\n17:58:02 |     label_truncate: 72\n17:58:02 |     learn_embeddings: True\n17:58:02 |     learn_positional_embeddings: True\n17:58:02 |     learningrate: 5e-05\n17:58:02 |     load_from_pretrained_ranker: True\n17:58:02 |     log_every_n_secs: 10.0\n17:58:02 |     log_every_n_steps: 50\n17:58:02 |     log_keep_fields: all\n17:58:02 |     loglevel: info\n17:58:02 |     lr_scheduler: reduceonplateau\n17:58:02 |     lr_scheduler_decay: 0.5\n17:58:02 |     lr_scheduler_patience: 3\n17:58:02 |     max_train_steps: -1\n17:58:02 |     max_train_time: 7200.0\n17:58:02 |     memory_attention: sqrt\n17:58:02 |     metrics: default\n17:58:02 |     model: transformer/classifier\n17:58:02 |     model_file: /tmp/model2\n17:58:02 |     model_parallel: False\n17:58:02 |     momentum: 0\n17:58:02 |     multitask_weights: [1]\n17:58:02 |     mutators: None\n17:58:02 |     n_decoder_layers: -1\n17:58:02 |     n_encoder_layers: -1\n17:58:02 |     n_heads: 12\n17:58:02 |     n_layers: 12\n17:58:02 |     n_positions: 1024\n17:58:02 |     n_segments: 2\n17:58:02 |     nesterov: True\n17:58:02 |     no_cuda: False\n17:58:02 |     normalize_sent_emb: False\n17:58:02 |     num_epochs: -1\n17:58:02 |     num_examples: -1\n17:58:02 |     num_workers: 0\n17:58:02 |     nus: [0.7]\n17:58:02 |     optimizer: adamax\n17:58:02 |     output_scaling: 0.06\n17:58:02 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n17:58:02 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:58:02 |     person_tokens: False\n17:58:02 |     print_scores: False\n17:58:02 |     rank_candidates: False\n17:58:02 |     rank_top_k: -1\n17:58:02 |     reduction_type: mean\n17:58:02 |     ref_class: None\n17:58:02 |     relu_dropout: 0.0\n17:58:02 |     repeat_blocking_heuristic: True\n17:58:02 |     report_filename: \n17:58:02 |     return_cand_scores: False\n17:58:02 |     save_after_valid: True\n17:58:02 |     save_every_n_secs: -1\n17:58:02 |     save_format: conversations\n17:58:02 |     share_encoders: False\n17:58:02 |     share_word_embeddings: False\n17:58:02 |     short_final_eval: False\n17:58:02 |     special_tok_lst: None\n17:58:02 |     split_lines: False\n17:58:02 |     starttime: Dec03_17-56\n17:58:02 |     task: fromfile:parlaiformat\n17:58:02 |     tensorboard_log: False\n17:58:02 |     tensorboard_logdir: None\n17:58:02 |     text_truncate: 360\n17:58:02 |     threshold: 0.5\n17:58:02 |     topk: 5\n17:58:02 |     train_predict: False\n17:58:02 |     truncate: 1024\n17:58:02 |     update_classifier_head_only: False\n17:58:02 |     update_freq: 1\n17:58:02 |     use_memories: False\n17:58:02 |     use_reply: none\n17:58:02 |     validation_cutoff: 1.0\n17:58:02 |     validation_every_n_epochs: -1\n17:58:02 |     validation_every_n_secs: 20.0\n17:58:02 |     validation_every_n_steps: -1\n17:58:02 |     validation_max_exs: -1\n17:58:02 |     validation_metric: accuracy\n17:58:02 |     validation_metric_mode: max\n17:58:02 |     validation_patience: 30\n17:58:02 |     validation_share_agent: False\n17:58:02 |     variant: xlm\n17:58:02 |     verbose: False\n17:58:02 |     wandb_entity: None\n17:58:02 |     wandb_log: False\n17:58:02 |     wandb_name: None\n17:58:02 |     wandb_project: None\n17:58:02 |     warmup_rate: 0.0001\n17:58:02 |     warmup_updates: 1000\n17:58:02 |     weight_decay: None\n17:58:02 |     world_logs: \n17:58:02 |     wrap_memory_encoder: False\n17:58:02 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:58:02 | creating task(s): fromfile:parlaiformat\n17:58:02 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:58:02 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run2/data_train-b.txt\n17:58:08 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6350 6.35e-10               .6011                 .6707   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5446            .6636              .6102   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7273  11.7 547.8 529.9       0          0 38.69  200 .6350   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .6688 2.855e-06 240.4 232.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 788.2 762.4        .6320\u001b[0m\n17:58:08 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6350 6.35e-10               .6011                 .6707   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5446            .6636              .6102   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7273  11.7 547.8 529.9       0          0 38.69  200 .6350   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .6688 2.855e-06 240.4 232.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 788.2 762.4        .6320\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:58:09.839717Z","iopub.execute_input":"2022-12-03T17:58:09.840036Z","iopub.status.idle":"2022-12-03T17:58:10.964421Z","shell.execute_reply.started":"2022-12-03T17:58:09.840003Z","shell.execute_reply":"2022-12-03T17:58:10.963073Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:58:10.967017Z","iopub.execute_input":"2022-12-03T17:58:10.967429Z","iopub.status.idle":"2022-12-03T17:59:12.565764Z","shell.execute_reply.started":"2022-12-03T17:58:10.967382Z","shell.execute_reply":"2022-12-03T17:59:12.564554Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"17:58:18 | building dictionary first...\n17:58:18 | No model with opt yet at: /tmp/model3(.opt)\n17:58:18 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n17:58:18 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:58:18 | Using CUDA\n17:58:18 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:58:18 | num words = 54944\n17:58:23 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:58:37 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:58:37 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n17:58:37 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n17:58:37 | Opt:\n17:58:37 |     activation: gelu\n17:58:37 |     adafactor_eps: '(1e-30, 0.001)'\n17:58:37 |     adam_eps: 1e-08\n17:58:37 |     add_p1_after_newln: False\n17:58:37 |     aggregate_micro: False\n17:58:37 |     allow_missing_init_opts: False\n17:58:37 |     attention_dropout: 0.1\n17:58:37 |     batchsize: 20\n17:58:37 |     betas: '(0.9, 0.999)'\n17:58:37 |     bpe_add_prefix_space: None\n17:58:37 |     bpe_debug: False\n17:58:37 |     bpe_dropout: None\n17:58:37 |     bpe_merge: None\n17:58:37 |     bpe_vocab: None\n17:58:37 |     candidates: inline\n17:58:37 |     cap_num_predictions: 100\n17:58:37 |     checkpoint_activations: False\n17:58:37 |     class_weights: None\n17:58:37 |     classes: \"['__notok__', '__ok__']\"\n17:58:37 |     classes_from_file: None\n17:58:37 |     data_parallel: True\n17:58:37 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:58:37 |     datatype: train\n17:58:37 |     delimiter: '\\n'\n17:58:37 |     dict_class: parlai.core.dict:DictionaryAgent\n17:58:37 |     dict_endtoken: __start__\n17:58:37 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n17:58:37 |     dict_include_test: False\n17:58:37 |     dict_include_valid: False\n17:58:37 |     dict_initpath: None\n17:58:37 |     dict_language: english\n17:58:37 |     dict_loaded: True\n17:58:37 |     dict_lower: True\n17:58:37 |     dict_max_ngram_size: -1\n17:58:37 |     dict_maxexs: -1\n17:58:37 |     dict_maxtokens: -1\n17:58:37 |     dict_minfreq: 0\n17:58:37 |     dict_nulltoken: __null__\n17:58:37 |     dict_starttoken: __start__\n17:58:37 |     dict_textfields: text,labels\n17:58:37 |     dict_tokenizer: bpe\n17:58:37 |     dict_unktoken: __unk__\n17:58:37 |     display_examples: False\n17:58:37 |     download_path: None\n17:58:37 |     dropout: 0.1\n17:58:37 |     dynamic_batching: None\n17:58:37 |     embedding_projection: random\n17:58:37 |     embedding_size: 768\n17:58:37 |     embedding_type: random\n17:58:37 |     embeddings_scale: False\n17:58:37 |     encode_candidate_vecs: True\n17:58:37 |     encode_candidate_vecs_batchsize: 256\n17:58:37 |     eval_batchsize: None\n17:58:37 |     eval_candidates: inline\n17:58:37 |     eval_dynamic_batching: None\n17:58:37 |     evaltask: None\n17:58:37 |     ffn_size: 3072\n17:58:37 |     final_extra_opt: \n17:58:37 |     fixed_candidate_vecs: reuse\n17:58:37 |     fixed_candidates_path: None\n17:58:37 |     force_fp16_tokens: False\n17:58:37 |     fp16: True\n17:58:37 |     fp16_impl: safe\n17:58:37 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt\n17:58:37 |     fromfile_datatype_extension: False\n17:58:37 |     gpu: -1\n17:58:37 |     gradient_clip: 0.1\n17:58:37 |     hide_labels: False\n17:58:37 |     history_add_global_end_token: None\n17:58:37 |     history_reversed: False\n17:58:37 |     history_size: 20\n17:58:37 |     ignore_bad_candidates: False\n17:58:37 |     ignore_labels: None\n17:58:37 |     image_cropsize: 224\n17:58:37 |     image_mode: raw\n17:58:37 |     image_size: 256\n17:58:37 |     inference: max\n17:58:37 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:58:37 |     init_opt: None\n17:58:37 |     interactive_candidates: fixed\n17:58:37 |     interactive_mode: False\n17:58:37 |     invsqrt_lr_decay_gamma: -1\n17:58:37 |     is_debug: False\n17:58:37 |     label_truncate: 72\n17:58:37 |     learn_embeddings: True\n17:58:37 |     learn_positional_embeddings: True\n17:58:37 |     learningrate: 5e-05\n17:58:37 |     load_from_checkpoint: False\n17:58:37 |     load_from_pretrained_ranker: True\n17:58:37 |     log_every_n_secs: 10.0\n17:58:37 |     log_every_n_steps: 50\n17:58:37 |     log_keep_fields: all\n17:58:37 |     loglevel: info\n17:58:37 |     lr_scheduler: reduceonplateau\n17:58:37 |     lr_scheduler_decay: 0.5\n17:58:37 |     lr_scheduler_patience: 3\n17:58:37 |     max_train_steps: -1\n17:58:37 |     max_train_time: 7200.0\n17:58:37 |     memory_attention: sqrt\n17:58:37 |     metrics: default\n17:58:37 |     model: transformer/classifier\n17:58:37 |     model_file: /tmp/model3\n17:58:37 |     model_parallel: False\n17:58:37 |     momentum: 0\n17:58:37 |     multitask_weights: [1]\n17:58:37 |     mutators: None\n17:58:37 |     n_decoder_layers: -1\n17:58:37 |     n_encoder_layers: -1\n17:58:37 |     n_heads: 12\n17:58:37 |     n_layers: 12\n17:58:37 |     n_positions: 1024\n17:58:37 |     n_segments: 2\n17:58:37 |     nesterov: True\n17:58:37 |     no_cuda: False\n17:58:37 |     normalize_sent_emb: False\n17:58:37 |     num_epochs: -1\n17:58:37 |     num_workers: 0\n17:58:37 |     nus: (0.7,)\n17:58:37 |     optimizer: adamax\n17:58:37 |     output_scaling: 0.06\n17:58:37 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n17:58:37 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:58:37 |     person_tokens: False\n17:58:37 |     print_scores: False\n17:58:37 |     rank_candidates: False\n17:58:37 |     rank_top_k: -1\n17:58:37 |     reduction_type: mean\n17:58:37 |     ref_class: None\n17:58:37 |     relu_dropout: 0.0\n17:58:37 |     repeat_blocking_heuristic: True\n17:58:37 |     return_cand_scores: False\n17:58:37 |     save_after_valid: True\n17:58:37 |     save_every_n_secs: -1\n17:58:37 |     save_format: conversations\n17:58:37 |     share_encoders: False\n17:58:37 |     share_word_embeddings: False\n17:58:37 |     short_final_eval: False\n17:58:37 |     special_tok_lst: None\n17:58:37 |     split_lines: False\n17:58:37 |     starttime: Dec03_17-58\n17:58:37 |     task: fromfile:parlaiformat\n17:58:37 |     tensorboard_log: False\n17:58:37 |     tensorboard_logdir: None\n17:58:37 |     text_truncate: 360\n17:58:37 |     threshold: 0.5\n17:58:37 |     topk: 5\n17:58:37 |     train_predict: False\n17:58:37 |     truncate: 1024\n17:58:37 |     update_classifier_head_only: False\n17:58:37 |     update_freq: 1\n17:58:37 |     use_memories: False\n17:58:37 |     use_reply: none\n17:58:37 |     validation_cutoff: 1.0\n17:58:37 |     validation_every_n_epochs: -1\n17:58:37 |     validation_every_n_secs: 20.0\n17:58:37 |     validation_every_n_steps: -1\n17:58:37 |     validation_max_exs: -1\n17:58:37 |     validation_metric: accuracy\n17:58:37 |     validation_metric_mode: max\n17:58:37 |     validation_patience: 30\n17:58:37 |     validation_share_agent: False\n17:58:37 |     variant: xlm\n17:58:37 |     verbose: False\n17:58:37 |     wandb_entity: None\n17:58:37 |     wandb_log: False\n17:58:37 |     wandb_name: None\n17:58:37 |     wandb_project: None\n17:58:37 |     warmup_rate: 0.0001\n17:58:37 |     warmup_updates: 1000\n17:58:37 |     weight_decay: None\n17:58:37 |     world_logs: \n17:58:37 |     wrap_memory_encoder: False\n17:58:38 | creating task(s): fromfile:parlaiformat\n17:58:38 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt\n17:58:38 | training...\n17:58:48 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .3075 3.075e-10               .2452                 .2778   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2195            .3603              .3277   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .4000 11.91     1 278.3 549.4       0          0 39.48  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .3075             32768  2.563    .1206 6.025 .7112 1.005e-06 120.5 237.9   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 398.8 787.3 1.978        .3013\n\n17:58:58 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6882 6.882e-10               .7019                 .6838   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7209            .6731              .6932   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6542 11.75     1 275.1  1062       0          0 77.19  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6882             32768  2.499    .1207 6.018 .6609 2.905e-06 120.4 464.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 395.4 1526 3.868        .6878\n\n17:58:58 | creating task(s): fromfile:parlaiformat\n17:58:58 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:58:58 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt\n17:58:58 | running eval: valid\n17:58:58 | eval completed in 0.20s\n17:58:58 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1840       0          0 134.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5932 2.905e-06    72 805.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2645            1\n\u001b[0m\n17:58:58 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n17:58:58 | saving best valid model: /tmp/model3\n17:58:58 | Saving dictionary to /tmp/model3.dict\n17:59:02 | task solved! stopping.\n17:59:02 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n17:59:02 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n17:59:02 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n17:59:02 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n17:59:02 | Using CUDA\n17:59:02 | loading dictionary from /tmp/model3.dict\n17:59:02 | num words = 54944\n17:59:07 | Loading existing model parameters from /tmp/model3\n17:59:09 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:59:10 | creating task(s): fromfile:parlaiformat\n17:59:10 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:59:10 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt\n17:59:10 | running eval: valid\n17:59:10 | eval completed in 0.20s\n17:59:10 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1820       0          0 132.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5932 2.905e-06    72 796.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2616            1\n\u001b[0m\n17:59:10 | creating task(s): fromfile:parlaiformat\n17:59:10 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:59:10 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt\n17:59:10 | running eval: test\n17:59:10 | eval completed in 0.20s\n17:59:10 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1835       0          0 133.8   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5932 2.905e-06    72   803       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2638            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:59:12.567498Z","iopub.execute_input":"2022-12-03T17:59:12.567906Z","iopub.status.idle":"2022-12-03T17:59:41.182362Z","shell.execute_reply.started":"2022-12-03T17:59:12.567857Z","shell.execute_reply":"2022-12-03T17:59:41.181142Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stdout","text":"17:59:20 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt)\u001b[0m\n17:59:20 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:59:20 | Using CUDA\n17:59:20 | loading dictionary from /tmp/model3.dict\n17:59:20 | num words = 54944\n17:59:24 | Loading existing model parameters from /tmp/model3\n17:59:32 | Total parameters: 128,042,498 (128,042,498 trainable)\n17:59:33 | Opt:\n17:59:33 |     activation: gelu\n17:59:33 |     adafactor_eps: '[1e-30, 0.001]'\n17:59:33 |     adam_eps: 1e-08\n17:59:33 |     add_p1_after_newln: False\n17:59:33 |     aggregate_micro: False\n17:59:33 |     allow_missing_init_opts: False\n17:59:33 |     area_under_curve_class: None\n17:59:33 |     area_under_curve_digits: -1\n17:59:33 |     attention_dropout: 0.1\n17:59:33 |     batchsize: 40\n17:59:33 |     betas: '[0.9, 0.999]'\n17:59:33 |     bpe_add_prefix_space: None\n17:59:33 |     bpe_debug: False\n17:59:33 |     bpe_dropout: None\n17:59:33 |     bpe_merge: None\n17:59:33 |     bpe_vocab: None\n17:59:33 |     candidates: inline\n17:59:33 |     cap_num_predictions: 100\n17:59:33 |     checkpoint_activations: False\n17:59:33 |     class_weights: None\n17:59:33 |     classes: \"['__notok__', '__ok__']\"\n17:59:33 |     classes_from_file: None\n17:59:33 |     data_parallel: True\n17:59:33 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n17:59:33 |     datatype: train\n17:59:33 |     delimiter: '\\n'\n17:59:33 |     dict_class: parlai.core.dict:DictionaryAgent\n17:59:33 |     dict_endtoken: __start__\n17:59:33 |     dict_file: /tmp/model3.dict\n17:59:33 |     dict_include_test: False\n17:59:33 |     dict_include_valid: False\n17:59:33 |     dict_initpath: None\n17:59:33 |     dict_language: english\n17:59:33 |     dict_loaded: True\n17:59:33 |     dict_lower: True\n17:59:33 |     dict_max_ngram_size: -1\n17:59:33 |     dict_maxexs: -1\n17:59:33 |     dict_maxtokens: -1\n17:59:33 |     dict_minfreq: 0\n17:59:33 |     dict_nulltoken: __null__\n17:59:33 |     dict_starttoken: __start__\n17:59:33 |     dict_textfields: text,labels\n17:59:33 |     dict_tokenizer: bpe\n17:59:33 |     dict_unktoken: __unk__\n17:59:33 |     display_examples: False\n17:59:33 |     download_path: None\n17:59:33 |     dropout: 0.1\n17:59:33 |     dynamic_batching: None\n17:59:33 |     embedding_projection: random\n17:59:33 |     embedding_size: 768\n17:59:33 |     embedding_type: random\n17:59:33 |     embeddings_scale: False\n17:59:33 |     encode_candidate_vecs: True\n17:59:33 |     encode_candidate_vecs_batchsize: 256\n17:59:33 |     eval_batchsize: None\n17:59:33 |     eval_candidates: inline\n17:59:33 |     eval_dynamic_batching: None\n17:59:33 |     evaltask: None\n17:59:33 |     ffn_size: 3072\n17:59:33 |     final_extra_opt: \n17:59:33 |     fixed_candidate_vecs: reuse\n17:59:33 |     fixed_candidates_path: None\n17:59:33 |     force_fp16_tokens: True\n17:59:33 |     fp16: True\n17:59:33 |     fp16_impl: safe\n17:59:33 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-a.txt\n17:59:33 |     fromfile_datatype_extension: False\n17:59:33 |     gpu: -1\n17:59:33 |     gradient_clip: 0.1\n17:59:33 |     hide_labels: False\n17:59:33 |     history_add_global_end_token: None\n17:59:33 |     history_reversed: False\n17:59:33 |     history_size: 20\n17:59:33 |     ignore_bad_candidates: False\n17:59:33 |     ignore_labels: None\n17:59:33 |     image_cropsize: 224\n17:59:33 |     image_mode: raw\n17:59:33 |     image_size: 256\n17:59:33 |     inference: max\n17:59:33 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n17:59:33 |     init_opt: None\n17:59:33 |     interactive_candidates: fixed\n17:59:33 |     interactive_mode: False\n17:59:33 |     invsqrt_lr_decay_gamma: -1\n17:59:33 |     is_debug: False\n17:59:33 |     label_truncate: 72\n17:59:33 |     learn_embeddings: True\n17:59:33 |     learn_positional_embeddings: True\n17:59:33 |     learningrate: 5e-05\n17:59:33 |     load_from_pretrained_ranker: True\n17:59:33 |     log_every_n_secs: 10.0\n17:59:33 |     log_every_n_steps: 50\n17:59:33 |     log_keep_fields: all\n17:59:33 |     loglevel: info\n17:59:33 |     lr_scheduler: reduceonplateau\n17:59:33 |     lr_scheduler_decay: 0.5\n17:59:33 |     lr_scheduler_patience: 3\n17:59:33 |     max_train_steps: -1\n17:59:33 |     max_train_time: 7200.0\n17:59:33 |     memory_attention: sqrt\n17:59:33 |     metrics: default\n17:59:33 |     model: transformer/classifier\n17:59:33 |     model_file: /tmp/model3\n17:59:33 |     model_parallel: False\n17:59:33 |     momentum: 0\n17:59:33 |     multitask_weights: [1]\n17:59:33 |     mutators: None\n17:59:33 |     n_decoder_layers: -1\n17:59:33 |     n_encoder_layers: -1\n17:59:33 |     n_heads: 12\n17:59:33 |     n_layers: 12\n17:59:33 |     n_positions: 1024\n17:59:33 |     n_segments: 2\n17:59:33 |     nesterov: True\n17:59:33 |     no_cuda: False\n17:59:33 |     normalize_sent_emb: False\n17:59:33 |     num_epochs: -1\n17:59:33 |     num_examples: -1\n17:59:33 |     num_workers: 0\n17:59:33 |     nus: [0.7]\n17:59:33 |     optimizer: adamax\n17:59:33 |     output_scaling: 0.06\n17:59:33 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n17:59:33 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n17:59:33 |     person_tokens: False\n17:59:33 |     print_scores: False\n17:59:33 |     rank_candidates: False\n17:59:33 |     rank_top_k: -1\n17:59:33 |     reduction_type: mean\n17:59:33 |     ref_class: None\n17:59:33 |     relu_dropout: 0.0\n17:59:33 |     repeat_blocking_heuristic: True\n17:59:33 |     report_filename: \n17:59:33 |     return_cand_scores: False\n17:59:33 |     save_after_valid: True\n17:59:33 |     save_every_n_secs: -1\n17:59:33 |     save_format: conversations\n17:59:33 |     share_encoders: False\n17:59:33 |     share_word_embeddings: False\n17:59:33 |     short_final_eval: False\n17:59:33 |     special_tok_lst: None\n17:59:33 |     split_lines: False\n17:59:33 |     starttime: Dec03_17-58\n17:59:33 |     task: fromfile:parlaiformat\n17:59:33 |     tensorboard_log: False\n17:59:33 |     tensorboard_logdir: None\n17:59:33 |     text_truncate: 360\n17:59:33 |     threshold: 0.5\n17:59:33 |     topk: 5\n17:59:33 |     train_predict: False\n17:59:33 |     truncate: 1024\n17:59:33 |     update_classifier_head_only: False\n17:59:33 |     update_freq: 1\n17:59:33 |     use_memories: False\n17:59:33 |     use_reply: none\n17:59:33 |     validation_cutoff: 1.0\n17:59:33 |     validation_every_n_epochs: -1\n17:59:33 |     validation_every_n_secs: 20.0\n17:59:33 |     validation_every_n_steps: -1\n17:59:33 |     validation_max_exs: -1\n17:59:33 |     validation_metric: accuracy\n17:59:33 |     validation_metric_mode: max\n17:59:33 |     validation_patience: 30\n17:59:33 |     validation_share_agent: False\n17:59:33 |     variant: xlm\n17:59:33 |     verbose: False\n17:59:33 |     wandb_entity: None\n17:59:33 |     wandb_log: False\n17:59:33 |     wandb_name: None\n17:59:33 |     wandb_project: None\n17:59:33 |     warmup_rate: 0.0001\n17:59:33 |     warmup_updates: 1000\n17:59:33 |     weight_decay: None\n17:59:33 |     world_logs: \n17:59:33 |     wrap_memory_encoder: False\n17:59:34 | Evaluating task fromfile:parlaiformat using datatype valid.\n17:59:34 | creating task(s): fromfile:parlaiformat\n17:59:34 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n17:59:34 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-a.txt\n17:59:39 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3450 3.45e-10               .4473                 .3869   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5300            .1963              .2540   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1600 11.47   539 516.8       0          0 38.35  200 .3450   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7267 2.905e-06   240 230.1       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 746.9        .3218\u001b[0m\n17:59:39 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3450 3.45e-10               .4473                 .3869   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5300            .1963              .2540   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1600 11.47   539 516.8       0          0 38.35  200 .3450   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7267 2.905e-06   240 230.1       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 746.9        .3218\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T17:59:41.184562Z","iopub.execute_input":"2022-12-03T17:59:41.184951Z","iopub.status.idle":"2022-12-03T18:00:08.282383Z","shell.execute_reply.started":"2022-12-03T17:59:41.184912Z","shell.execute_reply":"2022-12-03T18:00:08.280992Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stdout","text":"17:59:48 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_valid.txt)\u001b[0m\n17:59:48 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n17:59:48 | Using CUDA\n17:59:48 | loading dictionary from /tmp/model3.dict\n17:59:48 | num words = 54944\n17:59:53 | Loading existing model parameters from /tmp/model3\n17:59:59 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:00:00 | Opt:\n18:00:00 |     activation: gelu\n18:00:00 |     adafactor_eps: '[1e-30, 0.001]'\n18:00:00 |     adam_eps: 1e-08\n18:00:00 |     add_p1_after_newln: False\n18:00:00 |     aggregate_micro: False\n18:00:00 |     allow_missing_init_opts: False\n18:00:00 |     area_under_curve_class: None\n18:00:00 |     area_under_curve_digits: -1\n18:00:00 |     attention_dropout: 0.1\n18:00:00 |     batchsize: 40\n18:00:00 |     betas: '[0.9, 0.999]'\n18:00:00 |     bpe_add_prefix_space: None\n18:00:00 |     bpe_debug: False\n18:00:00 |     bpe_dropout: None\n18:00:00 |     bpe_merge: None\n18:00:00 |     bpe_vocab: None\n18:00:00 |     candidates: inline\n18:00:00 |     cap_num_predictions: 100\n18:00:00 |     checkpoint_activations: False\n18:00:00 |     class_weights: None\n18:00:00 |     classes: \"['__notok__', '__ok__']\"\n18:00:00 |     classes_from_file: None\n18:00:00 |     data_parallel: True\n18:00:00 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:00:00 |     datatype: train\n18:00:00 |     delimiter: '\\n'\n18:00:00 |     dict_class: parlai.core.dict:DictionaryAgent\n18:00:00 |     dict_endtoken: __start__\n18:00:00 |     dict_file: /tmp/model3.dict\n18:00:00 |     dict_include_test: False\n18:00:00 |     dict_include_valid: False\n18:00:00 |     dict_initpath: None\n18:00:00 |     dict_language: english\n18:00:00 |     dict_loaded: True\n18:00:00 |     dict_lower: True\n18:00:00 |     dict_max_ngram_size: -1\n18:00:00 |     dict_maxexs: -1\n18:00:00 |     dict_maxtokens: -1\n18:00:00 |     dict_minfreq: 0\n18:00:00 |     dict_nulltoken: __null__\n18:00:00 |     dict_starttoken: __start__\n18:00:00 |     dict_textfields: text,labels\n18:00:00 |     dict_tokenizer: bpe\n18:00:00 |     dict_unktoken: __unk__\n18:00:00 |     display_examples: False\n18:00:00 |     download_path: None\n18:00:00 |     dropout: 0.1\n18:00:00 |     dynamic_batching: None\n18:00:00 |     embedding_projection: random\n18:00:00 |     embedding_size: 768\n18:00:00 |     embedding_type: random\n18:00:00 |     embeddings_scale: False\n18:00:00 |     encode_candidate_vecs: True\n18:00:00 |     encode_candidate_vecs_batchsize: 256\n18:00:00 |     eval_batchsize: None\n18:00:00 |     eval_candidates: inline\n18:00:00 |     eval_dynamic_batching: None\n18:00:00 |     evaltask: None\n18:00:00 |     ffn_size: 3072\n18:00:00 |     final_extra_opt: \n18:00:00 |     fixed_candidate_vecs: reuse\n18:00:00 |     fixed_candidates_path: None\n18:00:00 |     force_fp16_tokens: True\n18:00:00 |     fp16: True\n18:00:00 |     fp16_impl: safe\n18:00:00 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-b.txt\n18:00:00 |     fromfile_datatype_extension: False\n18:00:00 |     gpu: -1\n18:00:00 |     gradient_clip: 0.1\n18:00:00 |     hide_labels: False\n18:00:00 |     history_add_global_end_token: None\n18:00:00 |     history_reversed: False\n18:00:00 |     history_size: 20\n18:00:00 |     ignore_bad_candidates: False\n18:00:00 |     ignore_labels: None\n18:00:00 |     image_cropsize: 224\n18:00:00 |     image_mode: raw\n18:00:00 |     image_size: 256\n18:00:00 |     inference: max\n18:00:00 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:00:00 |     init_opt: None\n18:00:00 |     interactive_candidates: fixed\n18:00:00 |     interactive_mode: False\n18:00:00 |     invsqrt_lr_decay_gamma: -1\n18:00:00 |     is_debug: False\n18:00:00 |     label_truncate: 72\n18:00:00 |     learn_embeddings: True\n18:00:00 |     learn_positional_embeddings: True\n18:00:00 |     learningrate: 5e-05\n18:00:00 |     load_from_pretrained_ranker: True\n18:00:00 |     log_every_n_secs: 10.0\n18:00:00 |     log_every_n_steps: 50\n18:00:00 |     log_keep_fields: all\n18:00:00 |     loglevel: info\n18:00:00 |     lr_scheduler: reduceonplateau\n18:00:00 |     lr_scheduler_decay: 0.5\n18:00:00 |     lr_scheduler_patience: 3\n18:00:00 |     max_train_steps: -1\n18:00:00 |     max_train_time: 7200.0\n18:00:00 |     memory_attention: sqrt\n18:00:00 |     metrics: default\n18:00:00 |     model: transformer/classifier\n18:00:00 |     model_file: /tmp/model3\n18:00:00 |     model_parallel: False\n18:00:00 |     momentum: 0\n18:00:00 |     multitask_weights: [1]\n18:00:00 |     mutators: None\n18:00:00 |     n_decoder_layers: -1\n18:00:00 |     n_encoder_layers: -1\n18:00:00 |     n_heads: 12\n18:00:00 |     n_layers: 12\n18:00:00 |     n_positions: 1024\n18:00:00 |     n_segments: 2\n18:00:00 |     nesterov: True\n18:00:00 |     no_cuda: False\n18:00:00 |     normalize_sent_emb: False\n18:00:00 |     num_epochs: -1\n18:00:00 |     num_examples: -1\n18:00:00 |     num_workers: 0\n18:00:00 |     nus: [0.7]\n18:00:00 |     optimizer: adamax\n18:00:00 |     output_scaling: 0.06\n18:00:00 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n18:00:00 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:00:00 |     person_tokens: False\n18:00:00 |     print_scores: False\n18:00:00 |     rank_candidates: False\n18:00:00 |     rank_top_k: -1\n18:00:00 |     reduction_type: mean\n18:00:00 |     ref_class: None\n18:00:00 |     relu_dropout: 0.0\n18:00:00 |     repeat_blocking_heuristic: True\n18:00:00 |     report_filename: \n18:00:00 |     return_cand_scores: False\n18:00:00 |     save_after_valid: True\n18:00:00 |     save_every_n_secs: -1\n18:00:00 |     save_format: conversations\n18:00:00 |     share_encoders: False\n18:00:00 |     share_word_embeddings: False\n18:00:00 |     short_final_eval: False\n18:00:00 |     special_tok_lst: None\n18:00:00 |     split_lines: False\n18:00:00 |     starttime: Dec03_17-58\n18:00:00 |     task: fromfile:parlaiformat\n18:00:00 |     tensorboard_log: False\n18:00:00 |     tensorboard_logdir: None\n18:00:00 |     text_truncate: 360\n18:00:00 |     threshold: 0.5\n18:00:00 |     topk: 5\n18:00:00 |     train_predict: False\n18:00:00 |     truncate: 1024\n18:00:00 |     update_classifier_head_only: False\n18:00:00 |     update_freq: 1\n18:00:00 |     use_memories: False\n18:00:00 |     use_reply: none\n18:00:00 |     validation_cutoff: 1.0\n18:00:00 |     validation_every_n_epochs: -1\n18:00:00 |     validation_every_n_secs: 20.0\n18:00:00 |     validation_every_n_steps: -1\n18:00:00 |     validation_max_exs: -1\n18:00:00 |     validation_metric: accuracy\n18:00:00 |     validation_metric_mode: max\n18:00:00 |     validation_patience: 30\n18:00:00 |     validation_share_agent: False\n18:00:00 |     variant: xlm\n18:00:00 |     verbose: False\n18:00:00 |     wandb_entity: None\n18:00:00 |     wandb_log: False\n18:00:00 |     wandb_name: None\n18:00:00 |     wandb_project: None\n18:00:00 |     warmup_rate: 0.0001\n18:00:00 |     warmup_updates: 1000\n18:00:00 |     weight_decay: None\n18:00:00 |     world_logs: \n18:00:00 |     wrap_memory_encoder: False\n18:00:01 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:00:01 | creating task(s): fromfile:parlaiformat\n18:00:01 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:00:01 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run3/data_train-b.txt\n18:00:06 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6550 6.55e-10               .7089                 .6131   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8400            .5767              .7460   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4700 11.47   539 496.5       0          0 36.84  200 .6550   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6665 2.905e-06   240 221.1       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 717.5        .6428\u001b[0m\n18:00:06 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6550 6.55e-10               .7089                 .6131   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8400            .5767              .7460   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .4700 11.47   539 496.5       0          0 36.84  200 .6550   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6665 2.905e-06   240 221.1       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     58  779 717.5        .6428\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:00:08.284756Z","iopub.execute_input":"2022-12-03T18:00:08.285193Z","iopub.status.idle":"2022-12-03T18:00:09.378332Z","shell.execute_reply.started":"2022-12-03T18:00:08.285148Z","shell.execute_reply":"2022-12-03T18:00:09.377018Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:00:09.380345Z","iopub.execute_input":"2022-12-03T18:00:09.380775Z","iopub.status.idle":"2022-12-03T18:01:28.450600Z","shell.execute_reply.started":"2022-12-03T18:00:09.380735Z","shell.execute_reply":"2022-12-03T18:01:28.449422Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"18:00:16 | building dictionary first...\n18:00:16 | No model with opt yet at: /tmp/model4(.opt)\n18:00:16 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:00:16 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:00:16 | Using CUDA\n18:00:16 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:00:16 | num words = 54944\n18:00:21 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:00:28 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:00:28 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:00:28 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:00:28 | Opt:\n18:00:28 |     activation: gelu\n18:00:28 |     adafactor_eps: '(1e-30, 0.001)'\n18:00:28 |     adam_eps: 1e-08\n18:00:28 |     add_p1_after_newln: False\n18:00:28 |     aggregate_micro: False\n18:00:28 |     allow_missing_init_opts: False\n18:00:28 |     attention_dropout: 0.1\n18:00:28 |     batchsize: 20\n18:00:28 |     betas: '(0.9, 0.999)'\n18:00:28 |     bpe_add_prefix_space: None\n18:00:28 |     bpe_debug: False\n18:00:28 |     bpe_dropout: None\n18:00:28 |     bpe_merge: None\n18:00:28 |     bpe_vocab: None\n18:00:28 |     candidates: inline\n18:00:28 |     cap_num_predictions: 100\n18:00:28 |     checkpoint_activations: False\n18:00:28 |     class_weights: None\n18:00:28 |     classes: \"['__notok__', '__ok__']\"\n18:00:28 |     classes_from_file: None\n18:00:28 |     data_parallel: True\n18:00:28 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:00:28 |     datatype: train\n18:00:28 |     delimiter: '\\n'\n18:00:28 |     dict_class: parlai.core.dict:DictionaryAgent\n18:00:28 |     dict_endtoken: __start__\n18:00:28 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:00:28 |     dict_include_test: False\n18:00:28 |     dict_include_valid: False\n18:00:28 |     dict_initpath: None\n18:00:28 |     dict_language: english\n18:00:28 |     dict_loaded: True\n18:00:28 |     dict_lower: True\n18:00:28 |     dict_max_ngram_size: -1\n18:00:28 |     dict_maxexs: -1\n18:00:28 |     dict_maxtokens: -1\n18:00:28 |     dict_minfreq: 0\n18:00:28 |     dict_nulltoken: __null__\n18:00:28 |     dict_starttoken: __start__\n18:00:28 |     dict_textfields: text,labels\n18:00:28 |     dict_tokenizer: bpe\n18:00:28 |     dict_unktoken: __unk__\n18:00:28 |     display_examples: False\n18:00:28 |     download_path: None\n18:00:28 |     dropout: 0.1\n18:00:28 |     dynamic_batching: None\n18:00:28 |     embedding_projection: random\n18:00:28 |     embedding_size: 768\n18:00:28 |     embedding_type: random\n18:00:28 |     embeddings_scale: False\n18:00:28 |     encode_candidate_vecs: True\n18:00:28 |     encode_candidate_vecs_batchsize: 256\n18:00:28 |     eval_batchsize: None\n18:00:28 |     eval_candidates: inline\n18:00:28 |     eval_dynamic_batching: None\n18:00:28 |     evaltask: None\n18:00:28 |     ffn_size: 3072\n18:00:28 |     final_extra_opt: \n18:00:28 |     fixed_candidate_vecs: reuse\n18:00:28 |     fixed_candidates_path: None\n18:00:28 |     force_fp16_tokens: False\n18:00:28 |     fp16: True\n18:00:28 |     fp16_impl: safe\n18:00:28 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt\n18:00:28 |     fromfile_datatype_extension: False\n18:00:28 |     gpu: -1\n18:00:28 |     gradient_clip: 0.1\n18:00:28 |     hide_labels: False\n18:00:28 |     history_add_global_end_token: None\n18:00:28 |     history_reversed: False\n18:00:28 |     history_size: 20\n18:00:28 |     ignore_bad_candidates: False\n18:00:28 |     ignore_labels: None\n18:00:28 |     image_cropsize: 224\n18:00:28 |     image_mode: raw\n18:00:28 |     image_size: 256\n18:00:28 |     inference: max\n18:00:28 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:00:28 |     init_opt: None\n18:00:28 |     interactive_candidates: fixed\n18:00:28 |     interactive_mode: False\n18:00:28 |     invsqrt_lr_decay_gamma: -1\n18:00:28 |     is_debug: False\n18:00:28 |     label_truncate: 72\n18:00:28 |     learn_embeddings: True\n18:00:28 |     learn_positional_embeddings: True\n18:00:28 |     learningrate: 5e-05\n18:00:28 |     load_from_checkpoint: False\n18:00:28 |     load_from_pretrained_ranker: True\n18:00:28 |     log_every_n_secs: 10.0\n18:00:28 |     log_every_n_steps: 50\n18:00:28 |     log_keep_fields: all\n18:00:28 |     loglevel: info\n18:00:28 |     lr_scheduler: reduceonplateau\n18:00:28 |     lr_scheduler_decay: 0.5\n18:00:28 |     lr_scheduler_patience: 3\n18:00:28 |     max_train_steps: -1\n18:00:28 |     max_train_time: 7200.0\n18:00:28 |     memory_attention: sqrt\n18:00:28 |     metrics: default\n18:00:28 |     model: transformer/classifier\n18:00:28 |     model_file: /tmp/model4\n18:00:28 |     model_parallel: False\n18:00:28 |     momentum: 0\n18:00:28 |     multitask_weights: [1]\n18:00:28 |     mutators: None\n18:00:28 |     n_decoder_layers: -1\n18:00:28 |     n_encoder_layers: -1\n18:00:28 |     n_heads: 12\n18:00:28 |     n_layers: 12\n18:00:28 |     n_positions: 1024\n18:00:28 |     n_segments: 2\n18:00:28 |     nesterov: True\n18:00:28 |     no_cuda: False\n18:00:28 |     normalize_sent_emb: False\n18:00:28 |     num_epochs: -1\n18:00:28 |     num_workers: 0\n18:00:28 |     nus: (0.7,)\n18:00:28 |     optimizer: adamax\n18:00:28 |     output_scaling: 0.06\n18:00:28 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n18:00:28 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:00:28 |     person_tokens: False\n18:00:28 |     print_scores: False\n18:00:28 |     rank_candidates: False\n18:00:28 |     rank_top_k: -1\n18:00:28 |     reduction_type: mean\n18:00:28 |     ref_class: None\n18:00:28 |     relu_dropout: 0.0\n18:00:28 |     repeat_blocking_heuristic: True\n18:00:28 |     return_cand_scores: False\n18:00:28 |     save_after_valid: True\n18:00:28 |     save_every_n_secs: -1\n18:00:28 |     save_format: conversations\n18:00:28 |     share_encoders: False\n18:00:28 |     share_word_embeddings: False\n18:00:28 |     short_final_eval: False\n18:00:28 |     special_tok_lst: None\n18:00:28 |     split_lines: False\n18:00:28 |     starttime: Dec03_18-00\n18:00:28 |     task: fromfile:parlaiformat\n18:00:28 |     tensorboard_log: False\n18:00:28 |     tensorboard_logdir: None\n18:00:28 |     text_truncate: 360\n18:00:28 |     threshold: 0.5\n18:00:28 |     topk: 5\n18:00:28 |     train_predict: False\n18:00:28 |     truncate: 1024\n18:00:28 |     update_classifier_head_only: False\n18:00:28 |     update_freq: 1\n18:00:28 |     use_memories: False\n18:00:28 |     use_reply: none\n18:00:28 |     validation_cutoff: 1.0\n18:00:28 |     validation_every_n_epochs: -1\n18:00:28 |     validation_every_n_secs: 20.0\n18:00:28 |     validation_every_n_steps: -1\n18:00:28 |     validation_max_exs: -1\n18:00:28 |     validation_metric: accuracy\n18:00:28 |     validation_metric_mode: max\n18:00:28 |     validation_patience: 30\n18:00:28 |     validation_share_agent: False\n18:00:28 |     variant: xlm\n18:00:28 |     verbose: False\n18:00:28 |     wandb_entity: None\n18:00:28 |     wandb_log: False\n18:00:28 |     wandb_name: None\n18:00:28 |     wandb_project: None\n18:00:28 |     warmup_rate: 0.0001\n18:00:28 |     warmup_updates: 1000\n18:00:28 |     weight_decay: None\n18:00:28 |     world_logs: \n18:00:28 |     wrap_memory_encoder: False\n18:00:28 | creating task(s): fromfile:parlaiformat\n18:00:28 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt\n18:00:28 | training...\n18:00:39 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .3976 3.976e-10               .2873                 .3469   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2452            .4784              .4249   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5472 11.93     1 278.6 578.4       0          0 41.52  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .3976             32768  2.735    .1189  5.99 .7092 1.055e-06 119.8 248.7   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 398.4 827.1 2.081        .3837\n\n18:00:49 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6526 6.526e-10               .6313                 .6933   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5795            .6716              .6221   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7297 12.16     1 283.2  1093       0          0 77.16  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6526             32768  2.655    .1189 6.026 .6653 2.955e-06 120.5   465   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   59 403.7 1558 3.867        .6509\n\n18:00:49 | creating task(s): fromfile:parlaiformat\n18:00:49 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:00:49 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt\n18:00:49 | running eval: valid\n18:00:49 | eval completed in 0.19s\n18:00:49 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8333 8.333e-10               .8333                 .8333   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8333            .8333              .8333   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8333 12.04 168.5  1920       0          0 136.7   24 .8333   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .6043 2.955e-06    72 820.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 240.5 2741        .8333\n\u001b[0m\n18:00:49 | \u001b[1;32mnew best accuracy: 0.8333\u001b[0m\n18:00:49 | saving best valid model: /tmp/model4\n18:00:49 | Saving dictionary to /tmp/model4.dict\n18:00:53 | saving model checkpoint: /tmp/model4.checkpoint\n18:00:53 | Saving dictionary to /tmp/model4.checkpoint.dict\n18:01:09 | time:41s total_exs:1900 total_steps:95 epochs:79.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9653 9.653e-10               .9653                 .9640   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9667            .9652              .9666   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9639 12.18     1 283.7  1016       0          0 71.63  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9653             32768   2.75    .1189     6 .5349 4.755e-06   120 429.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps  ups  weighted_f1  \n         0          0                   95 403.7 1446 3.59        .9653\n\n18:01:13 | time:44s total_exs:2160 total_steps:108 epochs:90.00\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1  11.7     1 274.1  1068       0          0 77.97  260   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.686    .1189 6.038 .3919 5.404e-06 120.8 470.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  108 394.8 1539 3.925            1\n\n18:01:13 | running eval: valid\n18:01:13 | eval completed in 0.20s\n18:01:13 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1877       0          0 133.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3059 5.404e-06    72 801.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    108 240.5 2678            1\n\u001b[0m\n18:01:13 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.8333)\u001b[0m\n18:01:13 | saving best valid model: /tmp/model4\n18:01:18 | task solved! stopping.\n18:01:18 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:01:18 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:01:18 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:01:18 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:01:18 | Using CUDA\n18:01:18 | loading dictionary from /tmp/model4.dict\n18:01:18 | num words = 54944\n18:01:23 | Loading existing model parameters from /tmp/model4\n18:01:25 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:01:26 | creating task(s): fromfile:parlaiformat\n18:01:26 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:01:26 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt\n18:01:26 | running eval: valid\n18:01:26 | eval completed in 0.20s\n18:01:26 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1843       0          0 131.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3059 5.404e-06    72 787.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    108 240.5 2630            1\n\u001b[0m\n18:01:26 | creating task(s): fromfile:parlaiformat\n18:01:26 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:01:26 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt\n18:01:26 | running eval: test\n18:01:26 | eval completed in 0.20s\n18:01:26 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1857       0          0 132.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3059 5.404e-06    72 793.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    108 240.5 2651            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:01:28.452694Z","iopub.execute_input":"2022-12-03T18:01:28.453078Z","iopub.status.idle":"2022-12-03T18:01:56.594749Z","shell.execute_reply.started":"2022-12-03T18:01:28.453039Z","shell.execute_reply":"2022-12-03T18:01:56.593555Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"18:01:35 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt)\u001b[0m\n18:01:35 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:01:35 | Using CUDA\n18:01:35 | loading dictionary from /tmp/model4.dict\n18:01:36 | num words = 54944\n18:01:40 | Loading existing model parameters from /tmp/model4\n18:01:47 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:01:48 | Opt:\n18:01:48 |     activation: gelu\n18:01:48 |     adafactor_eps: '[1e-30, 0.001]'\n18:01:48 |     adam_eps: 1e-08\n18:01:48 |     add_p1_after_newln: False\n18:01:48 |     aggregate_micro: False\n18:01:48 |     allow_missing_init_opts: False\n18:01:48 |     area_under_curve_class: None\n18:01:48 |     area_under_curve_digits: -1\n18:01:48 |     attention_dropout: 0.1\n18:01:48 |     batchsize: 40\n18:01:48 |     betas: '[0.9, 0.999]'\n18:01:48 |     bpe_add_prefix_space: None\n18:01:48 |     bpe_debug: False\n18:01:48 |     bpe_dropout: None\n18:01:48 |     bpe_merge: None\n18:01:48 |     bpe_vocab: None\n18:01:48 |     candidates: inline\n18:01:48 |     cap_num_predictions: 100\n18:01:48 |     checkpoint_activations: False\n18:01:48 |     class_weights: None\n18:01:48 |     classes: \"['__notok__', '__ok__']\"\n18:01:48 |     classes_from_file: None\n18:01:48 |     data_parallel: True\n18:01:48 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:01:48 |     datatype: train\n18:01:48 |     delimiter: '\\n'\n18:01:48 |     dict_class: parlai.core.dict:DictionaryAgent\n18:01:48 |     dict_endtoken: __start__\n18:01:48 |     dict_file: /tmp/model4.dict\n18:01:48 |     dict_include_test: False\n18:01:48 |     dict_include_valid: False\n18:01:48 |     dict_initpath: None\n18:01:48 |     dict_language: english\n18:01:48 |     dict_loaded: True\n18:01:48 |     dict_lower: True\n18:01:48 |     dict_max_ngram_size: -1\n18:01:48 |     dict_maxexs: -1\n18:01:48 |     dict_maxtokens: -1\n18:01:48 |     dict_minfreq: 0\n18:01:48 |     dict_nulltoken: __null__\n18:01:48 |     dict_starttoken: __start__\n18:01:48 |     dict_textfields: text,labels\n18:01:48 |     dict_tokenizer: bpe\n18:01:48 |     dict_unktoken: __unk__\n18:01:48 |     display_examples: False\n18:01:48 |     download_path: None\n18:01:48 |     dropout: 0.1\n18:01:48 |     dynamic_batching: None\n18:01:48 |     embedding_projection: random\n18:01:48 |     embedding_size: 768\n18:01:48 |     embedding_type: random\n18:01:48 |     embeddings_scale: False\n18:01:48 |     encode_candidate_vecs: True\n18:01:48 |     encode_candidate_vecs_batchsize: 256\n18:01:48 |     eval_batchsize: None\n18:01:48 |     eval_candidates: inline\n18:01:48 |     eval_dynamic_batching: None\n18:01:48 |     evaltask: None\n18:01:48 |     ffn_size: 3072\n18:01:48 |     final_extra_opt: \n18:01:48 |     fixed_candidate_vecs: reuse\n18:01:48 |     fixed_candidates_path: None\n18:01:48 |     force_fp16_tokens: True\n18:01:48 |     fp16: True\n18:01:48 |     fp16_impl: safe\n18:01:48 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-a.txt\n18:01:48 |     fromfile_datatype_extension: False\n18:01:48 |     gpu: -1\n18:01:48 |     gradient_clip: 0.1\n18:01:48 |     hide_labels: False\n18:01:48 |     history_add_global_end_token: None\n18:01:48 |     history_reversed: False\n18:01:48 |     history_size: 20\n18:01:48 |     ignore_bad_candidates: False\n18:01:48 |     ignore_labels: None\n18:01:48 |     image_cropsize: 224\n18:01:48 |     image_mode: raw\n18:01:48 |     image_size: 256\n18:01:48 |     inference: max\n18:01:48 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:01:48 |     init_opt: None\n18:01:48 |     interactive_candidates: fixed\n18:01:48 |     interactive_mode: False\n18:01:48 |     invsqrt_lr_decay_gamma: -1\n18:01:48 |     is_debug: False\n18:01:48 |     label_truncate: 72\n18:01:48 |     learn_embeddings: True\n18:01:48 |     learn_positional_embeddings: True\n18:01:48 |     learningrate: 5e-05\n18:01:48 |     load_from_pretrained_ranker: True\n18:01:48 |     log_every_n_secs: 10.0\n18:01:48 |     log_every_n_steps: 50\n18:01:48 |     log_keep_fields: all\n18:01:48 |     loglevel: info\n18:01:48 |     lr_scheduler: reduceonplateau\n18:01:48 |     lr_scheduler_decay: 0.5\n18:01:48 |     lr_scheduler_patience: 3\n18:01:48 |     max_train_steps: -1\n18:01:48 |     max_train_time: 7200.0\n18:01:48 |     memory_attention: sqrt\n18:01:48 |     metrics: default\n18:01:48 |     model: transformer/classifier\n18:01:48 |     model_file: /tmp/model4\n18:01:48 |     model_parallel: False\n18:01:48 |     momentum: 0\n18:01:48 |     multitask_weights: [1]\n18:01:48 |     mutators: None\n18:01:48 |     n_decoder_layers: -1\n18:01:48 |     n_encoder_layers: -1\n18:01:48 |     n_heads: 12\n18:01:48 |     n_layers: 12\n18:01:48 |     n_positions: 1024\n18:01:48 |     n_segments: 2\n18:01:48 |     nesterov: True\n18:01:48 |     no_cuda: False\n18:01:48 |     normalize_sent_emb: False\n18:01:48 |     num_epochs: -1\n18:01:48 |     num_examples: -1\n18:01:48 |     num_workers: 0\n18:01:48 |     nus: [0.7]\n18:01:48 |     optimizer: adamax\n18:01:48 |     output_scaling: 0.06\n18:01:48 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n18:01:48 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:01:48 |     person_tokens: False\n18:01:48 |     print_scores: False\n18:01:48 |     rank_candidates: False\n18:01:48 |     rank_top_k: -1\n18:01:48 |     reduction_type: mean\n18:01:48 |     ref_class: None\n18:01:48 |     relu_dropout: 0.0\n18:01:48 |     repeat_blocking_heuristic: True\n18:01:48 |     report_filename: \n18:01:48 |     return_cand_scores: False\n18:01:48 |     save_after_valid: True\n18:01:48 |     save_every_n_secs: -1\n18:01:48 |     save_format: conversations\n18:01:48 |     share_encoders: False\n18:01:48 |     share_word_embeddings: False\n18:01:48 |     short_final_eval: False\n18:01:48 |     special_tok_lst: None\n18:01:48 |     split_lines: False\n18:01:48 |     starttime: Dec03_18-00\n18:01:48 |     task: fromfile:parlaiformat\n18:01:48 |     tensorboard_log: False\n18:01:48 |     tensorboard_logdir: None\n18:01:48 |     text_truncate: 360\n18:01:48 |     threshold: 0.5\n18:01:48 |     topk: 5\n18:01:48 |     train_predict: False\n18:01:48 |     truncate: 1024\n18:01:48 |     update_classifier_head_only: False\n18:01:48 |     update_freq: 1\n18:01:48 |     use_memories: False\n18:01:48 |     use_reply: none\n18:01:48 |     validation_cutoff: 1.0\n18:01:48 |     validation_every_n_epochs: -1\n18:01:48 |     validation_every_n_secs: 20.0\n18:01:48 |     validation_every_n_steps: -1\n18:01:48 |     validation_max_exs: -1\n18:01:48 |     validation_metric: accuracy\n18:01:48 |     validation_metric_mode: max\n18:01:48 |     validation_patience: 30\n18:01:48 |     validation_share_agent: False\n18:01:48 |     variant: xlm\n18:01:48 |     verbose: False\n18:01:48 |     wandb_entity: None\n18:01:48 |     wandb_log: False\n18:01:48 |     wandb_name: None\n18:01:48 |     wandb_project: None\n18:01:48 |     warmup_rate: 0.0001\n18:01:48 |     warmup_updates: 1000\n18:01:48 |     weight_decay: None\n18:01:48 |     world_logs: \n18:01:48 |     wrap_memory_encoder: False\n18:01:48 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:01:48 | creating task(s): fromfile:parlaiformat\n18:01:48 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:01:48 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-a.txt\n18:01:55 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1250 1.25e-10               .1117                 .1146   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1089            .1379              .1346   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1414 11.46 538.2 451.8       0          0 33.58  200 .1250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .9193 5.404e-06 240.4 201.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 778.6 653.6        .1247\u001b[0m\n18:01:55 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1250 1.25e-10               .1117                 .1146   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1089            .1379              .1346   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1414 11.46 538.2 451.8       0          0 33.58  200 .1250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .9193 5.404e-06 240.4 201.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 778.6 653.6        .1247\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:01:56.597973Z","iopub.execute_input":"2022-12-03T18:01:56.598374Z","iopub.status.idle":"2022-12-03T18:02:22.590266Z","shell.execute_reply.started":"2022-12-03T18:01:56.598330Z","shell.execute_reply":"2022-12-03T18:02:22.589077Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"18:02:03 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_valid.txt)\u001b[0m\n18:02:03 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:02:03 | Using CUDA\n18:02:03 | loading dictionary from /tmp/model4.dict\n18:02:03 | num words = 54944\n18:02:08 | Loading existing model parameters from /tmp/model4\n18:02:13 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:02:14 | Opt:\n18:02:14 |     activation: gelu\n18:02:14 |     adafactor_eps: '[1e-30, 0.001]'\n18:02:14 |     adam_eps: 1e-08\n18:02:14 |     add_p1_after_newln: False\n18:02:14 |     aggregate_micro: False\n18:02:14 |     allow_missing_init_opts: False\n18:02:14 |     area_under_curve_class: None\n18:02:14 |     area_under_curve_digits: -1\n18:02:14 |     attention_dropout: 0.1\n18:02:14 |     batchsize: 40\n18:02:14 |     betas: '[0.9, 0.999]'\n18:02:14 |     bpe_add_prefix_space: None\n18:02:14 |     bpe_debug: False\n18:02:14 |     bpe_dropout: None\n18:02:14 |     bpe_merge: None\n18:02:14 |     bpe_vocab: None\n18:02:14 |     candidates: inline\n18:02:14 |     cap_num_predictions: 100\n18:02:14 |     checkpoint_activations: False\n18:02:14 |     class_weights: None\n18:02:14 |     classes: \"['__notok__', '__ok__']\"\n18:02:14 |     classes_from_file: None\n18:02:14 |     data_parallel: True\n18:02:14 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:02:14 |     datatype: train\n18:02:14 |     delimiter: '\\n'\n18:02:14 |     dict_class: parlai.core.dict:DictionaryAgent\n18:02:14 |     dict_endtoken: __start__\n18:02:14 |     dict_file: /tmp/model4.dict\n18:02:14 |     dict_include_test: False\n18:02:14 |     dict_include_valid: False\n18:02:14 |     dict_initpath: None\n18:02:14 |     dict_language: english\n18:02:14 |     dict_loaded: True\n18:02:14 |     dict_lower: True\n18:02:14 |     dict_max_ngram_size: -1\n18:02:14 |     dict_maxexs: -1\n18:02:14 |     dict_maxtokens: -1\n18:02:14 |     dict_minfreq: 0\n18:02:14 |     dict_nulltoken: __null__\n18:02:14 |     dict_starttoken: __start__\n18:02:14 |     dict_textfields: text,labels\n18:02:14 |     dict_tokenizer: bpe\n18:02:14 |     dict_unktoken: __unk__\n18:02:14 |     display_examples: False\n18:02:14 |     download_path: None\n18:02:14 |     dropout: 0.1\n18:02:14 |     dynamic_batching: None\n18:02:14 |     embedding_projection: random\n18:02:14 |     embedding_size: 768\n18:02:14 |     embedding_type: random\n18:02:14 |     embeddings_scale: False\n18:02:14 |     encode_candidate_vecs: True\n18:02:14 |     encode_candidate_vecs_batchsize: 256\n18:02:14 |     eval_batchsize: None\n18:02:14 |     eval_candidates: inline\n18:02:14 |     eval_dynamic_batching: None\n18:02:14 |     evaltask: None\n18:02:14 |     ffn_size: 3072\n18:02:14 |     final_extra_opt: \n18:02:14 |     fixed_candidate_vecs: reuse\n18:02:14 |     fixed_candidates_path: None\n18:02:14 |     force_fp16_tokens: True\n18:02:14 |     fp16: True\n18:02:14 |     fp16_impl: safe\n18:02:14 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-b.txt\n18:02:14 |     fromfile_datatype_extension: False\n18:02:14 |     gpu: -1\n18:02:14 |     gradient_clip: 0.1\n18:02:14 |     hide_labels: False\n18:02:14 |     history_add_global_end_token: None\n18:02:14 |     history_reversed: False\n18:02:14 |     history_size: 20\n18:02:14 |     ignore_bad_candidates: False\n18:02:14 |     ignore_labels: None\n18:02:14 |     image_cropsize: 224\n18:02:14 |     image_mode: raw\n18:02:14 |     image_size: 256\n18:02:14 |     inference: max\n18:02:14 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:02:14 |     init_opt: None\n18:02:14 |     interactive_candidates: fixed\n18:02:14 |     interactive_mode: False\n18:02:14 |     invsqrt_lr_decay_gamma: -1\n18:02:14 |     is_debug: False\n18:02:14 |     label_truncate: 72\n18:02:14 |     learn_embeddings: True\n18:02:14 |     learn_positional_embeddings: True\n18:02:14 |     learningrate: 5e-05\n18:02:14 |     load_from_pretrained_ranker: True\n18:02:14 |     log_every_n_secs: 10.0\n18:02:14 |     log_every_n_steps: 50\n18:02:14 |     log_keep_fields: all\n18:02:14 |     loglevel: info\n18:02:14 |     lr_scheduler: reduceonplateau\n18:02:14 |     lr_scheduler_decay: 0.5\n18:02:14 |     lr_scheduler_patience: 3\n18:02:14 |     max_train_steps: -1\n18:02:14 |     max_train_time: 7200.0\n18:02:14 |     memory_attention: sqrt\n18:02:14 |     metrics: default\n18:02:14 |     model: transformer/classifier\n18:02:14 |     model_file: /tmp/model4\n18:02:14 |     model_parallel: False\n18:02:14 |     momentum: 0\n18:02:14 |     multitask_weights: [1]\n18:02:14 |     mutators: None\n18:02:14 |     n_decoder_layers: -1\n18:02:14 |     n_encoder_layers: -1\n18:02:14 |     n_heads: 12\n18:02:14 |     n_layers: 12\n18:02:14 |     n_positions: 1024\n18:02:14 |     n_segments: 2\n18:02:14 |     nesterov: True\n18:02:14 |     no_cuda: False\n18:02:14 |     normalize_sent_emb: False\n18:02:14 |     num_epochs: -1\n18:02:14 |     num_examples: -1\n18:02:14 |     num_workers: 0\n18:02:14 |     nus: [0.7]\n18:02:14 |     optimizer: adamax\n18:02:14 |     output_scaling: 0.06\n18:02:14 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n18:02:14 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:02:14 |     person_tokens: False\n18:02:14 |     print_scores: False\n18:02:14 |     rank_candidates: False\n18:02:14 |     rank_top_k: -1\n18:02:14 |     reduction_type: mean\n18:02:14 |     ref_class: None\n18:02:14 |     relu_dropout: 0.0\n18:02:14 |     repeat_blocking_heuristic: True\n18:02:14 |     report_filename: \n18:02:14 |     return_cand_scores: False\n18:02:14 |     save_after_valid: True\n18:02:14 |     save_every_n_secs: -1\n18:02:14 |     save_format: conversations\n18:02:14 |     share_encoders: False\n18:02:14 |     share_word_embeddings: False\n18:02:14 |     short_final_eval: False\n18:02:14 |     special_tok_lst: None\n18:02:14 |     split_lines: False\n18:02:14 |     starttime: Dec03_18-00\n18:02:14 |     task: fromfile:parlaiformat\n18:02:14 |     tensorboard_log: False\n18:02:14 |     tensorboard_logdir: None\n18:02:14 |     text_truncate: 360\n18:02:14 |     threshold: 0.5\n18:02:14 |     topk: 5\n18:02:14 |     train_predict: False\n18:02:14 |     truncate: 1024\n18:02:14 |     update_classifier_head_only: False\n18:02:14 |     update_freq: 1\n18:02:14 |     use_memories: False\n18:02:14 |     use_reply: none\n18:02:14 |     validation_cutoff: 1.0\n18:02:14 |     validation_every_n_epochs: -1\n18:02:14 |     validation_every_n_secs: 20.0\n18:02:14 |     validation_every_n_steps: -1\n18:02:14 |     validation_max_exs: -1\n18:02:14 |     validation_metric: accuracy\n18:02:14 |     validation_metric_mode: max\n18:02:14 |     validation_patience: 30\n18:02:14 |     validation_share_agent: False\n18:02:14 |     variant: xlm\n18:02:14 |     verbose: False\n18:02:14 |     wandb_entity: None\n18:02:14 |     wandb_log: False\n18:02:14 |     wandb_name: None\n18:02:14 |     wandb_project: None\n18:02:14 |     warmup_rate: 0.0001\n18:02:14 |     warmup_updates: 1000\n18:02:14 |     weight_decay: None\n18:02:14 |     world_logs: \n18:02:14 |     wrap_memory_encoder: False\n18:02:15 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:02:15 | creating task(s): fromfile:parlaiformat\n18:02:15 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:02:15 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run4/data_train-b.txt\n18:02:21 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8750 8.75e-10               .8718                 .8854   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8586            .8780              .8654   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8911 11.46 538.2 499.2       0          0  37.1  200 .8750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .5310 5.404e-06 239.6 222.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 777.8 721.4        .8750\u001b[0m\n18:02:21 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8750 8.75e-10               .8718                 .8854   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8586            .8780              .8654   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8911 11.46 538.2 499.2       0          0  37.1  200 .8750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .5310 5.404e-06 239.6 222.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    108 777.8 721.4        .8750\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:02:22.592323Z","iopub.execute_input":"2022-12-03T18:02:22.592737Z","iopub.status.idle":"2022-12-03T18:02:23.724636Z","shell.execute_reply.started":"2022-12-03T18:02:22.592697Z","shell.execute_reply":"2022-12-03T18:02:23.723342Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:02:23.726721Z","iopub.execute_input":"2022-12-03T18:02:23.727115Z","iopub.status.idle":"2022-12-03T18:03:41.037841Z","shell.execute_reply.started":"2022-12-03T18:02:23.727073Z","shell.execute_reply":"2022-12-03T18:03:41.036620Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"18:02:31 | building dictionary first...\n18:02:31 | No model with opt yet at: /tmp/model5(.opt)\n18:02:31 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:02:31 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:02:31 | Using CUDA\n18:02:31 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:02:31 | num words = 54944\n18:02:35 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:02:41 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:02:41 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:02:41 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:02:41 | Opt:\n18:02:41 |     activation: gelu\n18:02:41 |     adafactor_eps: '(1e-30, 0.001)'\n18:02:41 |     adam_eps: 1e-08\n18:02:41 |     add_p1_after_newln: False\n18:02:41 |     aggregate_micro: False\n18:02:41 |     allow_missing_init_opts: False\n18:02:41 |     attention_dropout: 0.1\n18:02:41 |     batchsize: 20\n18:02:41 |     betas: '(0.9, 0.999)'\n18:02:41 |     bpe_add_prefix_space: None\n18:02:41 |     bpe_debug: False\n18:02:41 |     bpe_dropout: None\n18:02:41 |     bpe_merge: None\n18:02:41 |     bpe_vocab: None\n18:02:41 |     candidates: inline\n18:02:41 |     cap_num_predictions: 100\n18:02:41 |     checkpoint_activations: False\n18:02:41 |     class_weights: None\n18:02:41 |     classes: \"['__notok__', '__ok__']\"\n18:02:41 |     classes_from_file: None\n18:02:41 |     data_parallel: True\n18:02:41 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:02:41 |     datatype: train\n18:02:41 |     delimiter: '\\n'\n18:02:41 |     dict_class: parlai.core.dict:DictionaryAgent\n18:02:41 |     dict_endtoken: __start__\n18:02:41 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:02:41 |     dict_include_test: False\n18:02:41 |     dict_include_valid: False\n18:02:41 |     dict_initpath: None\n18:02:41 |     dict_language: english\n18:02:41 |     dict_loaded: True\n18:02:41 |     dict_lower: True\n18:02:41 |     dict_max_ngram_size: -1\n18:02:41 |     dict_maxexs: -1\n18:02:41 |     dict_maxtokens: -1\n18:02:41 |     dict_minfreq: 0\n18:02:41 |     dict_nulltoken: __null__\n18:02:41 |     dict_starttoken: __start__\n18:02:41 |     dict_textfields: text,labels\n18:02:41 |     dict_tokenizer: bpe\n18:02:41 |     dict_unktoken: __unk__\n18:02:41 |     display_examples: False\n18:02:41 |     download_path: None\n18:02:41 |     dropout: 0.1\n18:02:41 |     dynamic_batching: None\n18:02:41 |     embedding_projection: random\n18:02:41 |     embedding_size: 768\n18:02:41 |     embedding_type: random\n18:02:41 |     embeddings_scale: False\n18:02:41 |     encode_candidate_vecs: True\n18:02:41 |     encode_candidate_vecs_batchsize: 256\n18:02:41 |     eval_batchsize: None\n18:02:41 |     eval_candidates: inline\n18:02:41 |     eval_dynamic_batching: None\n18:02:41 |     evaltask: None\n18:02:41 |     ffn_size: 3072\n18:02:41 |     final_extra_opt: \n18:02:41 |     fixed_candidate_vecs: reuse\n18:02:41 |     fixed_candidates_path: None\n18:02:41 |     force_fp16_tokens: False\n18:02:41 |     fp16: True\n18:02:41 |     fp16_impl: safe\n18:02:41 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt\n18:02:41 |     fromfile_datatype_extension: False\n18:02:41 |     gpu: -1\n18:02:41 |     gradient_clip: 0.1\n18:02:41 |     hide_labels: False\n18:02:41 |     history_add_global_end_token: None\n18:02:41 |     history_reversed: False\n18:02:41 |     history_size: 20\n18:02:41 |     ignore_bad_candidates: False\n18:02:41 |     ignore_labels: None\n18:02:41 |     image_cropsize: 224\n18:02:41 |     image_mode: raw\n18:02:41 |     image_size: 256\n18:02:41 |     inference: max\n18:02:41 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:02:41 |     init_opt: None\n18:02:41 |     interactive_candidates: fixed\n18:02:41 |     interactive_mode: False\n18:02:41 |     invsqrt_lr_decay_gamma: -1\n18:02:41 |     is_debug: False\n18:02:41 |     label_truncate: 72\n18:02:41 |     learn_embeddings: True\n18:02:41 |     learn_positional_embeddings: True\n18:02:41 |     learningrate: 5e-05\n18:02:41 |     load_from_checkpoint: False\n18:02:41 |     load_from_pretrained_ranker: True\n18:02:41 |     log_every_n_secs: 10.0\n18:02:41 |     log_every_n_steps: 50\n18:02:41 |     log_keep_fields: all\n18:02:41 |     loglevel: info\n18:02:41 |     lr_scheduler: reduceonplateau\n18:02:41 |     lr_scheduler_decay: 0.5\n18:02:41 |     lr_scheduler_patience: 3\n18:02:41 |     max_train_steps: -1\n18:02:41 |     max_train_time: 7200.0\n18:02:41 |     memory_attention: sqrt\n18:02:41 |     metrics: default\n18:02:41 |     model: transformer/classifier\n18:02:41 |     model_file: /tmp/model5\n18:02:41 |     model_parallel: False\n18:02:41 |     momentum: 0\n18:02:41 |     multitask_weights: [1]\n18:02:41 |     mutators: None\n18:02:41 |     n_decoder_layers: -1\n18:02:41 |     n_encoder_layers: -1\n18:02:41 |     n_heads: 12\n18:02:41 |     n_layers: 12\n18:02:41 |     n_positions: 1024\n18:02:41 |     n_segments: 2\n18:02:41 |     nesterov: True\n18:02:41 |     no_cuda: False\n18:02:41 |     normalize_sent_emb: False\n18:02:41 |     num_epochs: -1\n18:02:41 |     num_workers: 0\n18:02:41 |     nus: (0.7,)\n18:02:41 |     optimizer: adamax\n18:02:41 |     output_scaling: 0.06\n18:02:41 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n18:02:41 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:02:41 |     person_tokens: False\n18:02:41 |     print_scores: False\n18:02:41 |     rank_candidates: False\n18:02:41 |     rank_top_k: -1\n18:02:41 |     reduction_type: mean\n18:02:41 |     ref_class: None\n18:02:41 |     relu_dropout: 0.0\n18:02:41 |     repeat_blocking_heuristic: True\n18:02:41 |     return_cand_scores: False\n18:02:41 |     save_after_valid: True\n18:02:41 |     save_every_n_secs: -1\n18:02:41 |     save_format: conversations\n18:02:41 |     share_encoders: False\n18:02:41 |     share_word_embeddings: False\n18:02:41 |     short_final_eval: False\n18:02:41 |     special_tok_lst: None\n18:02:41 |     split_lines: False\n18:02:41 |     starttime: Dec03_18-02\n18:02:41 |     task: fromfile:parlaiformat\n18:02:41 |     tensorboard_log: False\n18:02:41 |     tensorboard_logdir: None\n18:02:41 |     text_truncate: 360\n18:02:41 |     threshold: 0.5\n18:02:41 |     topk: 5\n18:02:41 |     train_predict: False\n18:02:41 |     truncate: 1024\n18:02:41 |     update_classifier_head_only: False\n18:02:41 |     update_freq: 1\n18:02:41 |     use_memories: False\n18:02:41 |     use_reply: none\n18:02:41 |     validation_cutoff: 1.0\n18:02:41 |     validation_every_n_epochs: -1\n18:02:41 |     validation_every_n_secs: 20.0\n18:02:41 |     validation_every_n_steps: -1\n18:02:41 |     validation_max_exs: -1\n18:02:41 |     validation_metric: accuracy\n18:02:41 |     validation_metric_mode: max\n18:02:41 |     validation_patience: 30\n18:02:41 |     validation_share_agent: False\n18:02:41 |     variant: xlm\n18:02:41 |     verbose: False\n18:02:41 |     wandb_entity: None\n18:02:41 |     wandb_log: False\n18:02:41 |     wandb_name: None\n18:02:41 |     wandb_project: None\n18:02:41 |     warmup_rate: 0.0001\n18:02:41 |     warmup_updates: 1000\n18:02:41 |     weight_decay: None\n18:02:41 |     world_logs: \n18:02:41 |     wrap_memory_encoder: False\n18:02:41 | creating task(s): fromfile:parlaiformat\n18:02:41 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt\n18:02:41 | training...\n18:02:52 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4476 4.476e-10               .3730                 .4340   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3270            .5064              .4559   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5694 10.59     1 251.9 518.1       0          0 41.14  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4476             32768  2.822    .1206 6.005 .6980 1.055e-06 120.1   247   \n    ltrunc  ltrunclen  total_train_updates  tpb   tps   ups  weighted_f1  \n         0          0                   21  372 765.1 2.062        .4394\n\n18:03:02 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7824 7.824e-10               .7833                 .7423   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8291            .7815              .8276   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7404 10.79     1 255.7 962.5       0          0 75.27  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7824             32768  2.622    .1207 5.949 .6434 2.905e-06   119 447.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 374.7 1410 3.772        .7824\n\n18:03:02 | creating task(s): fromfile:parlaiformat\n18:03:02 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:03:02 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt\n18:03:02 | running eval: valid\n18:03:02 | eval completed in 0.20s\n18:03:02 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9600                 .9231   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9565                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 10.67   152  1709       0          0 134.8   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5758 2.905e-06    72 809.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  224 2518        .9583\n\u001b[0m\n18:03:02 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n18:03:02 | saving best valid model: /tmp/model5\n18:03:02 | Saving dictionary to /tmp/model5.dict\n18:03:05 | saving model checkpoint: /tmp/model5.checkpoint\n18:03:05 | Saving dictionary to /tmp/model5.checkpoint.dict\n18:03:22 | time:41s total_exs:1880 total_steps:94 epochs:78.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9944 9.944e-10               .9944                 .9888   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9945                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9891 10.78     1 255.5 904.1       0          0 70.77  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9944             32768  2.784    .1207 5.981 .5178 4.705e-06 119.6 423.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   94 375.1 1327 3.547        .9944\n\n18:03:25 | time:44s total_exs:2120 total_steps:106 epochs:88.33\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.42     1 268.3  1049       0          0 78.18  240   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.764    .1207 6.008 .3849 5.304e-06 120.2 469.7   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  106 388.5 1519 3.938            1\n\n18:03:25 | running eval: valid\n18:03:26 | eval completed in 0.20s\n18:03:26 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1710       0          0   135   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0809     6 .3057 5.304e-06    72   810       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    106  224 2520            1\n\u001b[0m\n18:03:26 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n18:03:26 | saving best valid model: /tmp/model5\n18:03:31 | task solved! stopping.\n18:03:31 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:03:31 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:03:31 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:03:31 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:03:31 | Using CUDA\n18:03:31 | loading dictionary from /tmp/model5.dict\n18:03:31 | num words = 54944\n18:03:36 | Loading existing model parameters from /tmp/model5\n18:03:37 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:03:38 | creating task(s): fromfile:parlaiformat\n18:03:38 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:03:38 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt\n18:03:38 | running eval: valid\n18:03:39 | eval completed in 0.21s\n18:03:39 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1647       0          0   130   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3057 5.304e-06    72 780.1       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    106  224 2427            1\n\u001b[0m\n18:03:39 | creating task(s): fromfile:parlaiformat\n18:03:39 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:03:39 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt\n18:03:39 | running eval: test\n18:03:39 | eval completed in 0.20s\n18:03:39 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1684       0          0 132.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3057 5.304e-06    72 797.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    106  224 2481            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:03:41.039837Z","iopub.execute_input":"2022-12-03T18:03:41.040229Z","iopub.status.idle":"2022-12-03T18:04:09.227476Z","shell.execute_reply.started":"2022-12-03T18:03:41.040190Z","shell.execute_reply":"2022-12-03T18:04:09.226260Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"18:03:48 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt)\u001b[0m\n18:03:48 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:03:48 | Using CUDA\n18:03:48 | loading dictionary from /tmp/model5.dict\n18:03:48 | num words = 54944\n18:03:52 | Loading existing model parameters from /tmp/model5\n18:03:59 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:04:01 | Opt:\n18:04:01 |     activation: gelu\n18:04:01 |     adafactor_eps: '[1e-30, 0.001]'\n18:04:01 |     adam_eps: 1e-08\n18:04:01 |     add_p1_after_newln: False\n18:04:01 |     aggregate_micro: False\n18:04:01 |     allow_missing_init_opts: False\n18:04:01 |     area_under_curve_class: None\n18:04:01 |     area_under_curve_digits: -1\n18:04:01 |     attention_dropout: 0.1\n18:04:01 |     batchsize: 40\n18:04:01 |     betas: '[0.9, 0.999]'\n18:04:01 |     bpe_add_prefix_space: None\n18:04:01 |     bpe_debug: False\n18:04:01 |     bpe_dropout: None\n18:04:01 |     bpe_merge: None\n18:04:01 |     bpe_vocab: None\n18:04:01 |     candidates: inline\n18:04:01 |     cap_num_predictions: 100\n18:04:01 |     checkpoint_activations: False\n18:04:01 |     class_weights: None\n18:04:01 |     classes: \"['__notok__', '__ok__']\"\n18:04:01 |     classes_from_file: None\n18:04:01 |     data_parallel: True\n18:04:01 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:04:01 |     datatype: train\n18:04:01 |     delimiter: '\\n'\n18:04:01 |     dict_class: parlai.core.dict:DictionaryAgent\n18:04:01 |     dict_endtoken: __start__\n18:04:01 |     dict_file: /tmp/model5.dict\n18:04:01 |     dict_include_test: False\n18:04:01 |     dict_include_valid: False\n18:04:01 |     dict_initpath: None\n18:04:01 |     dict_language: english\n18:04:01 |     dict_loaded: True\n18:04:01 |     dict_lower: True\n18:04:01 |     dict_max_ngram_size: -1\n18:04:01 |     dict_maxexs: -1\n18:04:01 |     dict_maxtokens: -1\n18:04:01 |     dict_minfreq: 0\n18:04:01 |     dict_nulltoken: __null__\n18:04:01 |     dict_starttoken: __start__\n18:04:01 |     dict_textfields: text,labels\n18:04:01 |     dict_tokenizer: bpe\n18:04:01 |     dict_unktoken: __unk__\n18:04:01 |     display_examples: False\n18:04:01 |     download_path: None\n18:04:01 |     dropout: 0.1\n18:04:01 |     dynamic_batching: None\n18:04:01 |     embedding_projection: random\n18:04:01 |     embedding_size: 768\n18:04:01 |     embedding_type: random\n18:04:01 |     embeddings_scale: False\n18:04:01 |     encode_candidate_vecs: True\n18:04:01 |     encode_candidate_vecs_batchsize: 256\n18:04:01 |     eval_batchsize: None\n18:04:01 |     eval_candidates: inline\n18:04:01 |     eval_dynamic_batching: None\n18:04:01 |     evaltask: None\n18:04:01 |     ffn_size: 3072\n18:04:01 |     final_extra_opt: \n18:04:01 |     fixed_candidate_vecs: reuse\n18:04:01 |     fixed_candidates_path: None\n18:04:01 |     force_fp16_tokens: True\n18:04:01 |     fp16: True\n18:04:01 |     fp16_impl: safe\n18:04:01 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-a.txt\n18:04:01 |     fromfile_datatype_extension: False\n18:04:01 |     gpu: -1\n18:04:01 |     gradient_clip: 0.1\n18:04:01 |     hide_labels: False\n18:04:01 |     history_add_global_end_token: None\n18:04:01 |     history_reversed: False\n18:04:01 |     history_size: 20\n18:04:01 |     ignore_bad_candidates: False\n18:04:01 |     ignore_labels: None\n18:04:01 |     image_cropsize: 224\n18:04:01 |     image_mode: raw\n18:04:01 |     image_size: 256\n18:04:01 |     inference: max\n18:04:01 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:04:01 |     init_opt: None\n18:04:01 |     interactive_candidates: fixed\n18:04:01 |     interactive_mode: False\n18:04:01 |     invsqrt_lr_decay_gamma: -1\n18:04:01 |     is_debug: False\n18:04:01 |     label_truncate: 72\n18:04:01 |     learn_embeddings: True\n18:04:01 |     learn_positional_embeddings: True\n18:04:01 |     learningrate: 5e-05\n18:04:01 |     load_from_pretrained_ranker: True\n18:04:01 |     log_every_n_secs: 10.0\n18:04:01 |     log_every_n_steps: 50\n18:04:01 |     log_keep_fields: all\n18:04:01 |     loglevel: info\n18:04:01 |     lr_scheduler: reduceonplateau\n18:04:01 |     lr_scheduler_decay: 0.5\n18:04:01 |     lr_scheduler_patience: 3\n18:04:01 |     max_train_steps: -1\n18:04:01 |     max_train_time: 7200.0\n18:04:01 |     memory_attention: sqrt\n18:04:01 |     metrics: default\n18:04:01 |     model: transformer/classifier\n18:04:01 |     model_file: /tmp/model5\n18:04:01 |     model_parallel: False\n18:04:01 |     momentum: 0\n18:04:01 |     multitask_weights: [1]\n18:04:01 |     mutators: None\n18:04:01 |     n_decoder_layers: -1\n18:04:01 |     n_encoder_layers: -1\n18:04:01 |     n_heads: 12\n18:04:01 |     n_layers: 12\n18:04:01 |     n_positions: 1024\n18:04:01 |     n_segments: 2\n18:04:01 |     nesterov: True\n18:04:01 |     no_cuda: False\n18:04:01 |     normalize_sent_emb: False\n18:04:01 |     num_epochs: -1\n18:04:01 |     num_examples: -1\n18:04:01 |     num_workers: 0\n18:04:01 |     nus: [0.7]\n18:04:01 |     optimizer: adamax\n18:04:01 |     output_scaling: 0.06\n18:04:01 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n18:04:01 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:04:01 |     person_tokens: False\n18:04:01 |     print_scores: False\n18:04:01 |     rank_candidates: False\n18:04:01 |     rank_top_k: -1\n18:04:01 |     reduction_type: mean\n18:04:01 |     ref_class: None\n18:04:01 |     relu_dropout: 0.0\n18:04:01 |     repeat_blocking_heuristic: True\n18:04:01 |     report_filename: \n18:04:01 |     return_cand_scores: False\n18:04:01 |     save_after_valid: True\n18:04:01 |     save_every_n_secs: -1\n18:04:01 |     save_format: conversations\n18:04:01 |     share_encoders: False\n18:04:01 |     share_word_embeddings: False\n18:04:01 |     short_final_eval: False\n18:04:01 |     special_tok_lst: None\n18:04:01 |     split_lines: False\n18:04:01 |     starttime: Dec03_18-02\n18:04:01 |     task: fromfile:parlaiformat\n18:04:01 |     tensorboard_log: False\n18:04:01 |     tensorboard_logdir: None\n18:04:01 |     text_truncate: 360\n18:04:01 |     threshold: 0.5\n18:04:01 |     topk: 5\n18:04:01 |     train_predict: False\n18:04:01 |     truncate: 1024\n18:04:01 |     update_classifier_head_only: False\n18:04:01 |     update_freq: 1\n18:04:01 |     use_memories: False\n18:04:01 |     use_reply: none\n18:04:01 |     validation_cutoff: 1.0\n18:04:01 |     validation_every_n_epochs: -1\n18:04:01 |     validation_every_n_secs: 20.0\n18:04:01 |     validation_every_n_steps: -1\n18:04:01 |     validation_max_exs: -1\n18:04:01 |     validation_metric: accuracy\n18:04:01 |     validation_metric_mode: max\n18:04:01 |     validation_patience: 30\n18:04:01 |     validation_share_agent: False\n18:04:01 |     variant: xlm\n18:04:01 |     verbose: False\n18:04:01 |     wandb_entity: None\n18:04:01 |     wandb_log: False\n18:04:01 |     wandb_name: None\n18:04:01 |     wandb_project: None\n18:04:01 |     warmup_rate: 0.0001\n18:04:01 |     warmup_updates: 1000\n18:04:01 |     weight_decay: None\n18:04:01 |     world_logs: \n18:04:01 |     wrap_memory_encoder: False\n18:04:01 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:04:01 | creating task(s): fromfile:parlaiformat\n18:04:01 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:04:01 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-a.txt\n18:04:07 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1750 1.75e-10               .1270                 .1333   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1212            .2180              .2091   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2277 11.79 551.8 508.6       0          0 36.87  200 .1750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9058 5.304e-06 239.6 220.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    106 791.4 729.5        .1730\u001b[0m\n18:04:07 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1750 1.75e-10               .1270                 .1333   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1212            .2180              .2091   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2277 11.79 551.8 508.6       0          0 36.87  200 .1750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9058 5.304e-06 239.6 220.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    106 791.4 729.5        .1730\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:04:09.229328Z","iopub.execute_input":"2022-12-03T18:04:09.229763Z","iopub.status.idle":"2022-12-03T18:04:35.437787Z","shell.execute_reply.started":"2022-12-03T18:04:09.229723Z","shell.execute_reply":"2022-12-03T18:04:35.436463Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"18:04:16 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_valid.txt)\u001b[0m\n18:04:16 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:04:16 | Using CUDA\n18:04:16 | loading dictionary from /tmp/model5.dict\n18:04:16 | num words = 54944\n18:04:20 | Loading existing model parameters from /tmp/model5\n18:04:26 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:04:27 | Opt:\n18:04:27 |     activation: gelu\n18:04:27 |     adafactor_eps: '[1e-30, 0.001]'\n18:04:27 |     adam_eps: 1e-08\n18:04:27 |     add_p1_after_newln: False\n18:04:27 |     aggregate_micro: False\n18:04:27 |     allow_missing_init_opts: False\n18:04:27 |     area_under_curve_class: None\n18:04:27 |     area_under_curve_digits: -1\n18:04:27 |     attention_dropout: 0.1\n18:04:27 |     batchsize: 40\n18:04:27 |     betas: '[0.9, 0.999]'\n18:04:27 |     bpe_add_prefix_space: None\n18:04:27 |     bpe_debug: False\n18:04:27 |     bpe_dropout: None\n18:04:27 |     bpe_merge: None\n18:04:27 |     bpe_vocab: None\n18:04:27 |     candidates: inline\n18:04:27 |     cap_num_predictions: 100\n18:04:27 |     checkpoint_activations: False\n18:04:27 |     class_weights: None\n18:04:27 |     classes: \"['__notok__', '__ok__']\"\n18:04:27 |     classes_from_file: None\n18:04:27 |     data_parallel: True\n18:04:27 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:04:27 |     datatype: train\n18:04:27 |     delimiter: '\\n'\n18:04:27 |     dict_class: parlai.core.dict:DictionaryAgent\n18:04:27 |     dict_endtoken: __start__\n18:04:27 |     dict_file: /tmp/model5.dict\n18:04:27 |     dict_include_test: False\n18:04:27 |     dict_include_valid: False\n18:04:27 |     dict_initpath: None\n18:04:27 |     dict_language: english\n18:04:27 |     dict_loaded: True\n18:04:27 |     dict_lower: True\n18:04:27 |     dict_max_ngram_size: -1\n18:04:27 |     dict_maxexs: -1\n18:04:27 |     dict_maxtokens: -1\n18:04:27 |     dict_minfreq: 0\n18:04:27 |     dict_nulltoken: __null__\n18:04:27 |     dict_starttoken: __start__\n18:04:27 |     dict_textfields: text,labels\n18:04:27 |     dict_tokenizer: bpe\n18:04:27 |     dict_unktoken: __unk__\n18:04:27 |     display_examples: False\n18:04:27 |     download_path: None\n18:04:27 |     dropout: 0.1\n18:04:27 |     dynamic_batching: None\n18:04:27 |     embedding_projection: random\n18:04:27 |     embedding_size: 768\n18:04:27 |     embedding_type: random\n18:04:27 |     embeddings_scale: False\n18:04:27 |     encode_candidate_vecs: True\n18:04:27 |     encode_candidate_vecs_batchsize: 256\n18:04:27 |     eval_batchsize: None\n18:04:27 |     eval_candidates: inline\n18:04:27 |     eval_dynamic_batching: None\n18:04:27 |     evaltask: None\n18:04:27 |     ffn_size: 3072\n18:04:27 |     final_extra_opt: \n18:04:27 |     fixed_candidate_vecs: reuse\n18:04:27 |     fixed_candidates_path: None\n18:04:27 |     force_fp16_tokens: True\n18:04:27 |     fp16: True\n18:04:27 |     fp16_impl: safe\n18:04:27 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-b.txt\n18:04:27 |     fromfile_datatype_extension: False\n18:04:27 |     gpu: -1\n18:04:27 |     gradient_clip: 0.1\n18:04:27 |     hide_labels: False\n18:04:27 |     history_add_global_end_token: None\n18:04:27 |     history_reversed: False\n18:04:27 |     history_size: 20\n18:04:27 |     ignore_bad_candidates: False\n18:04:27 |     ignore_labels: None\n18:04:27 |     image_cropsize: 224\n18:04:27 |     image_mode: raw\n18:04:27 |     image_size: 256\n18:04:27 |     inference: max\n18:04:27 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:04:27 |     init_opt: None\n18:04:27 |     interactive_candidates: fixed\n18:04:27 |     interactive_mode: False\n18:04:27 |     invsqrt_lr_decay_gamma: -1\n18:04:27 |     is_debug: False\n18:04:27 |     label_truncate: 72\n18:04:27 |     learn_embeddings: True\n18:04:27 |     learn_positional_embeddings: True\n18:04:27 |     learningrate: 5e-05\n18:04:27 |     load_from_pretrained_ranker: True\n18:04:27 |     log_every_n_secs: 10.0\n18:04:27 |     log_every_n_steps: 50\n18:04:27 |     log_keep_fields: all\n18:04:27 |     loglevel: info\n18:04:27 |     lr_scheduler: reduceonplateau\n18:04:27 |     lr_scheduler_decay: 0.5\n18:04:27 |     lr_scheduler_patience: 3\n18:04:27 |     max_train_steps: -1\n18:04:27 |     max_train_time: 7200.0\n18:04:27 |     memory_attention: sqrt\n18:04:27 |     metrics: default\n18:04:27 |     model: transformer/classifier\n18:04:27 |     model_file: /tmp/model5\n18:04:27 |     model_parallel: False\n18:04:27 |     momentum: 0\n18:04:27 |     multitask_weights: [1]\n18:04:27 |     mutators: None\n18:04:27 |     n_decoder_layers: -1\n18:04:27 |     n_encoder_layers: -1\n18:04:27 |     n_heads: 12\n18:04:27 |     n_layers: 12\n18:04:27 |     n_positions: 1024\n18:04:27 |     n_segments: 2\n18:04:27 |     nesterov: True\n18:04:27 |     no_cuda: False\n18:04:27 |     normalize_sent_emb: False\n18:04:27 |     num_epochs: -1\n18:04:27 |     num_examples: -1\n18:04:27 |     num_workers: 0\n18:04:27 |     nus: [0.7]\n18:04:27 |     optimizer: adamax\n18:04:27 |     output_scaling: 0.06\n18:04:27 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n18:04:27 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:04:27 |     person_tokens: False\n18:04:27 |     print_scores: False\n18:04:27 |     rank_candidates: False\n18:04:27 |     rank_top_k: -1\n18:04:27 |     reduction_type: mean\n18:04:27 |     ref_class: None\n18:04:27 |     relu_dropout: 0.0\n18:04:27 |     repeat_blocking_heuristic: True\n18:04:27 |     report_filename: \n18:04:27 |     return_cand_scores: False\n18:04:27 |     save_after_valid: True\n18:04:27 |     save_every_n_secs: -1\n18:04:27 |     save_format: conversations\n18:04:27 |     share_encoders: False\n18:04:27 |     share_word_embeddings: False\n18:04:27 |     short_final_eval: False\n18:04:27 |     special_tok_lst: None\n18:04:27 |     split_lines: False\n18:04:27 |     starttime: Dec03_18-02\n18:04:27 |     task: fromfile:parlaiformat\n18:04:27 |     tensorboard_log: False\n18:04:27 |     tensorboard_logdir: None\n18:04:27 |     text_truncate: 360\n18:04:27 |     threshold: 0.5\n18:04:27 |     topk: 5\n18:04:27 |     train_predict: False\n18:04:27 |     truncate: 1024\n18:04:27 |     update_classifier_head_only: False\n18:04:27 |     update_freq: 1\n18:04:27 |     use_memories: False\n18:04:27 |     use_reply: none\n18:04:27 |     validation_cutoff: 1.0\n18:04:27 |     validation_every_n_epochs: -1\n18:04:27 |     validation_every_n_secs: 20.0\n18:04:27 |     validation_every_n_steps: -1\n18:04:27 |     validation_max_exs: -1\n18:04:27 |     validation_metric: accuracy\n18:04:27 |     validation_metric_mode: max\n18:04:27 |     validation_patience: 30\n18:04:27 |     validation_share_agent: False\n18:04:27 |     variant: xlm\n18:04:27 |     verbose: False\n18:04:27 |     wandb_entity: None\n18:04:27 |     wandb_log: False\n18:04:27 |     wandb_name: None\n18:04:27 |     wandb_project: None\n18:04:27 |     warmup_rate: 0.0001\n18:04:27 |     warmup_updates: 1000\n18:04:27 |     weight_decay: None\n18:04:27 |     world_logs: \n18:04:27 |     wrap_memory_encoder: False\n18:04:27 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:04:27 | creating task(s): fromfile:parlaiformat\n18:04:27 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:04:27 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr1type2/run5/data_train-b.txt\n18:04:33 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8250 8.25e-10               .8168                 .8667   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7723            .8325              .7909   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8788 11.79 551.8   491       0          0 35.59  200 .8250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5488 5.304e-06 240.4 213.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    106 792.2 704.9        .8246\u001b[0m\n18:04:33 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8250 8.25e-10               .8168                 .8667   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7723            .8325              .7909   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8788 11.79 551.8   491       0          0 35.59  200 .8250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5488 5.304e-06 240.4 213.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    106 792.2 704.9        .8246\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:04:35.439677Z","iopub.execute_input":"2022-12-03T18:04:35.440100Z","iopub.status.idle":"2022-12-03T18:04:36.603531Z","shell.execute_reply.started":"2022-12-03T18:04:35.440055Z","shell.execute_reply":"2022-12-03T18:04:36.602228Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"Choose completion prev2corr2type1","metadata":{}},{"cell_type":"markdown","source":"run 1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:04:36.605806Z","iopub.execute_input":"2022-12-03T18:04:36.606201Z","iopub.status.idle":"2022-12-03T18:05:29.260128Z","shell.execute_reply.started":"2022-12-03T18:04:36.606162Z","shell.execute_reply":"2022-12-03T18:05:29.258922Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"18:04:43 | building dictionary first...\n18:04:43 | No model with opt yet at: /tmp/model1(.opt)\n18:04:43 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:04:43 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:04:43 | Using CUDA\n18:04:43 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:04:43 | num words = 54944\n18:04:48 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:04:54 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:04:54 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:04:54 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:04:54 | Opt:\n18:04:54 |     activation: gelu\n18:04:54 |     adafactor_eps: '(1e-30, 0.001)'\n18:04:54 |     adam_eps: 1e-08\n18:04:54 |     add_p1_after_newln: False\n18:04:54 |     aggregate_micro: False\n18:04:54 |     allow_missing_init_opts: False\n18:04:54 |     attention_dropout: 0.1\n18:04:54 |     batchsize: 20\n18:04:54 |     betas: '(0.9, 0.999)'\n18:04:54 |     bpe_add_prefix_space: None\n18:04:54 |     bpe_debug: False\n18:04:54 |     bpe_dropout: None\n18:04:54 |     bpe_merge: None\n18:04:54 |     bpe_vocab: None\n18:04:54 |     candidates: inline\n18:04:54 |     cap_num_predictions: 100\n18:04:54 |     checkpoint_activations: False\n18:04:54 |     class_weights: None\n18:04:54 |     classes: \"['__notok__', '__ok__']\"\n18:04:54 |     classes_from_file: None\n18:04:54 |     data_parallel: True\n18:04:54 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:04:54 |     datatype: train\n18:04:54 |     delimiter: '\\n'\n18:04:54 |     dict_class: parlai.core.dict:DictionaryAgent\n18:04:54 |     dict_endtoken: __start__\n18:04:54 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:04:54 |     dict_include_test: False\n18:04:54 |     dict_include_valid: False\n18:04:54 |     dict_initpath: None\n18:04:54 |     dict_language: english\n18:04:54 |     dict_loaded: True\n18:04:54 |     dict_lower: True\n18:04:54 |     dict_max_ngram_size: -1\n18:04:54 |     dict_maxexs: -1\n18:04:54 |     dict_maxtokens: -1\n18:04:54 |     dict_minfreq: 0\n18:04:54 |     dict_nulltoken: __null__\n18:04:54 |     dict_starttoken: __start__\n18:04:54 |     dict_textfields: text,labels\n18:04:54 |     dict_tokenizer: bpe\n18:04:54 |     dict_unktoken: __unk__\n18:04:54 |     display_examples: False\n18:04:54 |     download_path: None\n18:04:54 |     dropout: 0.1\n18:04:54 |     dynamic_batching: None\n18:04:54 |     embedding_projection: random\n18:04:54 |     embedding_size: 768\n18:04:54 |     embedding_type: random\n18:04:54 |     embeddings_scale: False\n18:04:54 |     encode_candidate_vecs: True\n18:04:54 |     encode_candidate_vecs_batchsize: 256\n18:04:54 |     eval_batchsize: None\n18:04:54 |     eval_candidates: inline\n18:04:54 |     eval_dynamic_batching: None\n18:04:54 |     evaltask: None\n18:04:54 |     ffn_size: 3072\n18:04:54 |     final_extra_opt: \n18:04:54 |     fixed_candidate_vecs: reuse\n18:04:54 |     fixed_candidates_path: None\n18:04:54 |     force_fp16_tokens: False\n18:04:54 |     fp16: True\n18:04:54 |     fp16_impl: safe\n18:04:54 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt\n18:04:54 |     fromfile_datatype_extension: False\n18:04:54 |     gpu: -1\n18:04:54 |     gradient_clip: 0.1\n18:04:54 |     hide_labels: False\n18:04:54 |     history_add_global_end_token: None\n18:04:54 |     history_reversed: False\n18:04:54 |     history_size: 20\n18:04:54 |     ignore_bad_candidates: False\n18:04:54 |     ignore_labels: None\n18:04:54 |     image_cropsize: 224\n18:04:54 |     image_mode: raw\n18:04:54 |     image_size: 256\n18:04:54 |     inference: max\n18:04:54 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:04:54 |     init_opt: None\n18:04:54 |     interactive_candidates: fixed\n18:04:54 |     interactive_mode: False\n18:04:54 |     invsqrt_lr_decay_gamma: -1\n18:04:54 |     is_debug: False\n18:04:54 |     label_truncate: 72\n18:04:54 |     learn_embeddings: True\n18:04:54 |     learn_positional_embeddings: True\n18:04:54 |     learningrate: 5e-05\n18:04:54 |     load_from_checkpoint: False\n18:04:54 |     load_from_pretrained_ranker: True\n18:04:54 |     log_every_n_secs: 10.0\n18:04:54 |     log_every_n_steps: 50\n18:04:54 |     log_keep_fields: all\n18:04:54 |     loglevel: info\n18:04:54 |     lr_scheduler: reduceonplateau\n18:04:54 |     lr_scheduler_decay: 0.5\n18:04:54 |     lr_scheduler_patience: 3\n18:04:54 |     max_train_steps: -1\n18:04:54 |     max_train_time: 7200.0\n18:04:54 |     memory_attention: sqrt\n18:04:54 |     metrics: default\n18:04:54 |     model: transformer/classifier\n18:04:54 |     model_file: /tmp/model1\n18:04:54 |     model_parallel: False\n18:04:54 |     momentum: 0\n18:04:54 |     multitask_weights: [1]\n18:04:54 |     mutators: None\n18:04:54 |     n_decoder_layers: -1\n18:04:54 |     n_encoder_layers: -1\n18:04:54 |     n_heads: 12\n18:04:54 |     n_layers: 12\n18:04:54 |     n_positions: 1024\n18:04:54 |     n_segments: 2\n18:04:54 |     nesterov: True\n18:04:54 |     no_cuda: False\n18:04:54 |     normalize_sent_emb: False\n18:04:54 |     num_epochs: -1\n18:04:54 |     num_workers: 0\n18:04:54 |     nus: (0.7,)\n18:04:54 |     optimizer: adamax\n18:04:54 |     output_scaling: 0.06\n18:04:54 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n18:04:54 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:04:54 |     person_tokens: False\n18:04:54 |     print_scores: False\n18:04:54 |     rank_candidates: False\n18:04:54 |     rank_top_k: -1\n18:04:54 |     reduction_type: mean\n18:04:54 |     ref_class: None\n18:04:54 |     relu_dropout: 0.0\n18:04:54 |     repeat_blocking_heuristic: True\n18:04:54 |     return_cand_scores: False\n18:04:54 |     save_after_valid: True\n18:04:54 |     save_every_n_secs: -1\n18:04:54 |     save_format: conversations\n18:04:54 |     share_encoders: False\n18:04:54 |     share_word_embeddings: False\n18:04:54 |     short_final_eval: False\n18:04:54 |     special_tok_lst: None\n18:04:54 |     split_lines: False\n18:04:54 |     starttime: Dec03_18-04\n18:04:54 |     task: fromfile:parlaiformat\n18:04:54 |     tensorboard_log: False\n18:04:54 |     tensorboard_logdir: None\n18:04:54 |     text_truncate: 360\n18:04:54 |     threshold: 0.5\n18:04:54 |     topk: 5\n18:04:54 |     train_predict: False\n18:04:54 |     truncate: 1024\n18:04:54 |     update_classifier_head_only: False\n18:04:54 |     update_freq: 1\n18:04:54 |     use_memories: False\n18:04:54 |     use_reply: none\n18:04:54 |     validation_cutoff: 1.0\n18:04:54 |     validation_every_n_epochs: -1\n18:04:54 |     validation_every_n_secs: 20.0\n18:04:54 |     validation_every_n_steps: -1\n18:04:54 |     validation_max_exs: -1\n18:04:54 |     validation_metric: accuracy\n18:04:54 |     validation_metric_mode: max\n18:04:54 |     validation_patience: 30\n18:04:54 |     validation_share_agent: False\n18:04:54 |     variant: xlm\n18:04:54 |     verbose: False\n18:04:54 |     wandb_entity: None\n18:04:54 |     wandb_log: False\n18:04:54 |     wandb_name: None\n18:04:54 |     wandb_project: None\n18:04:54 |     warmup_rate: 0.0001\n18:04:54 |     warmup_updates: 1000\n18:04:54 |     weight_decay: None\n18:04:54 |     world_logs: \n18:04:54 |     wrap_memory_encoder: False\n18:04:54 | creating task(s): fromfile:parlaiformat\n18:04:54 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt\n18:04:54 | training...\n18:05:05 | time:10s total_exs:380 total_steps:19 epochs:15.83\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5526 5.526e-10               .5619                 .5215   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6089            .5430              .5906   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .5025 11.69     1 273.8   511       0          0 37.32  380   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5526             32768  2.811    .1189 5.942 .6882 9.549e-07 118.8 221.8   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps  ups  weighted_f1  \n         0          0                   19 392.6 732.8 1.87        .5519\n\n18:05:14 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8105 8.105e-10               .8043                 .8291   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7810            .8163              .7940   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8399 11.66     1 273.1  1059       0          0 77.56  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8105             32768  2.509    .1189 5.997 .6456 2.855e-06 119.9 465.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 393.1 1524 3.887        .8104\n\n18:05:14 | creating task(s): fromfile:parlaiformat\n18:05:14 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:05:14 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt\n18:05:14 | running eval: valid\n18:05:15 | eval completed in 0.19s\n18:05:15 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1907       0          0 141.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5848 2.855e-06    72 850.1       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 233.5 2758            1\n\u001b[0m\n18:05:15 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n18:05:15 | saving best valid model: /tmp/model1\n18:05:15 | Saving dictionary to /tmp/model1.dict\n18:05:18 | task solved! stopping.\n18:05:18 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:05:18 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:05:18 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:05:18 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:05:18 | Using CUDA\n18:05:18 | loading dictionary from /tmp/model1.dict\n18:05:18 | num words = 54944\n18:05:23 | Loading existing model parameters from /tmp/model1\n18:05:25 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:05:26 | creating task(s): fromfile:parlaiformat\n18:05:26 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:05:26 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt\n18:05:26 | running eval: valid\n18:05:27 | eval completed in 0.41s\n18:05:27 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5 918.7       0          0  68.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5848 2.855e-06    72 409.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 233.5 1328            1\n\u001b[0m\n18:05:27 | creating task(s): fromfile:parlaiformat\n18:05:27 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:05:27 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt\n18:05:27 | running eval: test\n18:05:27 | eval completed in 0.20s\n18:05:27 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1768       0          0 131.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5848 2.855e-06    72 787.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 233.5 2556            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:05:29.261930Z","iopub.execute_input":"2022-12-03T18:05:29.262315Z","iopub.status.idle":"2022-12-03T18:05:58.557163Z","shell.execute_reply.started":"2022-12-03T18:05:29.262276Z","shell.execute_reply":"2022-12-03T18:05:58.555458Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"18:05:37 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt)\u001b[0m\n18:05:37 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:05:37 | Using CUDA\n18:05:37 | loading dictionary from /tmp/model1.dict\n18:05:37 | num words = 54944\n18:05:41 | Loading existing model parameters from /tmp/model1\n18:05:49 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:05:51 | Opt:\n18:05:51 |     activation: gelu\n18:05:51 |     adafactor_eps: '[1e-30, 0.001]'\n18:05:51 |     adam_eps: 1e-08\n18:05:51 |     add_p1_after_newln: False\n18:05:51 |     aggregate_micro: False\n18:05:51 |     allow_missing_init_opts: False\n18:05:51 |     area_under_curve_class: None\n18:05:51 |     area_under_curve_digits: -1\n18:05:51 |     attention_dropout: 0.1\n18:05:51 |     batchsize: 40\n18:05:51 |     betas: '[0.9, 0.999]'\n18:05:51 |     bpe_add_prefix_space: None\n18:05:51 |     bpe_debug: False\n18:05:51 |     bpe_dropout: None\n18:05:51 |     bpe_merge: None\n18:05:51 |     bpe_vocab: None\n18:05:51 |     candidates: inline\n18:05:51 |     cap_num_predictions: 100\n18:05:51 |     checkpoint_activations: False\n18:05:51 |     class_weights: None\n18:05:51 |     classes: \"['__notok__', '__ok__']\"\n18:05:51 |     classes_from_file: None\n18:05:51 |     data_parallel: True\n18:05:51 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:05:51 |     datatype: train\n18:05:51 |     delimiter: '\\n'\n18:05:51 |     dict_class: parlai.core.dict:DictionaryAgent\n18:05:51 |     dict_endtoken: __start__\n18:05:51 |     dict_file: /tmp/model1.dict\n18:05:51 |     dict_include_test: False\n18:05:51 |     dict_include_valid: False\n18:05:51 |     dict_initpath: None\n18:05:51 |     dict_language: english\n18:05:51 |     dict_loaded: True\n18:05:51 |     dict_lower: True\n18:05:51 |     dict_max_ngram_size: -1\n18:05:51 |     dict_maxexs: -1\n18:05:51 |     dict_maxtokens: -1\n18:05:51 |     dict_minfreq: 0\n18:05:51 |     dict_nulltoken: __null__\n18:05:51 |     dict_starttoken: __start__\n18:05:51 |     dict_textfields: text,labels\n18:05:51 |     dict_tokenizer: bpe\n18:05:51 |     dict_unktoken: __unk__\n18:05:51 |     display_examples: False\n18:05:51 |     download_path: None\n18:05:51 |     dropout: 0.1\n18:05:51 |     dynamic_batching: None\n18:05:51 |     embedding_projection: random\n18:05:51 |     embedding_size: 768\n18:05:51 |     embedding_type: random\n18:05:51 |     embeddings_scale: False\n18:05:51 |     encode_candidate_vecs: True\n18:05:51 |     encode_candidate_vecs_batchsize: 256\n18:05:51 |     eval_batchsize: None\n18:05:51 |     eval_candidates: inline\n18:05:51 |     eval_dynamic_batching: None\n18:05:51 |     evaltask: None\n18:05:51 |     ffn_size: 3072\n18:05:51 |     final_extra_opt: \n18:05:51 |     fixed_candidate_vecs: reuse\n18:05:51 |     fixed_candidates_path: None\n18:05:51 |     force_fp16_tokens: True\n18:05:51 |     fp16: True\n18:05:51 |     fp16_impl: safe\n18:05:51 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-a.txt\n18:05:51 |     fromfile_datatype_extension: False\n18:05:51 |     gpu: -1\n18:05:51 |     gradient_clip: 0.1\n18:05:51 |     hide_labels: False\n18:05:51 |     history_add_global_end_token: None\n18:05:51 |     history_reversed: False\n18:05:51 |     history_size: 20\n18:05:51 |     ignore_bad_candidates: False\n18:05:51 |     ignore_labels: None\n18:05:51 |     image_cropsize: 224\n18:05:51 |     image_mode: raw\n18:05:51 |     image_size: 256\n18:05:51 |     inference: max\n18:05:51 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:05:51 |     init_opt: None\n18:05:51 |     interactive_candidates: fixed\n18:05:51 |     interactive_mode: False\n18:05:51 |     invsqrt_lr_decay_gamma: -1\n18:05:51 |     is_debug: False\n18:05:51 |     label_truncate: 72\n18:05:51 |     learn_embeddings: True\n18:05:51 |     learn_positional_embeddings: True\n18:05:51 |     learningrate: 5e-05\n18:05:51 |     load_from_pretrained_ranker: True\n18:05:51 |     log_every_n_secs: 10.0\n18:05:51 |     log_every_n_steps: 50\n18:05:51 |     log_keep_fields: all\n18:05:51 |     loglevel: info\n18:05:51 |     lr_scheduler: reduceonplateau\n18:05:51 |     lr_scheduler_decay: 0.5\n18:05:51 |     lr_scheduler_patience: 3\n18:05:51 |     max_train_steps: -1\n18:05:51 |     max_train_time: 7200.0\n18:05:51 |     memory_attention: sqrt\n18:05:51 |     metrics: default\n18:05:51 |     model: transformer/classifier\n18:05:51 |     model_file: /tmp/model1\n18:05:51 |     model_parallel: False\n18:05:51 |     momentum: 0\n18:05:51 |     multitask_weights: [1]\n18:05:51 |     mutators: None\n18:05:51 |     n_decoder_layers: -1\n18:05:51 |     n_encoder_layers: -1\n18:05:51 |     n_heads: 12\n18:05:51 |     n_layers: 12\n18:05:51 |     n_positions: 1024\n18:05:51 |     n_segments: 2\n18:05:51 |     nesterov: True\n18:05:51 |     no_cuda: False\n18:05:51 |     normalize_sent_emb: False\n18:05:51 |     num_epochs: -1\n18:05:51 |     num_examples: -1\n18:05:51 |     num_workers: 0\n18:05:51 |     nus: [0.7]\n18:05:51 |     optimizer: adamax\n18:05:51 |     output_scaling: 0.06\n18:05:51 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n18:05:51 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:05:51 |     person_tokens: False\n18:05:51 |     print_scores: False\n18:05:51 |     rank_candidates: False\n18:05:51 |     rank_top_k: -1\n18:05:51 |     reduction_type: mean\n18:05:51 |     ref_class: None\n18:05:51 |     relu_dropout: 0.0\n18:05:51 |     repeat_blocking_heuristic: True\n18:05:51 |     report_filename: \n18:05:51 |     return_cand_scores: False\n18:05:51 |     save_after_valid: True\n18:05:51 |     save_every_n_secs: -1\n18:05:51 |     save_format: conversations\n18:05:51 |     share_encoders: False\n18:05:51 |     share_word_embeddings: False\n18:05:51 |     short_final_eval: False\n18:05:51 |     special_tok_lst: None\n18:05:51 |     split_lines: False\n18:05:51 |     starttime: Dec03_18-04\n18:05:51 |     task: fromfile:parlaiformat\n18:05:51 |     tensorboard_log: False\n18:05:51 |     tensorboard_logdir: None\n18:05:51 |     text_truncate: 360\n18:05:51 |     threshold: 0.5\n18:05:51 |     topk: 5\n18:05:51 |     train_predict: False\n18:05:51 |     truncate: 1024\n18:05:51 |     update_classifier_head_only: False\n18:05:51 |     update_freq: 1\n18:05:51 |     use_memories: False\n18:05:51 |     use_reply: none\n18:05:51 |     validation_cutoff: 1.0\n18:05:51 |     validation_every_n_epochs: -1\n18:05:51 |     validation_every_n_secs: 20.0\n18:05:51 |     validation_every_n_steps: -1\n18:05:51 |     validation_max_exs: -1\n18:05:51 |     validation_metric: accuracy\n18:05:51 |     validation_metric_mode: max\n18:05:51 |     validation_patience: 30\n18:05:51 |     validation_share_agent: False\n18:05:51 |     variant: xlm\n18:05:51 |     verbose: False\n18:05:51 |     wandb_entity: None\n18:05:51 |     wandb_log: False\n18:05:51 |     wandb_name: None\n18:05:51 |     wandb_project: None\n18:05:51 |     warmup_rate: 0.0001\n18:05:51 |     warmup_updates: 1000\n18:05:51 |     weight_decay: None\n18:05:51 |     world_logs: \n18:05:51 |     wrap_memory_encoder: False\n18:05:51 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:05:51 | creating task(s): fromfile:parlaiformat\n18:05:51 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:05:51 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-a.txt\n18:05:57 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2900 2.9e-10               .3604                 .3279   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4000            .2022              .2308   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1800 11.13 525.2 492.4       0          0  37.5  200 .2900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .7319 2.855e-06   240   225       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 717.4        .2813\u001b[0m\n18:05:57 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .2900 2.9e-10               .3604                 .3279   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4000            .2022              .2308   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1800 11.13 525.2 492.4       0          0  37.5  200 .2900   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .7319 2.855e-06   240   225       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 717.4        .2813\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:05:58.559201Z","iopub.execute_input":"2022-12-03T18:05:58.559580Z","iopub.status.idle":"2022-12-03T18:06:25.044804Z","shell.execute_reply.started":"2022-12-03T18:05:58.559546Z","shell.execute_reply":"2022-12-03T18:06:25.043625Z"},"trusted":true},"execution_count":129,"outputs":[{"name":"stdout","text":"18:06:05 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_valid.txt)\u001b[0m\n18:06:05 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:06:05 | Using CUDA\n18:06:05 | loading dictionary from /tmp/model1.dict\n18:06:05 | num words = 54944\n18:06:10 | Loading existing model parameters from /tmp/model1\n18:06:16 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:06:17 | Opt:\n18:06:17 |     activation: gelu\n18:06:17 |     adafactor_eps: '[1e-30, 0.001]'\n18:06:17 |     adam_eps: 1e-08\n18:06:17 |     add_p1_after_newln: False\n18:06:17 |     aggregate_micro: False\n18:06:17 |     allow_missing_init_opts: False\n18:06:17 |     area_under_curve_class: None\n18:06:17 |     area_under_curve_digits: -1\n18:06:17 |     attention_dropout: 0.1\n18:06:17 |     batchsize: 40\n18:06:17 |     betas: '[0.9, 0.999]'\n18:06:17 |     bpe_add_prefix_space: None\n18:06:17 |     bpe_debug: False\n18:06:17 |     bpe_dropout: None\n18:06:17 |     bpe_merge: None\n18:06:17 |     bpe_vocab: None\n18:06:17 |     candidates: inline\n18:06:17 |     cap_num_predictions: 100\n18:06:17 |     checkpoint_activations: False\n18:06:17 |     class_weights: None\n18:06:17 |     classes: \"['__notok__', '__ok__']\"\n18:06:17 |     classes_from_file: None\n18:06:17 |     data_parallel: True\n18:06:17 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:06:17 |     datatype: train\n18:06:17 |     delimiter: '\\n'\n18:06:17 |     dict_class: parlai.core.dict:DictionaryAgent\n18:06:17 |     dict_endtoken: __start__\n18:06:17 |     dict_file: /tmp/model1.dict\n18:06:17 |     dict_include_test: False\n18:06:17 |     dict_include_valid: False\n18:06:17 |     dict_initpath: None\n18:06:17 |     dict_language: english\n18:06:17 |     dict_loaded: True\n18:06:17 |     dict_lower: True\n18:06:17 |     dict_max_ngram_size: -1\n18:06:17 |     dict_maxexs: -1\n18:06:17 |     dict_maxtokens: -1\n18:06:17 |     dict_minfreq: 0\n18:06:17 |     dict_nulltoken: __null__\n18:06:17 |     dict_starttoken: __start__\n18:06:17 |     dict_textfields: text,labels\n18:06:17 |     dict_tokenizer: bpe\n18:06:17 |     dict_unktoken: __unk__\n18:06:17 |     display_examples: False\n18:06:17 |     download_path: None\n18:06:17 |     dropout: 0.1\n18:06:17 |     dynamic_batching: None\n18:06:17 |     embedding_projection: random\n18:06:17 |     embedding_size: 768\n18:06:17 |     embedding_type: random\n18:06:17 |     embeddings_scale: False\n18:06:17 |     encode_candidate_vecs: True\n18:06:17 |     encode_candidate_vecs_batchsize: 256\n18:06:17 |     eval_batchsize: None\n18:06:17 |     eval_candidates: inline\n18:06:17 |     eval_dynamic_batching: None\n18:06:17 |     evaltask: None\n18:06:17 |     ffn_size: 3072\n18:06:17 |     final_extra_opt: \n18:06:17 |     fixed_candidate_vecs: reuse\n18:06:17 |     fixed_candidates_path: None\n18:06:17 |     force_fp16_tokens: True\n18:06:17 |     fp16: True\n18:06:17 |     fp16_impl: safe\n18:06:17 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-b.txt\n18:06:17 |     fromfile_datatype_extension: False\n18:06:17 |     gpu: -1\n18:06:17 |     gradient_clip: 0.1\n18:06:17 |     hide_labels: False\n18:06:17 |     history_add_global_end_token: None\n18:06:17 |     history_reversed: False\n18:06:17 |     history_size: 20\n18:06:17 |     ignore_bad_candidates: False\n18:06:17 |     ignore_labels: None\n18:06:17 |     image_cropsize: 224\n18:06:17 |     image_mode: raw\n18:06:17 |     image_size: 256\n18:06:17 |     inference: max\n18:06:17 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:06:17 |     init_opt: None\n18:06:17 |     interactive_candidates: fixed\n18:06:17 |     interactive_mode: False\n18:06:17 |     invsqrt_lr_decay_gamma: -1\n18:06:17 |     is_debug: False\n18:06:17 |     label_truncate: 72\n18:06:17 |     learn_embeddings: True\n18:06:17 |     learn_positional_embeddings: True\n18:06:17 |     learningrate: 5e-05\n18:06:17 |     load_from_pretrained_ranker: True\n18:06:17 |     log_every_n_secs: 10.0\n18:06:17 |     log_every_n_steps: 50\n18:06:17 |     log_keep_fields: all\n18:06:17 |     loglevel: info\n18:06:17 |     lr_scheduler: reduceonplateau\n18:06:17 |     lr_scheduler_decay: 0.5\n18:06:17 |     lr_scheduler_patience: 3\n18:06:17 |     max_train_steps: -1\n18:06:17 |     max_train_time: 7200.0\n18:06:17 |     memory_attention: sqrt\n18:06:17 |     metrics: default\n18:06:17 |     model: transformer/classifier\n18:06:17 |     model_file: /tmp/model1\n18:06:17 |     model_parallel: False\n18:06:17 |     momentum: 0\n18:06:17 |     multitask_weights: [1]\n18:06:17 |     mutators: None\n18:06:17 |     n_decoder_layers: -1\n18:06:17 |     n_encoder_layers: -1\n18:06:17 |     n_heads: 12\n18:06:17 |     n_layers: 12\n18:06:17 |     n_positions: 1024\n18:06:17 |     n_segments: 2\n18:06:17 |     nesterov: True\n18:06:17 |     no_cuda: False\n18:06:17 |     normalize_sent_emb: False\n18:06:17 |     num_epochs: -1\n18:06:17 |     num_examples: -1\n18:06:17 |     num_workers: 0\n18:06:17 |     nus: [0.7]\n18:06:17 |     optimizer: adamax\n18:06:17 |     output_scaling: 0.06\n18:06:17 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n18:06:17 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:06:17 |     person_tokens: False\n18:06:17 |     print_scores: False\n18:06:17 |     rank_candidates: False\n18:06:17 |     rank_top_k: -1\n18:06:17 |     reduction_type: mean\n18:06:17 |     ref_class: None\n18:06:17 |     relu_dropout: 0.0\n18:06:17 |     repeat_blocking_heuristic: True\n18:06:17 |     report_filename: \n18:06:17 |     return_cand_scores: False\n18:06:17 |     save_after_valid: True\n18:06:17 |     save_every_n_secs: -1\n18:06:17 |     save_format: conversations\n18:06:17 |     share_encoders: False\n18:06:17 |     share_word_embeddings: False\n18:06:17 |     short_final_eval: False\n18:06:17 |     special_tok_lst: None\n18:06:17 |     split_lines: False\n18:06:17 |     starttime: Dec03_18-04\n18:06:17 |     task: fromfile:parlaiformat\n18:06:17 |     tensorboard_log: False\n18:06:17 |     tensorboard_logdir: None\n18:06:17 |     text_truncate: 360\n18:06:17 |     threshold: 0.5\n18:06:17 |     topk: 5\n18:06:17 |     train_predict: False\n18:06:17 |     truncate: 1024\n18:06:17 |     update_classifier_head_only: False\n18:06:17 |     update_freq: 1\n18:06:17 |     use_memories: False\n18:06:17 |     use_reply: none\n18:06:17 |     validation_cutoff: 1.0\n18:06:17 |     validation_every_n_epochs: -1\n18:06:17 |     validation_every_n_secs: 20.0\n18:06:17 |     validation_every_n_steps: -1\n18:06:17 |     validation_max_exs: -1\n18:06:17 |     validation_metric: accuracy\n18:06:17 |     validation_metric_mode: max\n18:06:17 |     validation_patience: 30\n18:06:17 |     validation_share_agent: False\n18:06:17 |     variant: xlm\n18:06:17 |     verbose: False\n18:06:17 |     wandb_entity: None\n18:06:17 |     wandb_log: False\n18:06:17 |     wandb_name: None\n18:06:17 |     wandb_project: None\n18:06:17 |     warmup_rate: 0.0001\n18:06:17 |     warmup_updates: 1000\n18:06:17 |     weight_decay: None\n18:06:17 |     world_logs: \n18:06:17 |     wrap_memory_encoder: False\n18:06:17 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:06:17 | creating task(s): fromfile:parlaiformat\n18:06:17 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:06:17 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run1/data_train-b.txt\n18:06:23 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7100 7.1e-10               .7387                 .6721   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8200            .6742              .7692   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6000 11.13 525.2 504.3       0          0 38.41  200 .7100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .6604 2.855e-06   240 230.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 734.8        .7064\u001b[0m\n18:06:23 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .7100 7.1e-10               .7387                 .6721   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8200            .6742              .7692   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6000 11.13 525.2 504.3       0          0 38.41  200 .7100   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .6604 2.855e-06   240 230.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 765.2 734.8        .7064\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:06:25.046706Z","iopub.execute_input":"2022-12-03T18:06:25.047104Z","iopub.status.idle":"2022-12-03T18:06:26.130271Z","shell.execute_reply.started":"2022-12-03T18:06:25.047066Z","shell.execute_reply":"2022-12-03T18:06:26.128815Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:06:26.132148Z","iopub.execute_input":"2022-12-03T18:06:26.132577Z","iopub.status.idle":"2022-12-03T18:07:23.428239Z","shell.execute_reply.started":"2022-12-03T18:06:26.132535Z","shell.execute_reply":"2022-12-03T18:07:23.427036Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"18:06:33 | building dictionary first...\n18:06:33 | No model with opt yet at: /tmp/model2(.opt)\n18:06:33 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:06:33 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:06:33 | Using CUDA\n18:06:33 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:06:33 | num words = 54944\n18:06:37 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:06:44 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:06:44 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:06:44 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:06:44 | Opt:\n18:06:44 |     activation: gelu\n18:06:44 |     adafactor_eps: '(1e-30, 0.001)'\n18:06:44 |     adam_eps: 1e-08\n18:06:44 |     add_p1_after_newln: False\n18:06:44 |     aggregate_micro: False\n18:06:44 |     allow_missing_init_opts: False\n18:06:44 |     attention_dropout: 0.1\n18:06:44 |     batchsize: 20\n18:06:44 |     betas: '(0.9, 0.999)'\n18:06:44 |     bpe_add_prefix_space: None\n18:06:44 |     bpe_debug: False\n18:06:44 |     bpe_dropout: None\n18:06:44 |     bpe_merge: None\n18:06:44 |     bpe_vocab: None\n18:06:44 |     candidates: inline\n18:06:44 |     cap_num_predictions: 100\n18:06:44 |     checkpoint_activations: False\n18:06:44 |     class_weights: None\n18:06:44 |     classes: \"['__notok__', '__ok__']\"\n18:06:44 |     classes_from_file: None\n18:06:44 |     data_parallel: True\n18:06:44 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:06:44 |     datatype: train\n18:06:44 |     delimiter: '\\n'\n18:06:44 |     dict_class: parlai.core.dict:DictionaryAgent\n18:06:44 |     dict_endtoken: __start__\n18:06:44 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:06:44 |     dict_include_test: False\n18:06:44 |     dict_include_valid: False\n18:06:44 |     dict_initpath: None\n18:06:44 |     dict_language: english\n18:06:44 |     dict_loaded: True\n18:06:44 |     dict_lower: True\n18:06:44 |     dict_max_ngram_size: -1\n18:06:44 |     dict_maxexs: -1\n18:06:44 |     dict_maxtokens: -1\n18:06:44 |     dict_minfreq: 0\n18:06:44 |     dict_nulltoken: __null__\n18:06:44 |     dict_starttoken: __start__\n18:06:44 |     dict_textfields: text,labels\n18:06:44 |     dict_tokenizer: bpe\n18:06:44 |     dict_unktoken: __unk__\n18:06:44 |     display_examples: False\n18:06:44 |     download_path: None\n18:06:44 |     dropout: 0.1\n18:06:44 |     dynamic_batching: None\n18:06:44 |     embedding_projection: random\n18:06:44 |     embedding_size: 768\n18:06:44 |     embedding_type: random\n18:06:44 |     embeddings_scale: False\n18:06:44 |     encode_candidate_vecs: True\n18:06:44 |     encode_candidate_vecs_batchsize: 256\n18:06:44 |     eval_batchsize: None\n18:06:44 |     eval_candidates: inline\n18:06:44 |     eval_dynamic_batching: None\n18:06:44 |     evaltask: None\n18:06:44 |     ffn_size: 3072\n18:06:44 |     final_extra_opt: \n18:06:44 |     fixed_candidate_vecs: reuse\n18:06:44 |     fixed_candidates_path: None\n18:06:44 |     force_fp16_tokens: False\n18:06:44 |     fp16: True\n18:06:44 |     fp16_impl: safe\n18:06:44 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt\n18:06:44 |     fromfile_datatype_extension: False\n18:06:44 |     gpu: -1\n18:06:44 |     gradient_clip: 0.1\n18:06:44 |     hide_labels: False\n18:06:44 |     history_add_global_end_token: None\n18:06:44 |     history_reversed: False\n18:06:44 |     history_size: 20\n18:06:44 |     ignore_bad_candidates: False\n18:06:44 |     ignore_labels: None\n18:06:44 |     image_cropsize: 224\n18:06:44 |     image_mode: raw\n18:06:44 |     image_size: 256\n18:06:44 |     inference: max\n18:06:44 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:06:44 |     init_opt: None\n18:06:44 |     interactive_candidates: fixed\n18:06:44 |     interactive_mode: False\n18:06:44 |     invsqrt_lr_decay_gamma: -1\n18:06:44 |     is_debug: False\n18:06:44 |     label_truncate: 72\n18:06:44 |     learn_embeddings: True\n18:06:44 |     learn_positional_embeddings: True\n18:06:44 |     learningrate: 5e-05\n18:06:44 |     load_from_checkpoint: False\n18:06:44 |     load_from_pretrained_ranker: True\n18:06:44 |     log_every_n_secs: 10.0\n18:06:44 |     log_every_n_steps: 50\n18:06:44 |     log_keep_fields: all\n18:06:44 |     loglevel: info\n18:06:44 |     lr_scheduler: reduceonplateau\n18:06:44 |     lr_scheduler_decay: 0.5\n18:06:44 |     lr_scheduler_patience: 3\n18:06:44 |     max_train_steps: -1\n18:06:44 |     max_train_time: 7200.0\n18:06:44 |     memory_attention: sqrt\n18:06:44 |     metrics: default\n18:06:44 |     model: transformer/classifier\n18:06:44 |     model_file: /tmp/model2\n18:06:44 |     model_parallel: False\n18:06:44 |     momentum: 0\n18:06:44 |     multitask_weights: [1]\n18:06:44 |     mutators: None\n18:06:44 |     n_decoder_layers: -1\n18:06:44 |     n_encoder_layers: -1\n18:06:44 |     n_heads: 12\n18:06:44 |     n_layers: 12\n18:06:44 |     n_positions: 1024\n18:06:44 |     n_segments: 2\n18:06:44 |     nesterov: True\n18:06:44 |     no_cuda: False\n18:06:44 |     normalize_sent_emb: False\n18:06:44 |     num_epochs: -1\n18:06:44 |     num_workers: 0\n18:06:44 |     nus: (0.7,)\n18:06:44 |     optimizer: adamax\n18:06:44 |     output_scaling: 0.06\n18:06:44 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n18:06:44 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:06:44 |     person_tokens: False\n18:06:44 |     print_scores: False\n18:06:44 |     rank_candidates: False\n18:06:44 |     rank_top_k: -1\n18:06:44 |     reduction_type: mean\n18:06:44 |     ref_class: None\n18:06:44 |     relu_dropout: 0.0\n18:06:44 |     repeat_blocking_heuristic: True\n18:06:44 |     return_cand_scores: False\n18:06:44 |     save_after_valid: True\n18:06:44 |     save_every_n_secs: -1\n18:06:44 |     save_format: conversations\n18:06:44 |     share_encoders: False\n18:06:44 |     share_word_embeddings: False\n18:06:44 |     short_final_eval: False\n18:06:44 |     special_tok_lst: None\n18:06:44 |     split_lines: False\n18:06:44 |     starttime: Dec03_18-06\n18:06:44 |     task: fromfile:parlaiformat\n18:06:44 |     tensorboard_log: False\n18:06:44 |     tensorboard_logdir: None\n18:06:44 |     text_truncate: 360\n18:06:44 |     threshold: 0.5\n18:06:44 |     topk: 5\n18:06:44 |     train_predict: False\n18:06:44 |     truncate: 1024\n18:06:44 |     update_classifier_head_only: False\n18:06:44 |     update_freq: 1\n18:06:44 |     use_memories: False\n18:06:44 |     use_reply: none\n18:06:44 |     validation_cutoff: 1.0\n18:06:44 |     validation_every_n_epochs: -1\n18:06:44 |     validation_every_n_secs: 20.0\n18:06:44 |     validation_every_n_steps: -1\n18:06:44 |     validation_max_exs: -1\n18:06:44 |     validation_metric: accuracy\n18:06:44 |     validation_metric_mode: max\n18:06:44 |     validation_patience: 30\n18:06:44 |     validation_share_agent: False\n18:06:44 |     variant: xlm\n18:06:44 |     verbose: False\n18:06:44 |     wandb_entity: None\n18:06:44 |     wandb_log: False\n18:06:44 |     wandb_name: None\n18:06:44 |     wandb_project: None\n18:06:44 |     warmup_rate: 0.0001\n18:06:44 |     warmup_updates: 1000\n18:06:44 |     weight_decay: None\n18:06:44 |     world_logs: \n18:06:44 |     wrap_memory_encoder: False\n18:06:44 | creating task(s): fromfile:parlaiformat\n18:06:44 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt\n18:06:44 | training...\n18:06:55 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6143 6.143e-10               .6943                 .5897   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8440            .4774              .6852   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .3663 11.23     1 264.6 548.9       0          0 41.49  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6143             32768  2.763    .1206 6.038 .6721 1.055e-06 120.8 250.5   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 385.4 799.4 2.079        .5900\n\n18:07:04 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9092 9.092e-10               .9132                 .8747   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9553            .9048              .9507   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8632 11.25     1   265  1029       0          0 77.68  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9092             32768  2.593    .1207     6 .6219 2.955e-06   120 466.1   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                   59  385 1495 3.893        .9090\n\n18:07:04 | creating task(s): fromfile:parlaiformat\n18:07:04 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:07:04 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt\n18:07:04 | running eval: valid\n18:07:05 | eval completed in 0.19s\n18:07:05 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1794       0          0 137.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5475 2.955e-06    72 827.8       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  228 2622            1\n\u001b[0m\n18:07:05 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n18:07:05 | saving best valid model: /tmp/model2\n18:07:05 | Saving dictionary to /tmp/model2.dict\n18:07:08 | task solved! stopping.\n18:07:08 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:07:08 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:07:08 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:07:08 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:07:08 | Using CUDA\n18:07:08 | loading dictionary from /tmp/model2.dict\n18:07:08 | num words = 54944\n18:07:14 | Loading existing model parameters from /tmp/model2\n18:07:18 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:07:20 | creating task(s): fromfile:parlaiformat\n18:07:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:07:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt\n18:07:20 | running eval: valid\n18:07:20 | eval completed in 0.26s\n18:07:20 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1384       0          0 105.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5475 2.955e-06    72 638.9       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  228 2023            1\n\u001b[0m\n18:07:20 | creating task(s): fromfile:parlaiformat\n18:07:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:07:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt\n18:07:20 | running eval: test\n18:07:20 | eval completed in 0.21s\n18:07:20 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1671       0          0 128.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5475 2.955e-06    72 771.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     59  228 2443            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:07:23.431283Z","iopub.execute_input":"2022-12-03T18:07:23.431692Z","iopub.status.idle":"2022-12-03T18:07:58.877905Z","shell.execute_reply.started":"2022-12-03T18:07:23.431648Z","shell.execute_reply":"2022-12-03T18:07:58.876682Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"18:07:37 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt)\u001b[0m\n18:07:37 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:07:37 | Using CUDA\n18:07:37 | loading dictionary from /tmp/model2.dict\n18:07:37 | num words = 54944\n18:07:42 | Loading existing model parameters from /tmp/model2\n18:07:49 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:07:50 | Opt:\n18:07:50 |     activation: gelu\n18:07:50 |     adafactor_eps: '[1e-30, 0.001]'\n18:07:50 |     adam_eps: 1e-08\n18:07:50 |     add_p1_after_newln: False\n18:07:50 |     aggregate_micro: False\n18:07:50 |     allow_missing_init_opts: False\n18:07:50 |     area_under_curve_class: None\n18:07:50 |     area_under_curve_digits: -1\n18:07:50 |     attention_dropout: 0.1\n18:07:50 |     batchsize: 40\n18:07:50 |     betas: '[0.9, 0.999]'\n18:07:50 |     bpe_add_prefix_space: None\n18:07:50 |     bpe_debug: False\n18:07:50 |     bpe_dropout: None\n18:07:50 |     bpe_merge: None\n18:07:50 |     bpe_vocab: None\n18:07:50 |     candidates: inline\n18:07:50 |     cap_num_predictions: 100\n18:07:50 |     checkpoint_activations: False\n18:07:50 |     class_weights: None\n18:07:50 |     classes: \"['__notok__', '__ok__']\"\n18:07:50 |     classes_from_file: None\n18:07:50 |     data_parallel: True\n18:07:50 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:07:50 |     datatype: train\n18:07:50 |     delimiter: '\\n'\n18:07:50 |     dict_class: parlai.core.dict:DictionaryAgent\n18:07:50 |     dict_endtoken: __start__\n18:07:50 |     dict_file: /tmp/model2.dict\n18:07:50 |     dict_include_test: False\n18:07:50 |     dict_include_valid: False\n18:07:50 |     dict_initpath: None\n18:07:50 |     dict_language: english\n18:07:50 |     dict_loaded: True\n18:07:50 |     dict_lower: True\n18:07:50 |     dict_max_ngram_size: -1\n18:07:50 |     dict_maxexs: -1\n18:07:50 |     dict_maxtokens: -1\n18:07:50 |     dict_minfreq: 0\n18:07:50 |     dict_nulltoken: __null__\n18:07:50 |     dict_starttoken: __start__\n18:07:50 |     dict_textfields: text,labels\n18:07:50 |     dict_tokenizer: bpe\n18:07:50 |     dict_unktoken: __unk__\n18:07:50 |     display_examples: False\n18:07:50 |     download_path: None\n18:07:50 |     dropout: 0.1\n18:07:50 |     dynamic_batching: None\n18:07:50 |     embedding_projection: random\n18:07:50 |     embedding_size: 768\n18:07:50 |     embedding_type: random\n18:07:50 |     embeddings_scale: False\n18:07:50 |     encode_candidate_vecs: True\n18:07:50 |     encode_candidate_vecs_batchsize: 256\n18:07:50 |     eval_batchsize: None\n18:07:50 |     eval_candidates: inline\n18:07:50 |     eval_dynamic_batching: None\n18:07:50 |     evaltask: None\n18:07:50 |     ffn_size: 3072\n18:07:50 |     final_extra_opt: \n18:07:50 |     fixed_candidate_vecs: reuse\n18:07:50 |     fixed_candidates_path: None\n18:07:50 |     force_fp16_tokens: True\n18:07:50 |     fp16: True\n18:07:50 |     fp16_impl: safe\n18:07:50 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-a.txt\n18:07:50 |     fromfile_datatype_extension: False\n18:07:50 |     gpu: -1\n18:07:50 |     gradient_clip: 0.1\n18:07:50 |     hide_labels: False\n18:07:50 |     history_add_global_end_token: None\n18:07:50 |     history_reversed: False\n18:07:50 |     history_size: 20\n18:07:50 |     ignore_bad_candidates: False\n18:07:50 |     ignore_labels: None\n18:07:50 |     image_cropsize: 224\n18:07:50 |     image_mode: raw\n18:07:50 |     image_size: 256\n18:07:50 |     inference: max\n18:07:50 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:07:50 |     init_opt: None\n18:07:50 |     interactive_candidates: fixed\n18:07:50 |     interactive_mode: False\n18:07:50 |     invsqrt_lr_decay_gamma: -1\n18:07:50 |     is_debug: False\n18:07:50 |     label_truncate: 72\n18:07:50 |     learn_embeddings: True\n18:07:50 |     learn_positional_embeddings: True\n18:07:50 |     learningrate: 5e-05\n18:07:50 |     load_from_pretrained_ranker: True\n18:07:50 |     log_every_n_secs: 10.0\n18:07:50 |     log_every_n_steps: 50\n18:07:50 |     log_keep_fields: all\n18:07:50 |     loglevel: info\n18:07:50 |     lr_scheduler: reduceonplateau\n18:07:50 |     lr_scheduler_decay: 0.5\n18:07:50 |     lr_scheduler_patience: 3\n18:07:50 |     max_train_steps: -1\n18:07:51 |     max_train_time: 7200.0\n18:07:51 |     memory_attention: sqrt\n18:07:51 |     metrics: default\n18:07:51 |     model: transformer/classifier\n18:07:51 |     model_file: /tmp/model2\n18:07:51 |     model_parallel: False\n18:07:51 |     momentum: 0\n18:07:51 |     multitask_weights: [1]\n18:07:51 |     mutators: None\n18:07:51 |     n_decoder_layers: -1\n18:07:51 |     n_encoder_layers: -1\n18:07:51 |     n_heads: 12\n18:07:51 |     n_layers: 12\n18:07:51 |     n_positions: 1024\n18:07:51 |     n_segments: 2\n18:07:51 |     nesterov: True\n18:07:51 |     no_cuda: False\n18:07:51 |     normalize_sent_emb: False\n18:07:51 |     num_epochs: -1\n18:07:51 |     num_examples: -1\n18:07:51 |     num_workers: 0\n18:07:51 |     nus: [0.7]\n18:07:51 |     optimizer: adamax\n18:07:51 |     output_scaling: 0.06\n18:07:51 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n18:07:51 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:07:51 |     person_tokens: False\n18:07:51 |     print_scores: False\n18:07:51 |     rank_candidates: False\n18:07:51 |     rank_top_k: -1\n18:07:51 |     reduction_type: mean\n18:07:51 |     ref_class: None\n18:07:51 |     relu_dropout: 0.0\n18:07:51 |     repeat_blocking_heuristic: True\n18:07:51 |     report_filename: \n18:07:51 |     return_cand_scores: False\n18:07:51 |     save_after_valid: True\n18:07:51 |     save_every_n_secs: -1\n18:07:51 |     save_format: conversations\n18:07:51 |     share_encoders: False\n18:07:51 |     share_word_embeddings: False\n18:07:51 |     short_final_eval: False\n18:07:51 |     special_tok_lst: None\n18:07:51 |     split_lines: False\n18:07:51 |     starttime: Dec03_18-06\n18:07:51 |     task: fromfile:parlaiformat\n18:07:51 |     tensorboard_log: False\n18:07:51 |     tensorboard_logdir: None\n18:07:51 |     text_truncate: 360\n18:07:51 |     threshold: 0.5\n18:07:51 |     topk: 5\n18:07:51 |     train_predict: False\n18:07:51 |     truncate: 1024\n18:07:51 |     update_classifier_head_only: False\n18:07:51 |     update_freq: 1\n18:07:51 |     use_memories: False\n18:07:51 |     use_reply: none\n18:07:51 |     validation_cutoff: 1.0\n18:07:51 |     validation_every_n_epochs: -1\n18:07:51 |     validation_every_n_secs: 20.0\n18:07:51 |     validation_every_n_steps: -1\n18:07:51 |     validation_max_exs: -1\n18:07:51 |     validation_metric: accuracy\n18:07:51 |     validation_metric_mode: max\n18:07:51 |     validation_patience: 30\n18:07:51 |     validation_share_agent: False\n18:07:51 |     variant: xlm\n18:07:51 |     verbose: False\n18:07:51 |     wandb_entity: None\n18:07:51 |     wandb_log: False\n18:07:51 |     wandb_name: None\n18:07:51 |     wandb_project: None\n18:07:51 |     warmup_rate: 0.0001\n18:07:51 |     warmup_updates: 1000\n18:07:51 |     weight_decay: None\n18:07:51 |     world_logs: \n18:07:51 |     wrap_memory_encoder: False\n18:07:51 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:07:51 | creating task(s): fromfile:parlaiformat\n18:07:51 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:07:51 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-a.txt\n18:07:57 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2950 2.95e-10               .3562                 .3277   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3900            .2210              .2469   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2000  11.7 547.8 519.5       0          0 37.93  200 .2950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7505 2.955e-06   240 227.6       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 787.8 747.1        .2886\u001b[0m\n18:07:57 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2950 2.95e-10               .3562                 .3277   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3900            .2210              .2469   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2000  11.7 547.8 519.5       0          0 37.93  200 .2950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7505 2.955e-06   240 227.6       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 787.8 747.1        .2886\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:07:58.880136Z","iopub.execute_input":"2022-12-03T18:07:58.880634Z","iopub.status.idle":"2022-12-03T18:08:25.183671Z","shell.execute_reply.started":"2022-12-03T18:07:58.880574Z","shell.execute_reply":"2022-12-03T18:08:25.182382Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"18:08:05 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_valid.txt)\u001b[0m\n18:08:05 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:08:05 | Using CUDA\n18:08:05 | loading dictionary from /tmp/model2.dict\n18:08:05 | num words = 54944\n18:08:10 | Loading existing model parameters from /tmp/model2\n18:08:16 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:08:17 | Opt:\n18:08:17 |     activation: gelu\n18:08:17 |     adafactor_eps: '[1e-30, 0.001]'\n18:08:17 |     adam_eps: 1e-08\n18:08:17 |     add_p1_after_newln: False\n18:08:17 |     aggregate_micro: False\n18:08:17 |     allow_missing_init_opts: False\n18:08:17 |     area_under_curve_class: None\n18:08:17 |     area_under_curve_digits: -1\n18:08:17 |     attention_dropout: 0.1\n18:08:17 |     batchsize: 40\n18:08:17 |     betas: '[0.9, 0.999]'\n18:08:17 |     bpe_add_prefix_space: None\n18:08:17 |     bpe_debug: False\n18:08:17 |     bpe_dropout: None\n18:08:17 |     bpe_merge: None\n18:08:17 |     bpe_vocab: None\n18:08:17 |     candidates: inline\n18:08:17 |     cap_num_predictions: 100\n18:08:17 |     checkpoint_activations: False\n18:08:17 |     class_weights: None\n18:08:17 |     classes: \"['__notok__', '__ok__']\"\n18:08:17 |     classes_from_file: None\n18:08:17 |     data_parallel: True\n18:08:17 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:08:17 |     datatype: train\n18:08:17 |     delimiter: '\\n'\n18:08:17 |     dict_class: parlai.core.dict:DictionaryAgent\n18:08:17 |     dict_endtoken: __start__\n18:08:17 |     dict_file: /tmp/model2.dict\n18:08:17 |     dict_include_test: False\n18:08:17 |     dict_include_valid: False\n18:08:17 |     dict_initpath: None\n18:08:17 |     dict_language: english\n18:08:17 |     dict_loaded: True\n18:08:17 |     dict_lower: True\n18:08:17 |     dict_max_ngram_size: -1\n18:08:17 |     dict_maxexs: -1\n18:08:17 |     dict_maxtokens: -1\n18:08:17 |     dict_minfreq: 0\n18:08:17 |     dict_nulltoken: __null__\n18:08:17 |     dict_starttoken: __start__\n18:08:17 |     dict_textfields: text,labels\n18:08:17 |     dict_tokenizer: bpe\n18:08:17 |     dict_unktoken: __unk__\n18:08:17 |     display_examples: False\n18:08:17 |     download_path: None\n18:08:17 |     dropout: 0.1\n18:08:17 |     dynamic_batching: None\n18:08:17 |     embedding_projection: random\n18:08:17 |     embedding_size: 768\n18:08:17 |     embedding_type: random\n18:08:17 |     embeddings_scale: False\n18:08:17 |     encode_candidate_vecs: True\n18:08:17 |     encode_candidate_vecs_batchsize: 256\n18:08:17 |     eval_batchsize: None\n18:08:17 |     eval_candidates: inline\n18:08:17 |     eval_dynamic_batching: None\n18:08:17 |     evaltask: None\n18:08:17 |     ffn_size: 3072\n18:08:17 |     final_extra_opt: \n18:08:17 |     fixed_candidate_vecs: reuse\n18:08:17 |     fixed_candidates_path: None\n18:08:17 |     force_fp16_tokens: True\n18:08:17 |     fp16: True\n18:08:17 |     fp16_impl: safe\n18:08:17 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-b.txt\n18:08:17 |     fromfile_datatype_extension: False\n18:08:17 |     gpu: -1\n18:08:17 |     gradient_clip: 0.1\n18:08:17 |     hide_labels: False\n18:08:17 |     history_add_global_end_token: None\n18:08:17 |     history_reversed: False\n18:08:17 |     history_size: 20\n18:08:17 |     ignore_bad_candidates: False\n18:08:17 |     ignore_labels: None\n18:08:17 |     image_cropsize: 224\n18:08:17 |     image_mode: raw\n18:08:17 |     image_size: 256\n18:08:17 |     inference: max\n18:08:17 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:08:17 |     init_opt: None\n18:08:17 |     interactive_candidates: fixed\n18:08:17 |     interactive_mode: False\n18:08:17 |     invsqrt_lr_decay_gamma: -1\n18:08:17 |     is_debug: False\n18:08:17 |     label_truncate: 72\n18:08:17 |     learn_embeddings: True\n18:08:17 |     learn_positional_embeddings: True\n18:08:17 |     learningrate: 5e-05\n18:08:17 |     load_from_pretrained_ranker: True\n18:08:17 |     log_every_n_secs: 10.0\n18:08:17 |     log_every_n_steps: 50\n18:08:17 |     log_keep_fields: all\n18:08:17 |     loglevel: info\n18:08:17 |     lr_scheduler: reduceonplateau\n18:08:17 |     lr_scheduler_decay: 0.5\n18:08:17 |     lr_scheduler_patience: 3\n18:08:17 |     max_train_steps: -1\n18:08:17 |     max_train_time: 7200.0\n18:08:17 |     memory_attention: sqrt\n18:08:17 |     metrics: default\n18:08:17 |     model: transformer/classifier\n18:08:17 |     model_file: /tmp/model2\n18:08:17 |     model_parallel: False\n18:08:17 |     momentum: 0\n18:08:17 |     multitask_weights: [1]\n18:08:17 |     mutators: None\n18:08:17 |     n_decoder_layers: -1\n18:08:17 |     n_encoder_layers: -1\n18:08:17 |     n_heads: 12\n18:08:17 |     n_layers: 12\n18:08:17 |     n_positions: 1024\n18:08:17 |     n_segments: 2\n18:08:17 |     nesterov: True\n18:08:17 |     no_cuda: False\n18:08:17 |     normalize_sent_emb: False\n18:08:17 |     num_epochs: -1\n18:08:17 |     num_examples: -1\n18:08:17 |     num_workers: 0\n18:08:17 |     nus: [0.7]\n18:08:17 |     optimizer: adamax\n18:08:17 |     output_scaling: 0.06\n18:08:17 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n18:08:17 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:08:17 |     person_tokens: False\n18:08:17 |     print_scores: False\n18:08:17 |     rank_candidates: False\n18:08:17 |     rank_top_k: -1\n18:08:17 |     reduction_type: mean\n18:08:17 |     ref_class: None\n18:08:17 |     relu_dropout: 0.0\n18:08:17 |     repeat_blocking_heuristic: True\n18:08:17 |     report_filename: \n18:08:17 |     return_cand_scores: False\n18:08:17 |     save_after_valid: True\n18:08:17 |     save_every_n_secs: -1\n18:08:17 |     save_format: conversations\n18:08:17 |     share_encoders: False\n18:08:17 |     share_word_embeddings: False\n18:08:17 |     short_final_eval: False\n18:08:17 |     special_tok_lst: None\n18:08:17 |     split_lines: False\n18:08:17 |     starttime: Dec03_18-06\n18:08:17 |     task: fromfile:parlaiformat\n18:08:17 |     tensorboard_log: False\n18:08:17 |     tensorboard_logdir: None\n18:08:17 |     text_truncate: 360\n18:08:17 |     threshold: 0.5\n18:08:17 |     topk: 5\n18:08:17 |     train_predict: False\n18:08:17 |     truncate: 1024\n18:08:17 |     update_classifier_head_only: False\n18:08:17 |     update_freq: 1\n18:08:17 |     use_memories: False\n18:08:17 |     use_reply: none\n18:08:17 |     validation_cutoff: 1.0\n18:08:17 |     validation_every_n_epochs: -1\n18:08:17 |     validation_every_n_secs: 20.0\n18:08:17 |     validation_every_n_steps: -1\n18:08:17 |     validation_max_exs: -1\n18:08:17 |     validation_metric: accuracy\n18:08:17 |     validation_metric_mode: max\n18:08:17 |     validation_patience: 30\n18:08:17 |     validation_share_agent: False\n18:08:17 |     variant: xlm\n18:08:17 |     verbose: False\n18:08:17 |     wandb_entity: None\n18:08:17 |     wandb_log: False\n18:08:17 |     wandb_name: None\n18:08:17 |     wandb_project: None\n18:08:17 |     warmup_rate: 0.0001\n18:08:17 |     warmup_updates: 1000\n18:08:17 |     weight_decay: None\n18:08:17 |     world_logs: \n18:08:17 |     wrap_memory_encoder: False\n18:08:17 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:08:17 | creating task(s): fromfile:parlaiformat\n18:08:17 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:08:17 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run2/data_train-b.txt\n18:08:23 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7050 7.05e-10               .7306                 .6723   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8000            .6740              .7531   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6100  11.7 547.8 491.4       0          0 35.88  200 .7050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6447 2.955e-06   240 215.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 787.8 706.6        .7023\u001b[0m\n18:08:23 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7050 7.05e-10               .7306                 .6723   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8000            .6740              .7531   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .6100  11.7 547.8 491.4       0          0 35.88  200 .7050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6447 2.955e-06   240 215.3       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     59 787.8 706.6        .7023\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:08:25.185900Z","iopub.execute_input":"2022-12-03T18:08:25.186334Z","iopub.status.idle":"2022-12-03T18:08:26.283588Z","shell.execute_reply.started":"2022-12-03T18:08:25.186293Z","shell.execute_reply":"2022-12-03T18:08:26.282296Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:08:26.285813Z","iopub.execute_input":"2022-12-03T18:08:26.286236Z","iopub.status.idle":"2022-12-03T18:09:43.493475Z","shell.execute_reply.started":"2022-12-03T18:08:26.286196Z","shell.execute_reply":"2022-12-03T18:09:43.492273Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"18:08:33 | building dictionary first...\n18:08:33 | No model with opt yet at: /tmp/model3(.opt)\n18:08:33 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:08:33 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:08:33 | Using CUDA\n18:08:33 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:08:33 | num words = 54944\n18:08:37 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:08:44 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:08:44 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:08:44 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:08:44 | Opt:\n18:08:44 |     activation: gelu\n18:08:44 |     adafactor_eps: '(1e-30, 0.001)'\n18:08:44 |     adam_eps: 1e-08\n18:08:44 |     add_p1_after_newln: False\n18:08:44 |     aggregate_micro: False\n18:08:44 |     allow_missing_init_opts: False\n18:08:44 |     attention_dropout: 0.1\n18:08:44 |     batchsize: 20\n18:08:44 |     betas: '(0.9, 0.999)'\n18:08:44 |     bpe_add_prefix_space: None\n18:08:44 |     bpe_debug: False\n18:08:44 |     bpe_dropout: None\n18:08:44 |     bpe_merge: None\n18:08:44 |     bpe_vocab: None\n18:08:44 |     candidates: inline\n18:08:44 |     cap_num_predictions: 100\n18:08:44 |     checkpoint_activations: False\n18:08:44 |     class_weights: None\n18:08:44 |     classes: \"['__notok__', '__ok__']\"\n18:08:44 |     classes_from_file: None\n18:08:44 |     data_parallel: True\n18:08:44 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:08:44 |     datatype: train\n18:08:44 |     delimiter: '\\n'\n18:08:44 |     dict_class: parlai.core.dict:DictionaryAgent\n18:08:44 |     dict_endtoken: __start__\n18:08:44 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:08:44 |     dict_include_test: False\n18:08:44 |     dict_include_valid: False\n18:08:44 |     dict_initpath: None\n18:08:44 |     dict_language: english\n18:08:44 |     dict_loaded: True\n18:08:44 |     dict_lower: True\n18:08:44 |     dict_max_ngram_size: -1\n18:08:44 |     dict_maxexs: -1\n18:08:44 |     dict_maxtokens: -1\n18:08:44 |     dict_minfreq: 0\n18:08:44 |     dict_nulltoken: __null__\n18:08:44 |     dict_starttoken: __start__\n18:08:44 |     dict_textfields: text,labels\n18:08:44 |     dict_tokenizer: bpe\n18:08:44 |     dict_unktoken: __unk__\n18:08:44 |     display_examples: False\n18:08:44 |     download_path: None\n18:08:44 |     dropout: 0.1\n18:08:44 |     dynamic_batching: None\n18:08:44 |     embedding_projection: random\n18:08:44 |     embedding_size: 768\n18:08:44 |     embedding_type: random\n18:08:44 |     embeddings_scale: False\n18:08:44 |     encode_candidate_vecs: True\n18:08:44 |     encode_candidate_vecs_batchsize: 256\n18:08:44 |     eval_batchsize: None\n18:08:44 |     eval_candidates: inline\n18:08:44 |     eval_dynamic_batching: None\n18:08:44 |     evaltask: None\n18:08:44 |     ffn_size: 3072\n18:08:44 |     final_extra_opt: \n18:08:44 |     fixed_candidate_vecs: reuse\n18:08:44 |     fixed_candidates_path: None\n18:08:44 |     force_fp16_tokens: False\n18:08:44 |     fp16: True\n18:08:44 |     fp16_impl: safe\n18:08:44 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt\n18:08:44 |     fromfile_datatype_extension: False\n18:08:44 |     gpu: -1\n18:08:44 |     gradient_clip: 0.1\n18:08:44 |     hide_labels: False\n18:08:44 |     history_add_global_end_token: None\n18:08:44 |     history_reversed: False\n18:08:44 |     history_size: 20\n18:08:44 |     ignore_bad_candidates: False\n18:08:44 |     ignore_labels: None\n18:08:44 |     image_cropsize: 224\n18:08:44 |     image_mode: raw\n18:08:44 |     image_size: 256\n18:08:44 |     inference: max\n18:08:44 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:08:44 |     init_opt: None\n18:08:44 |     interactive_candidates: fixed\n18:08:44 |     interactive_mode: False\n18:08:44 |     invsqrt_lr_decay_gamma: -1\n18:08:44 |     is_debug: False\n18:08:44 |     label_truncate: 72\n18:08:44 |     learn_embeddings: True\n18:08:44 |     learn_positional_embeddings: True\n18:08:44 |     learningrate: 5e-05\n18:08:44 |     load_from_checkpoint: False\n18:08:44 |     load_from_pretrained_ranker: True\n18:08:44 |     log_every_n_secs: 10.0\n18:08:44 |     log_every_n_steps: 50\n18:08:44 |     log_keep_fields: all\n18:08:44 |     loglevel: info\n18:08:44 |     lr_scheduler: reduceonplateau\n18:08:44 |     lr_scheduler_decay: 0.5\n18:08:44 |     lr_scheduler_patience: 3\n18:08:44 |     max_train_steps: -1\n18:08:44 |     max_train_time: 7200.0\n18:08:44 |     memory_attention: sqrt\n18:08:44 |     metrics: default\n18:08:44 |     model: transformer/classifier\n18:08:44 |     model_file: /tmp/model3\n18:08:44 |     model_parallel: False\n18:08:44 |     momentum: 0\n18:08:44 |     multitask_weights: [1]\n18:08:44 |     mutators: None\n18:08:44 |     n_decoder_layers: -1\n18:08:44 |     n_encoder_layers: -1\n18:08:44 |     n_heads: 12\n18:08:44 |     n_layers: 12\n18:08:44 |     n_positions: 1024\n18:08:44 |     n_segments: 2\n18:08:44 |     nesterov: True\n18:08:44 |     no_cuda: False\n18:08:44 |     normalize_sent_emb: False\n18:08:44 |     num_epochs: -1\n18:08:44 |     num_workers: 0\n18:08:44 |     nus: (0.7,)\n18:08:44 |     optimizer: adamax\n18:08:44 |     output_scaling: 0.06\n18:08:44 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n18:08:44 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:08:44 |     person_tokens: False\n18:08:44 |     print_scores: False\n18:08:44 |     rank_candidates: False\n18:08:44 |     rank_top_k: -1\n18:08:44 |     reduction_type: mean\n18:08:44 |     ref_class: None\n18:08:44 |     relu_dropout: 0.0\n18:08:44 |     repeat_blocking_heuristic: True\n18:08:44 |     return_cand_scores: False\n18:08:44 |     save_after_valid: True\n18:08:44 |     save_every_n_secs: -1\n18:08:44 |     save_format: conversations\n18:08:44 |     share_encoders: False\n18:08:44 |     share_word_embeddings: False\n18:08:44 |     short_final_eval: False\n18:08:44 |     special_tok_lst: None\n18:08:44 |     split_lines: False\n18:08:44 |     starttime: Dec03_18-08\n18:08:44 |     task: fromfile:parlaiformat\n18:08:44 |     tensorboard_log: False\n18:08:44 |     tensorboard_logdir: None\n18:08:44 |     text_truncate: 360\n18:08:44 |     threshold: 0.5\n18:08:44 |     topk: 5\n18:08:44 |     train_predict: False\n18:08:44 |     truncate: 1024\n18:08:44 |     update_classifier_head_only: False\n18:08:44 |     update_freq: 1\n18:08:44 |     use_memories: False\n18:08:44 |     use_reply: none\n18:08:44 |     validation_cutoff: 1.0\n18:08:44 |     validation_every_n_epochs: -1\n18:08:44 |     validation_every_n_secs: 20.0\n18:08:44 |     validation_every_n_steps: -1\n18:08:44 |     validation_max_exs: -1\n18:08:44 |     validation_metric: accuracy\n18:08:44 |     validation_metric_mode: max\n18:08:44 |     validation_patience: 30\n18:08:44 |     validation_share_agent: False\n18:08:44 |     variant: xlm\n18:08:44 |     verbose: False\n18:08:44 |     wandb_entity: None\n18:08:44 |     wandb_log: False\n18:08:44 |     wandb_name: None\n18:08:44 |     wandb_project: None\n18:08:44 |     warmup_rate: 0.0001\n18:08:44 |     warmup_updates: 1000\n18:08:44 |     weight_decay: None\n18:08:44 |     world_logs: \n18:08:44 |     wrap_memory_encoder: False\n18:08:44 | creating task(s): fromfile:parlaiformat\n18:08:44 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt\n18:08:44 | training...\n18:08:55 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4475 4.475e-10               .5143                 .4517   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5969            .3594              .4397   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .3039 11.69     1 273.9 545.4       0          0 39.83  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4475             32768  2.866    .1206  5.98 .6987 1.005e-06 119.6 238.2   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 393.4 783.6 1.996        .4353\n\n18:09:04 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7079 7.079e-10               .7363                 .6769   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8073            .6726              .7550   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6064  11.9     1   278  1080       0          0 77.71  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7079             32768   2.83    .1207 6.011 .6460 2.905e-06 120.2 467.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 398.2 1547 3.895        .7048\n\n18:09:04 | creating task(s): fromfile:parlaiformat\n18:09:04 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:09:04 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt\n18:09:04 | running eval: valid\n18:09:05 | eval completed in 0.20s\n18:09:05 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9600                 .9231   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9565                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 11.71 164.5  1813       0          0 132.2   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5746 2.905e-06    72 793.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     58 236.5 2607        .9583\n\u001b[0m\n18:09:05 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n18:09:05 | saving best valid model: /tmp/model3\n18:09:05 | Saving dictionary to /tmp/model3.dict\n18:09:08 | saving model checkpoint: /tmp/model3.checkpoint\n18:09:08 | Saving dictionary to /tmp/model3.checkpoint.dict\n18:09:25 | time:41s total_exs:1840 total_steps:92 epochs:76.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9882 9.882e-10               .9884                 .9770   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9881                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9765 11.76     1 275.1 934.7       0          0 67.94  680   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9882             32768  3.207    .1207     6 .5016 4.605e-06   120 407.7   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   92 395.1 1342 3.405        .9882\n\n18:09:28 | time:44s total_exs:2080 total_steps:104 epochs:86.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.89     1 277.8  1100       0          0  79.2  240   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.662    .1207 6.008 .3674 5.204e-06 120.2 475.9   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps  ups  weighted_f1  \n         0          0                  104  398 1576 3.99            1\n\n18:09:28 | running eval: valid\n18:09:28 | eval completed in 0.19s\n18:09:28 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1864       0          0 135.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .2853 5.204e-06    72 815.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 236.5 2680            1\n\u001b[0m\n18:09:28 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n18:09:28 | saving best valid model: /tmp/model3\n18:09:33 | task solved! stopping.\n18:09:33 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:09:33 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:09:33 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:09:33 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:09:33 | Using CUDA\n18:09:33 | loading dictionary from /tmp/model3.dict\n18:09:33 | num words = 54944\n18:09:38 | Loading existing model parameters from /tmp/model3\n18:09:40 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:09:41 | creating task(s): fromfile:parlaiformat\n18:09:41 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:09:41 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt\n18:09:41 | running eval: valid\n18:09:41 | eval completed in 0.22s\n18:09:41 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1714       0          0   125   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2853 5.204e-06    72 750.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 236.5 2465            1\n\u001b[0m\n18:09:41 | creating task(s): fromfile:parlaiformat\n18:09:41 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:09:41 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt\n18:09:41 | running eval: test\n18:09:41 | eval completed in 0.20s\n18:09:41 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1807       0          0 131.8   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2853 5.204e-06    72 790.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    104 236.5 2598            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:09:43.495445Z","iopub.execute_input":"2022-12-03T18:09:43.495838Z","iopub.status.idle":"2022-12-03T18:10:11.036801Z","shell.execute_reply.started":"2022-12-03T18:09:43.495807Z","shell.execute_reply":"2022-12-03T18:10:11.035552Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"18:09:51 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt)\u001b[0m\n18:09:51 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:09:51 | Using CUDA\n18:09:51 | loading dictionary from /tmp/model3.dict\n18:09:51 | num words = 54944\n18:09:55 | Loading existing model parameters from /tmp/model3\n18:10:02 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:10:03 | Opt:\n18:10:03 |     activation: gelu\n18:10:03 |     adafactor_eps: '[1e-30, 0.001]'\n18:10:03 |     adam_eps: 1e-08\n18:10:03 |     add_p1_after_newln: False\n18:10:03 |     aggregate_micro: False\n18:10:03 |     allow_missing_init_opts: False\n18:10:03 |     area_under_curve_class: None\n18:10:03 |     area_under_curve_digits: -1\n18:10:03 |     attention_dropout: 0.1\n18:10:03 |     batchsize: 40\n18:10:03 |     betas: '[0.9, 0.999]'\n18:10:03 |     bpe_add_prefix_space: None\n18:10:03 |     bpe_debug: False\n18:10:03 |     bpe_dropout: None\n18:10:03 |     bpe_merge: None\n18:10:03 |     bpe_vocab: None\n18:10:03 |     candidates: inline\n18:10:03 |     cap_num_predictions: 100\n18:10:03 |     checkpoint_activations: False\n18:10:03 |     class_weights: None\n18:10:03 |     classes: \"['__notok__', '__ok__']\"\n18:10:03 |     classes_from_file: None\n18:10:03 |     data_parallel: True\n18:10:03 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:10:03 |     datatype: train\n18:10:03 |     delimiter: '\\n'\n18:10:03 |     dict_class: parlai.core.dict:DictionaryAgent\n18:10:03 |     dict_endtoken: __start__\n18:10:03 |     dict_file: /tmp/model3.dict\n18:10:03 |     dict_include_test: False\n18:10:03 |     dict_include_valid: False\n18:10:03 |     dict_initpath: None\n18:10:03 |     dict_language: english\n18:10:03 |     dict_loaded: True\n18:10:03 |     dict_lower: True\n18:10:03 |     dict_max_ngram_size: -1\n18:10:03 |     dict_maxexs: -1\n18:10:03 |     dict_maxtokens: -1\n18:10:03 |     dict_minfreq: 0\n18:10:03 |     dict_nulltoken: __null__\n18:10:03 |     dict_starttoken: __start__\n18:10:03 |     dict_textfields: text,labels\n18:10:03 |     dict_tokenizer: bpe\n18:10:03 |     dict_unktoken: __unk__\n18:10:03 |     display_examples: False\n18:10:03 |     download_path: None\n18:10:03 |     dropout: 0.1\n18:10:03 |     dynamic_batching: None\n18:10:03 |     embedding_projection: random\n18:10:03 |     embedding_size: 768\n18:10:03 |     embedding_type: random\n18:10:03 |     embeddings_scale: False\n18:10:03 |     encode_candidate_vecs: True\n18:10:03 |     encode_candidate_vecs_batchsize: 256\n18:10:03 |     eval_batchsize: None\n18:10:03 |     eval_candidates: inline\n18:10:03 |     eval_dynamic_batching: None\n18:10:03 |     evaltask: None\n18:10:03 |     ffn_size: 3072\n18:10:03 |     final_extra_opt: \n18:10:03 |     fixed_candidate_vecs: reuse\n18:10:03 |     fixed_candidates_path: None\n18:10:03 |     force_fp16_tokens: True\n18:10:03 |     fp16: True\n18:10:03 |     fp16_impl: safe\n18:10:03 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-a.txt\n18:10:03 |     fromfile_datatype_extension: False\n18:10:03 |     gpu: -1\n18:10:03 |     gradient_clip: 0.1\n18:10:03 |     hide_labels: False\n18:10:03 |     history_add_global_end_token: None\n18:10:03 |     history_reversed: False\n18:10:03 |     history_size: 20\n18:10:03 |     ignore_bad_candidates: False\n18:10:03 |     ignore_labels: None\n18:10:03 |     image_cropsize: 224\n18:10:03 |     image_mode: raw\n18:10:03 |     image_size: 256\n18:10:03 |     inference: max\n18:10:03 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:10:03 |     init_opt: None\n18:10:03 |     interactive_candidates: fixed\n18:10:03 |     interactive_mode: False\n18:10:03 |     invsqrt_lr_decay_gamma: -1\n18:10:03 |     is_debug: False\n18:10:03 |     label_truncate: 72\n18:10:03 |     learn_embeddings: True\n18:10:03 |     learn_positional_embeddings: True\n18:10:03 |     learningrate: 5e-05\n18:10:03 |     load_from_pretrained_ranker: True\n18:10:03 |     log_every_n_secs: 10.0\n18:10:03 |     log_every_n_steps: 50\n18:10:03 |     log_keep_fields: all\n18:10:03 |     loglevel: info\n18:10:03 |     lr_scheduler: reduceonplateau\n18:10:03 |     lr_scheduler_decay: 0.5\n18:10:03 |     lr_scheduler_patience: 3\n18:10:03 |     max_train_steps: -1\n18:10:03 |     max_train_time: 7200.0\n18:10:03 |     memory_attention: sqrt\n18:10:03 |     metrics: default\n18:10:03 |     model: transformer/classifier\n18:10:03 |     model_file: /tmp/model3\n18:10:03 |     model_parallel: False\n18:10:03 |     momentum: 0\n18:10:03 |     multitask_weights: [1]\n18:10:03 |     mutators: None\n18:10:03 |     n_decoder_layers: -1\n18:10:03 |     n_encoder_layers: -1\n18:10:03 |     n_heads: 12\n18:10:03 |     n_layers: 12\n18:10:03 |     n_positions: 1024\n18:10:03 |     n_segments: 2\n18:10:03 |     nesterov: True\n18:10:03 |     no_cuda: False\n18:10:03 |     normalize_sent_emb: False\n18:10:03 |     num_epochs: -1\n18:10:03 |     num_examples: -1\n18:10:03 |     num_workers: 0\n18:10:03 |     nus: [0.7]\n18:10:03 |     optimizer: adamax\n18:10:03 |     output_scaling: 0.06\n18:10:03 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n18:10:03 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:10:03 |     person_tokens: False\n18:10:03 |     print_scores: False\n18:10:03 |     rank_candidates: False\n18:10:03 |     rank_top_k: -1\n18:10:03 |     reduction_type: mean\n18:10:03 |     ref_class: None\n18:10:03 |     relu_dropout: 0.0\n18:10:03 |     repeat_blocking_heuristic: True\n18:10:03 |     report_filename: \n18:10:03 |     return_cand_scores: False\n18:10:03 |     save_after_valid: True\n18:10:03 |     save_every_n_secs: -1\n18:10:03 |     save_format: conversations\n18:10:03 |     share_encoders: False\n18:10:03 |     share_word_embeddings: False\n18:10:03 |     short_final_eval: False\n18:10:03 |     special_tok_lst: None\n18:10:03 |     split_lines: False\n18:10:03 |     starttime: Dec03_18-08\n18:10:03 |     task: fromfile:parlaiformat\n18:10:03 |     tensorboard_log: False\n18:10:03 |     tensorboard_logdir: None\n18:10:03 |     text_truncate: 360\n18:10:03 |     threshold: 0.5\n18:10:03 |     topk: 5\n18:10:03 |     train_predict: False\n18:10:03 |     truncate: 1024\n18:10:03 |     update_classifier_head_only: False\n18:10:03 |     update_freq: 1\n18:10:03 |     use_memories: False\n18:10:03 |     use_reply: none\n18:10:03 |     validation_cutoff: 1.0\n18:10:03 |     validation_every_n_epochs: -1\n18:10:03 |     validation_every_n_secs: 20.0\n18:10:03 |     validation_every_n_steps: -1\n18:10:03 |     validation_max_exs: -1\n18:10:03 |     validation_metric: accuracy\n18:10:03 |     validation_metric_mode: max\n18:10:03 |     validation_patience: 30\n18:10:03 |     validation_share_agent: False\n18:10:03 |     variant: xlm\n18:10:03 |     verbose: False\n18:10:03 |     wandb_entity: None\n18:10:03 |     wandb_log: False\n18:10:03 |     wandb_name: None\n18:10:03 |     wandb_project: None\n18:10:03 |     warmup_rate: 0.0001\n18:10:03 |     warmup_updates: 1000\n18:10:03 |     weight_decay: None\n18:10:03 |     world_logs: \n18:10:03 |     wrap_memory_encoder: False\n18:10:03 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:10:03 | creating task(s): fromfile:parlaiformat\n18:10:03 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:10:03 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-a.txt\n18:10:09 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1250 1.25e-10               .1206                 .1224   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1188            .1294              .1275   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1313 11.47   539 498.3       0          0 36.98  200 .1250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .9312 5.204e-06 240.4 222.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 779.4 720.5        .1249\u001b[0m\n18:10:09 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1250 1.25e-10               .1206                 .1224   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1188            .1294              .1275   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1313 11.47   539 498.3       0          0 36.98  200 .1250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .9312 5.204e-06 240.4 222.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 779.4 720.5        .1249\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:10:11.039631Z","iopub.execute_input":"2022-12-03T18:10:11.040042Z","iopub.status.idle":"2022-12-03T18:10:37.512413Z","shell.execute_reply.started":"2022-12-03T18:10:11.040000Z","shell.execute_reply":"2022-12-03T18:10:37.511070Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"18:10:18 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_valid.txt)\u001b[0m\n18:10:18 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:10:18 | Using CUDA\n18:10:18 | loading dictionary from /tmp/model3.dict\n18:10:18 | num words = 54944\n18:10:23 | Loading existing model parameters from /tmp/model3\n18:10:28 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:10:29 | Opt:\n18:10:29 |     activation: gelu\n18:10:29 |     adafactor_eps: '[1e-30, 0.001]'\n18:10:29 |     adam_eps: 1e-08\n18:10:29 |     add_p1_after_newln: False\n18:10:29 |     aggregate_micro: False\n18:10:29 |     allow_missing_init_opts: False\n18:10:29 |     area_under_curve_class: None\n18:10:29 |     area_under_curve_digits: -1\n18:10:29 |     attention_dropout: 0.1\n18:10:29 |     batchsize: 40\n18:10:29 |     betas: '[0.9, 0.999]'\n18:10:29 |     bpe_add_prefix_space: None\n18:10:29 |     bpe_debug: False\n18:10:29 |     bpe_dropout: None\n18:10:29 |     bpe_merge: None\n18:10:29 |     bpe_vocab: None\n18:10:29 |     candidates: inline\n18:10:29 |     cap_num_predictions: 100\n18:10:29 |     checkpoint_activations: False\n18:10:29 |     class_weights: None\n18:10:29 |     classes: \"['__notok__', '__ok__']\"\n18:10:29 |     classes_from_file: None\n18:10:29 |     data_parallel: True\n18:10:29 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:10:29 |     datatype: train\n18:10:29 |     delimiter: '\\n'\n18:10:29 |     dict_class: parlai.core.dict:DictionaryAgent\n18:10:29 |     dict_endtoken: __start__\n18:10:29 |     dict_file: /tmp/model3.dict\n18:10:29 |     dict_include_test: False\n18:10:29 |     dict_include_valid: False\n18:10:29 |     dict_initpath: None\n18:10:29 |     dict_language: english\n18:10:29 |     dict_loaded: True\n18:10:29 |     dict_lower: True\n18:10:29 |     dict_max_ngram_size: -1\n18:10:29 |     dict_maxexs: -1\n18:10:29 |     dict_maxtokens: -1\n18:10:29 |     dict_minfreq: 0\n18:10:29 |     dict_nulltoken: __null__\n18:10:29 |     dict_starttoken: __start__\n18:10:29 |     dict_textfields: text,labels\n18:10:29 |     dict_tokenizer: bpe\n18:10:29 |     dict_unktoken: __unk__\n18:10:29 |     display_examples: False\n18:10:29 |     download_path: None\n18:10:29 |     dropout: 0.1\n18:10:29 |     dynamic_batching: None\n18:10:29 |     embedding_projection: random\n18:10:29 |     embedding_size: 768\n18:10:29 |     embedding_type: random\n18:10:29 |     embeddings_scale: False\n18:10:29 |     encode_candidate_vecs: True\n18:10:29 |     encode_candidate_vecs_batchsize: 256\n18:10:29 |     eval_batchsize: None\n18:10:29 |     eval_candidates: inline\n18:10:29 |     eval_dynamic_batching: None\n18:10:29 |     evaltask: None\n18:10:29 |     ffn_size: 3072\n18:10:29 |     final_extra_opt: \n18:10:29 |     fixed_candidate_vecs: reuse\n18:10:29 |     fixed_candidates_path: None\n18:10:29 |     force_fp16_tokens: True\n18:10:29 |     fp16: True\n18:10:29 |     fp16_impl: safe\n18:10:29 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-b.txt\n18:10:29 |     fromfile_datatype_extension: False\n18:10:29 |     gpu: -1\n18:10:29 |     gradient_clip: 0.1\n18:10:29 |     hide_labels: False\n18:10:29 |     history_add_global_end_token: None\n18:10:29 |     history_reversed: False\n18:10:29 |     history_size: 20\n18:10:29 |     ignore_bad_candidates: False\n18:10:29 |     ignore_labels: None\n18:10:29 |     image_cropsize: 224\n18:10:29 |     image_mode: raw\n18:10:29 |     image_size: 256\n18:10:29 |     inference: max\n18:10:29 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:10:29 |     init_opt: None\n18:10:29 |     interactive_candidates: fixed\n18:10:29 |     interactive_mode: False\n18:10:29 |     invsqrt_lr_decay_gamma: -1\n18:10:29 |     is_debug: False\n18:10:29 |     label_truncate: 72\n18:10:29 |     learn_embeddings: True\n18:10:29 |     learn_positional_embeddings: True\n18:10:29 |     learningrate: 5e-05\n18:10:29 |     load_from_pretrained_ranker: True\n18:10:29 |     log_every_n_secs: 10.0\n18:10:29 |     log_every_n_steps: 50\n18:10:29 |     log_keep_fields: all\n18:10:29 |     loglevel: info\n18:10:29 |     lr_scheduler: reduceonplateau\n18:10:29 |     lr_scheduler_decay: 0.5\n18:10:29 |     lr_scheduler_patience: 3\n18:10:29 |     max_train_steps: -1\n18:10:29 |     max_train_time: 7200.0\n18:10:29 |     memory_attention: sqrt\n18:10:29 |     metrics: default\n18:10:29 |     model: transformer/classifier\n18:10:29 |     model_file: /tmp/model3\n18:10:29 |     model_parallel: False\n18:10:29 |     momentum: 0\n18:10:29 |     multitask_weights: [1]\n18:10:29 |     mutators: None\n18:10:29 |     n_decoder_layers: -1\n18:10:29 |     n_encoder_layers: -1\n18:10:29 |     n_heads: 12\n18:10:29 |     n_layers: 12\n18:10:29 |     n_positions: 1024\n18:10:29 |     n_segments: 2\n18:10:29 |     nesterov: True\n18:10:29 |     no_cuda: False\n18:10:29 |     normalize_sent_emb: False\n18:10:29 |     num_epochs: -1\n18:10:29 |     num_examples: -1\n18:10:29 |     num_workers: 0\n18:10:29 |     nus: [0.7]\n18:10:29 |     optimizer: adamax\n18:10:29 |     output_scaling: 0.06\n18:10:29 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n18:10:29 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:10:29 |     person_tokens: False\n18:10:29 |     print_scores: False\n18:10:29 |     rank_candidates: False\n18:10:29 |     rank_top_k: -1\n18:10:29 |     reduction_type: mean\n18:10:29 |     ref_class: None\n18:10:29 |     relu_dropout: 0.0\n18:10:29 |     repeat_blocking_heuristic: True\n18:10:29 |     report_filename: \n18:10:29 |     return_cand_scores: False\n18:10:29 |     save_after_valid: True\n18:10:29 |     save_every_n_secs: -1\n18:10:29 |     save_format: conversations\n18:10:29 |     share_encoders: False\n18:10:29 |     share_word_embeddings: False\n18:10:29 |     short_final_eval: False\n18:10:29 |     special_tok_lst: None\n18:10:29 |     split_lines: False\n18:10:29 |     starttime: Dec03_18-08\n18:10:29 |     task: fromfile:parlaiformat\n18:10:29 |     tensorboard_log: False\n18:10:29 |     tensorboard_logdir: None\n18:10:29 |     text_truncate: 360\n18:10:29 |     threshold: 0.5\n18:10:29 |     topk: 5\n18:10:29 |     train_predict: False\n18:10:29 |     truncate: 1024\n18:10:29 |     update_classifier_head_only: False\n18:10:29 |     update_freq: 1\n18:10:29 |     use_memories: False\n18:10:29 |     use_reply: none\n18:10:29 |     validation_cutoff: 1.0\n18:10:29 |     validation_every_n_epochs: -1\n18:10:29 |     validation_every_n_secs: 20.0\n18:10:29 |     validation_every_n_steps: -1\n18:10:29 |     validation_max_exs: -1\n18:10:29 |     validation_metric: accuracy\n18:10:29 |     validation_metric_mode: max\n18:10:29 |     validation_patience: 30\n18:10:29 |     validation_share_agent: False\n18:10:29 |     variant: xlm\n18:10:29 |     verbose: False\n18:10:29 |     wandb_entity: None\n18:10:29 |     wandb_log: False\n18:10:29 |     wandb_name: None\n18:10:29 |     wandb_project: None\n18:10:29 |     warmup_rate: 0.0001\n18:10:29 |     warmup_updates: 1000\n18:10:29 |     weight_decay: None\n18:10:29 |     world_logs: \n18:10:29 |     wrap_memory_encoder: False\n18:10:30 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:10:30 | creating task(s): fromfile:parlaiformat\n18:10:30 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:10:30 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run3/data_train-b.txt\n18:10:35 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8750 8.75e-10               .8731                 .8776   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8687            .8768              .8725   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8812 11.47   539 505.1       0          0 37.48  200 .8750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .5249 5.204e-06 239.6 224.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 778.6 729.6        .8750\u001b[0m\n18:10:35 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8750 8.75e-10               .8731                 .8776   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8687            .8768              .8725   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8812 11.47   539 505.1       0          0 37.48  200 .8750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .5249 5.204e-06 239.6 224.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    104 778.6 729.6        .8750\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:10:37.514795Z","iopub.execute_input":"2022-12-03T18:10:37.515180Z","iopub.status.idle":"2022-12-03T18:10:38.685308Z","shell.execute_reply.started":"2022-12-03T18:10:37.515140Z","shell.execute_reply":"2022-12-03T18:10:38.683987Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:10:38.687519Z","iopub.execute_input":"2022-12-03T18:10:38.687913Z","iopub.status.idle":"2022-12-03T18:12:01.033465Z","shell.execute_reply.started":"2022-12-03T18:10:38.687873Z","shell.execute_reply":"2022-12-03T18:12:01.032226Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"18:10:45 | building dictionary first...\n18:10:45 | No model with opt yet at: /tmp/model4(.opt)\n18:10:45 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:10:45 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:10:45 | Using CUDA\n18:10:45 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:10:46 | num words = 54944\n18:10:50 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:11:00 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:11:00 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:11:00 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:11:00 | Opt:\n18:11:00 |     activation: gelu\n18:11:00 |     adafactor_eps: '(1e-30, 0.001)'\n18:11:00 |     adam_eps: 1e-08\n18:11:00 |     add_p1_after_newln: False\n18:11:00 |     aggregate_micro: False\n18:11:00 |     allow_missing_init_opts: False\n18:11:00 |     attention_dropout: 0.1\n18:11:00 |     batchsize: 20\n18:11:00 |     betas: '(0.9, 0.999)'\n18:11:00 |     bpe_add_prefix_space: None\n18:11:00 |     bpe_debug: False\n18:11:00 |     bpe_dropout: None\n18:11:00 |     bpe_merge: None\n18:11:00 |     bpe_vocab: None\n18:11:00 |     candidates: inline\n18:11:00 |     cap_num_predictions: 100\n18:11:00 |     checkpoint_activations: False\n18:11:00 |     class_weights: None\n18:11:00 |     classes: \"['__notok__', '__ok__']\"\n18:11:00 |     classes_from_file: None\n18:11:00 |     data_parallel: True\n18:11:00 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:11:00 |     datatype: train\n18:11:00 |     delimiter: '\\n'\n18:11:00 |     dict_class: parlai.core.dict:DictionaryAgent\n18:11:00 |     dict_endtoken: __start__\n18:11:00 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:11:00 |     dict_include_test: False\n18:11:00 |     dict_include_valid: False\n18:11:00 |     dict_initpath: None\n18:11:00 |     dict_language: english\n18:11:00 |     dict_loaded: True\n18:11:00 |     dict_lower: True\n18:11:00 |     dict_max_ngram_size: -1\n18:11:00 |     dict_maxexs: -1\n18:11:00 |     dict_maxtokens: -1\n18:11:00 |     dict_minfreq: 0\n18:11:00 |     dict_nulltoken: __null__\n18:11:00 |     dict_starttoken: __start__\n18:11:00 |     dict_textfields: text,labels\n18:11:00 |     dict_tokenizer: bpe\n18:11:00 |     dict_unktoken: __unk__\n18:11:00 |     display_examples: False\n18:11:00 |     download_path: None\n18:11:00 |     dropout: 0.1\n18:11:00 |     dynamic_batching: None\n18:11:00 |     embedding_projection: random\n18:11:00 |     embedding_size: 768\n18:11:00 |     embedding_type: random\n18:11:00 |     embeddings_scale: False\n18:11:00 |     encode_candidate_vecs: True\n18:11:00 |     encode_candidate_vecs_batchsize: 256\n18:11:00 |     eval_batchsize: None\n18:11:00 |     eval_candidates: inline\n18:11:00 |     eval_dynamic_batching: None\n18:11:00 |     evaltask: None\n18:11:00 |     ffn_size: 3072\n18:11:00 |     final_extra_opt: \n18:11:00 |     fixed_candidate_vecs: reuse\n18:11:00 |     fixed_candidates_path: None\n18:11:00 |     force_fp16_tokens: False\n18:11:00 |     fp16: True\n18:11:00 |     fp16_impl: safe\n18:11:00 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt\n18:11:00 |     fromfile_datatype_extension: False\n18:11:00 |     gpu: -1\n18:11:00 |     gradient_clip: 0.1\n18:11:00 |     hide_labels: False\n18:11:00 |     history_add_global_end_token: None\n18:11:00 |     history_reversed: False\n18:11:00 |     history_size: 20\n18:11:00 |     ignore_bad_candidates: False\n18:11:00 |     ignore_labels: None\n18:11:00 |     image_cropsize: 224\n18:11:00 |     image_mode: raw\n18:11:00 |     image_size: 256\n18:11:00 |     inference: max\n18:11:00 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:11:00 |     init_opt: None\n18:11:00 |     interactive_candidates: fixed\n18:11:00 |     interactive_mode: False\n18:11:00 |     invsqrt_lr_decay_gamma: -1\n18:11:00 |     is_debug: False\n18:11:00 |     label_truncate: 72\n18:11:00 |     learn_embeddings: True\n18:11:00 |     learn_positional_embeddings: True\n18:11:00 |     learningrate: 5e-05\n18:11:00 |     load_from_checkpoint: False\n18:11:00 |     load_from_pretrained_ranker: True\n18:11:00 |     log_every_n_secs: 10.0\n18:11:00 |     log_every_n_steps: 50\n18:11:00 |     log_keep_fields: all\n18:11:00 |     loglevel: info\n18:11:00 |     lr_scheduler: reduceonplateau\n18:11:00 |     lr_scheduler_decay: 0.5\n18:11:00 |     lr_scheduler_patience: 3\n18:11:00 |     max_train_steps: -1\n18:11:00 |     max_train_time: 7200.0\n18:11:00 |     memory_attention: sqrt\n18:11:00 |     metrics: default\n18:11:00 |     model: transformer/classifier\n18:11:00 |     model_file: /tmp/model4\n18:11:00 |     model_parallel: False\n18:11:00 |     momentum: 0\n18:11:00 |     multitask_weights: [1]\n18:11:00 |     mutators: None\n18:11:00 |     n_decoder_layers: -1\n18:11:00 |     n_encoder_layers: -1\n18:11:00 |     n_heads: 12\n18:11:00 |     n_layers: 12\n18:11:00 |     n_positions: 1024\n18:11:00 |     n_segments: 2\n18:11:00 |     nesterov: True\n18:11:00 |     no_cuda: False\n18:11:00 |     normalize_sent_emb: False\n18:11:00 |     num_epochs: -1\n18:11:00 |     num_workers: 0\n18:11:00 |     nus: (0.7,)\n18:11:00 |     optimizer: adamax\n18:11:00 |     output_scaling: 0.06\n18:11:00 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n18:11:00 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:11:00 |     person_tokens: False\n18:11:00 |     print_scores: False\n18:11:00 |     rank_candidates: False\n18:11:00 |     rank_top_k: -1\n18:11:00 |     reduction_type: mean\n18:11:00 |     ref_class: None\n18:11:00 |     relu_dropout: 0.0\n18:11:00 |     repeat_blocking_heuristic: True\n18:11:00 |     return_cand_scores: False\n18:11:00 |     save_after_valid: True\n18:11:00 |     save_every_n_secs: -1\n18:11:00 |     save_format: conversations\n18:11:00 |     share_encoders: False\n18:11:00 |     share_word_embeddings: False\n18:11:00 |     short_final_eval: False\n18:11:00 |     special_tok_lst: None\n18:11:00 |     split_lines: False\n18:11:00 |     starttime: Dec03_18-10\n18:11:00 |     task: fromfile:parlaiformat\n18:11:00 |     tensorboard_log: False\n18:11:00 |     tensorboard_logdir: None\n18:11:00 |     text_truncate: 360\n18:11:00 |     threshold: 0.5\n18:11:00 |     topk: 5\n18:11:00 |     train_predict: False\n18:11:00 |     truncate: 1024\n18:11:00 |     update_classifier_head_only: False\n18:11:00 |     update_freq: 1\n18:11:00 |     use_memories: False\n18:11:00 |     use_reply: none\n18:11:00 |     validation_cutoff: 1.0\n18:11:00 |     validation_every_n_epochs: -1\n18:11:00 |     validation_every_n_secs: 20.0\n18:11:00 |     validation_every_n_steps: -1\n18:11:00 |     validation_max_exs: -1\n18:11:00 |     validation_metric: accuracy\n18:11:00 |     validation_metric_mode: max\n18:11:00 |     validation_patience: 30\n18:11:00 |     validation_share_agent: False\n18:11:00 |     variant: xlm\n18:11:00 |     verbose: False\n18:11:00 |     wandb_entity: None\n18:11:00 |     wandb_log: False\n18:11:00 |     wandb_name: None\n18:11:00 |     wandb_project: None\n18:11:00 |     warmup_rate: 0.0001\n18:11:00 |     warmup_updates: 1000\n18:11:00 |     weight_decay: None\n18:11:00 |     world_logs: \n18:11:00 |     wrap_memory_encoder: False\n18:11:01 | creating task(s): fromfile:parlaiformat\n18:11:01 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt\n18:11:01 | training...\n18:11:11 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .5600 5.6e-10               .5138                 .5439   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4869            .5982              .5721   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6268 11.91     1 278.3 553.8       0          0  39.8  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5600             32768  2.787    .1189 5.955 .6873 1.005e-06 119.1   237   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 397.4 790.8 1.994        .5579\n\n18:11:21 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7541 7.541e-10               .7520                 .7731   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7321            .7560              .7363   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7769 11.78     1 275.6  1042       0          0 75.61  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7541             32768  2.736    .1189 6.019 .6478 2.855e-06 120.4 455.1   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                   57  396 1497 3.789        .7540\n\n18:11:21 | creating task(s): fromfile:parlaiformat\n18:11:21 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:11:21 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt\n18:11:21 | running eval: valid\n18:11:21 | eval completed in 0.20s\n18:11:21 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9565                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9167            .9600              .9231   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                      1 12.04 168.5  1854       0          0   132   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5857 2.855e-06    72 792.1       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2646        .9583\n\u001b[0m\n18:11:21 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n18:11:21 | saving best valid model: /tmp/model4\n18:11:21 | Saving dictionary to /tmp/model4.dict\n18:11:25 | saving model checkpoint: /tmp/model4.checkpoint\n18:11:25 | Saving dictionary to /tmp/model4.checkpoint.dict\n18:11:43 | time:42s total_exs:1860 total_steps:93 epochs:77.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9944 9.944e-10               .9944                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9888            .9945              .9891   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.97     1 279.3   988       0          0 70.74  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9944             32768  2.824    .1189 5.994 .5179 4.655e-06 119.9 424.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   93 399.2 1412 3.545        .9944\n\n18:11:45 | time:45s total_exs:2060 total_steps:103 epochs:85.83\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.79     1 275.8  1054       0          0 76.46  200   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.717    .1189  5.98 .3855 5.154e-06 119.6 457.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  103 395.4 1512 3.856            1\n\n18:11:45 | running eval: valid\n18:11:45 | eval completed in 0.19s\n18:11:45 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1936       0          0 137.8   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3126 5.154e-06    72   827       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    103 240.5 2763            1\n\u001b[0m\n18:11:45 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n18:11:45 | saving best valid model: /tmp/model4\n18:11:50 | task solved! stopping.\n18:11:50 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:11:50 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:11:50 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:11:50 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:11:50 | Using CUDA\n18:11:50 | loading dictionary from /tmp/model4.dict\n18:11:51 | num words = 54944\n18:11:55 | Loading existing model parameters from /tmp/model4\n18:11:57 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:11:58 | creating task(s): fromfile:parlaiformat\n18:11:58 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:11:58 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt\n18:11:58 | running eval: valid\n18:11:59 | eval completed in 0.25s\n18:11:59 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1547       0          0 110.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3126 5.154e-06    72   661       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    103 240.5 2208            1\n\u001b[0m\n18:11:59 | creating task(s): fromfile:parlaiformat\n18:11:59 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:11:59 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt\n18:11:59 | running eval: test\n18:11:59 | eval completed in 0.23s\n18:11:59 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1635       0          0 116.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3126 5.154e-06    72 698.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    103 240.5 2334            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:12:01.035248Z","iopub.execute_input":"2022-12-03T18:12:01.035675Z","iopub.status.idle":"2022-12-03T18:12:28.603090Z","shell.execute_reply.started":"2022-12-03T18:12:01.035633Z","shell.execute_reply":"2022-12-03T18:12:28.601913Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stdout","text":"18:12:07 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt)\u001b[0m\n18:12:07 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:12:07 | Using CUDA\n18:12:07 | loading dictionary from /tmp/model4.dict\n18:12:08 | num words = 54944\n18:12:12 | Loading existing model parameters from /tmp/model4\n18:12:19 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:12:20 | Opt:\n18:12:20 |     activation: gelu\n18:12:20 |     adafactor_eps: '[1e-30, 0.001]'\n18:12:20 |     adam_eps: 1e-08\n18:12:20 |     add_p1_after_newln: False\n18:12:20 |     aggregate_micro: False\n18:12:20 |     allow_missing_init_opts: False\n18:12:20 |     area_under_curve_class: None\n18:12:20 |     area_under_curve_digits: -1\n18:12:20 |     attention_dropout: 0.1\n18:12:20 |     batchsize: 40\n18:12:20 |     betas: '[0.9, 0.999]'\n18:12:20 |     bpe_add_prefix_space: None\n18:12:20 |     bpe_debug: False\n18:12:20 |     bpe_dropout: None\n18:12:20 |     bpe_merge: None\n18:12:20 |     bpe_vocab: None\n18:12:20 |     candidates: inline\n18:12:20 |     cap_num_predictions: 100\n18:12:20 |     checkpoint_activations: False\n18:12:20 |     class_weights: None\n18:12:20 |     classes: \"['__notok__', '__ok__']\"\n18:12:20 |     classes_from_file: None\n18:12:20 |     data_parallel: True\n18:12:20 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:12:20 |     datatype: train\n18:12:20 |     delimiter: '\\n'\n18:12:20 |     dict_class: parlai.core.dict:DictionaryAgent\n18:12:20 |     dict_endtoken: __start__\n18:12:20 |     dict_file: /tmp/model4.dict\n18:12:20 |     dict_include_test: False\n18:12:20 |     dict_include_valid: False\n18:12:20 |     dict_initpath: None\n18:12:20 |     dict_language: english\n18:12:20 |     dict_loaded: True\n18:12:20 |     dict_lower: True\n18:12:20 |     dict_max_ngram_size: -1\n18:12:20 |     dict_maxexs: -1\n18:12:20 |     dict_maxtokens: -1\n18:12:20 |     dict_minfreq: 0\n18:12:20 |     dict_nulltoken: __null__\n18:12:20 |     dict_starttoken: __start__\n18:12:20 |     dict_textfields: text,labels\n18:12:20 |     dict_tokenizer: bpe\n18:12:20 |     dict_unktoken: __unk__\n18:12:20 |     display_examples: False\n18:12:20 |     download_path: None\n18:12:20 |     dropout: 0.1\n18:12:20 |     dynamic_batching: None\n18:12:20 |     embedding_projection: random\n18:12:20 |     embedding_size: 768\n18:12:20 |     embedding_type: random\n18:12:20 |     embeddings_scale: False\n18:12:20 |     encode_candidate_vecs: True\n18:12:20 |     encode_candidate_vecs_batchsize: 256\n18:12:20 |     eval_batchsize: None\n18:12:20 |     eval_candidates: inline\n18:12:20 |     eval_dynamic_batching: None\n18:12:20 |     evaltask: None\n18:12:20 |     ffn_size: 3072\n18:12:20 |     final_extra_opt: \n18:12:20 |     fixed_candidate_vecs: reuse\n18:12:20 |     fixed_candidates_path: None\n18:12:20 |     force_fp16_tokens: True\n18:12:20 |     fp16: True\n18:12:20 |     fp16_impl: safe\n18:12:20 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-a.txt\n18:12:20 |     fromfile_datatype_extension: False\n18:12:20 |     gpu: -1\n18:12:20 |     gradient_clip: 0.1\n18:12:20 |     hide_labels: False\n18:12:20 |     history_add_global_end_token: None\n18:12:20 |     history_reversed: False\n18:12:20 |     history_size: 20\n18:12:20 |     ignore_bad_candidates: False\n18:12:20 |     ignore_labels: None\n18:12:20 |     image_cropsize: 224\n18:12:20 |     image_mode: raw\n18:12:20 |     image_size: 256\n18:12:20 |     inference: max\n18:12:20 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:12:20 |     init_opt: None\n18:12:20 |     interactive_candidates: fixed\n18:12:20 |     interactive_mode: False\n18:12:20 |     invsqrt_lr_decay_gamma: -1\n18:12:20 |     is_debug: False\n18:12:20 |     label_truncate: 72\n18:12:20 |     learn_embeddings: True\n18:12:20 |     learn_positional_embeddings: True\n18:12:20 |     learningrate: 5e-05\n18:12:20 |     load_from_pretrained_ranker: True\n18:12:20 |     log_every_n_secs: 10.0\n18:12:20 |     log_every_n_steps: 50\n18:12:20 |     log_keep_fields: all\n18:12:20 |     loglevel: info\n18:12:20 |     lr_scheduler: reduceonplateau\n18:12:20 |     lr_scheduler_decay: 0.5\n18:12:20 |     lr_scheduler_patience: 3\n18:12:20 |     max_train_steps: -1\n18:12:20 |     max_train_time: 7200.0\n18:12:20 |     memory_attention: sqrt\n18:12:20 |     metrics: default\n18:12:20 |     model: transformer/classifier\n18:12:20 |     model_file: /tmp/model4\n18:12:20 |     model_parallel: False\n18:12:20 |     momentum: 0\n18:12:20 |     multitask_weights: [1]\n18:12:20 |     mutators: None\n18:12:20 |     n_decoder_layers: -1\n18:12:20 |     n_encoder_layers: -1\n18:12:20 |     n_heads: 12\n18:12:20 |     n_layers: 12\n18:12:20 |     n_positions: 1024\n18:12:20 |     n_segments: 2\n18:12:20 |     nesterov: True\n18:12:20 |     no_cuda: False\n18:12:20 |     normalize_sent_emb: False\n18:12:20 |     num_epochs: -1\n18:12:20 |     num_examples: -1\n18:12:20 |     num_workers: 0\n18:12:20 |     nus: [0.7]\n18:12:20 |     optimizer: adamax\n18:12:20 |     output_scaling: 0.06\n18:12:20 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n18:12:20 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:12:20 |     person_tokens: False\n18:12:20 |     print_scores: False\n18:12:20 |     rank_candidates: False\n18:12:20 |     rank_top_k: -1\n18:12:20 |     reduction_type: mean\n18:12:20 |     ref_class: None\n18:12:20 |     relu_dropout: 0.0\n18:12:20 |     repeat_blocking_heuristic: True\n18:12:20 |     report_filename: \n18:12:20 |     return_cand_scores: False\n18:12:20 |     save_after_valid: True\n18:12:20 |     save_every_n_secs: -1\n18:12:20 |     save_format: conversations\n18:12:20 |     share_encoders: False\n18:12:20 |     share_word_embeddings: False\n18:12:20 |     short_final_eval: False\n18:12:20 |     special_tok_lst: None\n18:12:20 |     split_lines: False\n18:12:20 |     starttime: Dec03_18-10\n18:12:20 |     task: fromfile:parlaiformat\n18:12:20 |     tensorboard_log: False\n18:12:20 |     tensorboard_logdir: None\n18:12:20 |     text_truncate: 360\n18:12:20 |     threshold: 0.5\n18:12:20 |     topk: 5\n18:12:20 |     train_predict: False\n18:12:20 |     truncate: 1024\n18:12:20 |     update_classifier_head_only: False\n18:12:20 |     update_freq: 1\n18:12:20 |     use_memories: False\n18:12:20 |     use_reply: none\n18:12:20 |     validation_cutoff: 1.0\n18:12:20 |     validation_every_n_epochs: -1\n18:12:20 |     validation_every_n_secs: 20.0\n18:12:20 |     validation_every_n_steps: -1\n18:12:20 |     validation_max_exs: -1\n18:12:20 |     validation_metric: accuracy\n18:12:20 |     validation_metric_mode: max\n18:12:20 |     validation_patience: 30\n18:12:20 |     validation_share_agent: False\n18:12:20 |     variant: xlm\n18:12:20 |     verbose: False\n18:12:20 |     wandb_entity: None\n18:12:20 |     wandb_log: False\n18:12:20 |     wandb_name: None\n18:12:20 |     wandb_project: None\n18:12:20 |     warmup_rate: 0.0001\n18:12:20 |     warmup_updates: 1000\n18:12:20 |     weight_decay: None\n18:12:20 |     world_logs: \n18:12:20 |     wrap_memory_encoder: False\n18:12:21 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:12:21 | creating task(s): fromfile:parlaiformat\n18:12:21 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:12:21 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-a.txt\n18:12:26 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1850 1.85e-10               .2049                 .2019   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2079            .1641              .1667   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1616 11.46 538.2 506.8       0          0 37.67  200 .1850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .8864 5.154e-06 240.4 226.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 778.6 733.2        .1847\u001b[0m\n18:12:26 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1850 1.85e-10               .2049                 .2019   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2079            .1641              .1667   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1616 11.46 538.2 506.8       0          0 37.67  200 .1850   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .8864 5.154e-06 240.4 226.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 778.6 733.2        .1847\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:12:28.604936Z","iopub.execute_input":"2022-12-03T18:12:28.605337Z","iopub.status.idle":"2022-12-03T18:12:54.953863Z","shell.execute_reply.started":"2022-12-03T18:12:28.605298Z","shell.execute_reply":"2022-12-03T18:12:54.952563Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"18:12:35 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_valid.txt)\u001b[0m\n18:12:35 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:12:35 | Using CUDA\n18:12:35 | loading dictionary from /tmp/model4.dict\n18:12:36 | num words = 54944\n18:12:40 | Loading existing model parameters from /tmp/model4\n18:12:46 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:12:47 | Opt:\n18:12:47 |     activation: gelu\n18:12:47 |     adafactor_eps: '[1e-30, 0.001]'\n18:12:47 |     adam_eps: 1e-08\n18:12:47 |     add_p1_after_newln: False\n18:12:47 |     aggregate_micro: False\n18:12:47 |     allow_missing_init_opts: False\n18:12:47 |     area_under_curve_class: None\n18:12:47 |     area_under_curve_digits: -1\n18:12:47 |     attention_dropout: 0.1\n18:12:47 |     batchsize: 40\n18:12:47 |     betas: '[0.9, 0.999]'\n18:12:47 |     bpe_add_prefix_space: None\n18:12:47 |     bpe_debug: False\n18:12:47 |     bpe_dropout: None\n18:12:47 |     bpe_merge: None\n18:12:47 |     bpe_vocab: None\n18:12:47 |     candidates: inline\n18:12:47 |     cap_num_predictions: 100\n18:12:47 |     checkpoint_activations: False\n18:12:47 |     class_weights: None\n18:12:47 |     classes: \"['__notok__', '__ok__']\"\n18:12:47 |     classes_from_file: None\n18:12:47 |     data_parallel: True\n18:12:47 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:12:47 |     datatype: train\n18:12:47 |     delimiter: '\\n'\n18:12:47 |     dict_class: parlai.core.dict:DictionaryAgent\n18:12:47 |     dict_endtoken: __start__\n18:12:47 |     dict_file: /tmp/model4.dict\n18:12:47 |     dict_include_test: False\n18:12:47 |     dict_include_valid: False\n18:12:47 |     dict_initpath: None\n18:12:47 |     dict_language: english\n18:12:47 |     dict_loaded: True\n18:12:47 |     dict_lower: True\n18:12:47 |     dict_max_ngram_size: -1\n18:12:47 |     dict_maxexs: -1\n18:12:47 |     dict_maxtokens: -1\n18:12:47 |     dict_minfreq: 0\n18:12:47 |     dict_nulltoken: __null__\n18:12:47 |     dict_starttoken: __start__\n18:12:47 |     dict_textfields: text,labels\n18:12:47 |     dict_tokenizer: bpe\n18:12:47 |     dict_unktoken: __unk__\n18:12:47 |     display_examples: False\n18:12:47 |     download_path: None\n18:12:47 |     dropout: 0.1\n18:12:47 |     dynamic_batching: None\n18:12:47 |     embedding_projection: random\n18:12:47 |     embedding_size: 768\n18:12:47 |     embedding_type: random\n18:12:47 |     embeddings_scale: False\n18:12:47 |     encode_candidate_vecs: True\n18:12:47 |     encode_candidate_vecs_batchsize: 256\n18:12:47 |     eval_batchsize: None\n18:12:47 |     eval_candidates: inline\n18:12:47 |     eval_dynamic_batching: None\n18:12:47 |     evaltask: None\n18:12:47 |     ffn_size: 3072\n18:12:47 |     final_extra_opt: \n18:12:47 |     fixed_candidate_vecs: reuse\n18:12:47 |     fixed_candidates_path: None\n18:12:47 |     force_fp16_tokens: True\n18:12:47 |     fp16: True\n18:12:47 |     fp16_impl: safe\n18:12:47 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-b.txt\n18:12:47 |     fromfile_datatype_extension: False\n18:12:47 |     gpu: -1\n18:12:47 |     gradient_clip: 0.1\n18:12:47 |     hide_labels: False\n18:12:47 |     history_add_global_end_token: None\n18:12:47 |     history_reversed: False\n18:12:47 |     history_size: 20\n18:12:47 |     ignore_bad_candidates: False\n18:12:47 |     ignore_labels: None\n18:12:47 |     image_cropsize: 224\n18:12:47 |     image_mode: raw\n18:12:47 |     image_size: 256\n18:12:47 |     inference: max\n18:12:47 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:12:47 |     init_opt: None\n18:12:47 |     interactive_candidates: fixed\n18:12:47 |     interactive_mode: False\n18:12:47 |     invsqrt_lr_decay_gamma: -1\n18:12:47 |     is_debug: False\n18:12:47 |     label_truncate: 72\n18:12:47 |     learn_embeddings: True\n18:12:47 |     learn_positional_embeddings: True\n18:12:47 |     learningrate: 5e-05\n18:12:47 |     load_from_pretrained_ranker: True\n18:12:47 |     log_every_n_secs: 10.0\n18:12:47 |     log_every_n_steps: 50\n18:12:47 |     log_keep_fields: all\n18:12:47 |     loglevel: info\n18:12:47 |     lr_scheduler: reduceonplateau\n18:12:47 |     lr_scheduler_decay: 0.5\n18:12:47 |     lr_scheduler_patience: 3\n18:12:47 |     max_train_steps: -1\n18:12:47 |     max_train_time: 7200.0\n18:12:47 |     memory_attention: sqrt\n18:12:47 |     metrics: default\n18:12:47 |     model: transformer/classifier\n18:12:47 |     model_file: /tmp/model4\n18:12:47 |     model_parallel: False\n18:12:47 |     momentum: 0\n18:12:47 |     multitask_weights: [1]\n18:12:47 |     mutators: None\n18:12:47 |     n_decoder_layers: -1\n18:12:47 |     n_encoder_layers: -1\n18:12:47 |     n_heads: 12\n18:12:47 |     n_layers: 12\n18:12:47 |     n_positions: 1024\n18:12:47 |     n_segments: 2\n18:12:47 |     nesterov: True\n18:12:47 |     no_cuda: False\n18:12:47 |     normalize_sent_emb: False\n18:12:47 |     num_epochs: -1\n18:12:47 |     num_examples: -1\n18:12:47 |     num_workers: 0\n18:12:47 |     nus: [0.7]\n18:12:47 |     optimizer: adamax\n18:12:47 |     output_scaling: 0.06\n18:12:47 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n18:12:47 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:12:47 |     person_tokens: False\n18:12:47 |     print_scores: False\n18:12:47 |     rank_candidates: False\n18:12:47 |     rank_top_k: -1\n18:12:47 |     reduction_type: mean\n18:12:47 |     ref_class: None\n18:12:47 |     relu_dropout: 0.0\n18:12:47 |     repeat_blocking_heuristic: True\n18:12:47 |     report_filename: \n18:12:47 |     return_cand_scores: False\n18:12:47 |     save_after_valid: True\n18:12:47 |     save_every_n_secs: -1\n18:12:47 |     save_format: conversations\n18:12:47 |     share_encoders: False\n18:12:47 |     share_word_embeddings: False\n18:12:47 |     short_final_eval: False\n18:12:47 |     special_tok_lst: None\n18:12:47 |     split_lines: False\n18:12:47 |     starttime: Dec03_18-10\n18:12:47 |     task: fromfile:parlaiformat\n18:12:47 |     tensorboard_log: False\n18:12:47 |     tensorboard_logdir: None\n18:12:47 |     text_truncate: 360\n18:12:47 |     threshold: 0.5\n18:12:47 |     topk: 5\n18:12:47 |     train_predict: False\n18:12:47 |     truncate: 1024\n18:12:47 |     update_classifier_head_only: False\n18:12:47 |     update_freq: 1\n18:12:47 |     use_memories: False\n18:12:47 |     use_reply: none\n18:12:47 |     validation_cutoff: 1.0\n18:12:47 |     validation_every_n_epochs: -1\n18:12:47 |     validation_every_n_secs: 20.0\n18:12:47 |     validation_every_n_steps: -1\n18:12:47 |     validation_max_exs: -1\n18:12:47 |     validation_metric: accuracy\n18:12:47 |     validation_metric_mode: max\n18:12:47 |     validation_patience: 30\n18:12:47 |     validation_share_agent: False\n18:12:47 |     variant: xlm\n18:12:47 |     verbose: False\n18:12:47 |     wandb_entity: None\n18:12:47 |     wandb_log: False\n18:12:47 |     wandb_name: None\n18:12:47 |     wandb_project: None\n18:12:47 |     warmup_rate: 0.0001\n18:12:47 |     warmup_updates: 1000\n18:12:47 |     weight_decay: None\n18:12:47 |     world_logs: \n18:12:47 |     wrap_memory_encoder: False\n18:12:47 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:12:47 | creating task(s): fromfile:parlaiformat\n18:12:47 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:12:47 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run4/data_train-b.txt\n18:12:53 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8150 8.15e-10               .8177                 .7981   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8384            .8122              .8333   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7921 11.46 538.2 507.3       0          0  37.7  200 .8150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .5571 5.154e-06 239.6 225.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 777.8 733.1        .8149\u001b[0m\n18:12:53 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8150 8.15e-10               .8177                 .7981   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8384            .8122              .8333   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .7921 11.46 538.2 507.3       0          0  37.7  200 .8150   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .5571 5.154e-06 239.6 225.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    103 777.8 733.1        .8149\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:12:54.955653Z","iopub.execute_input":"2022-12-03T18:12:54.956077Z","iopub.status.idle":"2022-12-03T18:12:56.111826Z","shell.execute_reply.started":"2022-12-03T18:12:54.956034Z","shell.execute_reply":"2022-12-03T18:12:56.110465Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:12:56.114153Z","iopub.execute_input":"2022-12-03T18:12:56.114499Z","iopub.status.idle":"2022-12-03T18:13:53.412767Z","shell.execute_reply.started":"2022-12-03T18:12:56.114458Z","shell.execute_reply":"2022-12-03T18:13:53.411560Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"18:13:03 | building dictionary first...\n18:13:03 | No model with opt yet at: /tmp/model5(.opt)\n18:13:03 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:13:03 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:13:03 | Using CUDA\n18:13:03 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:13:03 | num words = 54944\n18:13:08 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:13:18 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:13:18 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:13:18 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:13:18 | Opt:\n18:13:18 |     activation: gelu\n18:13:18 |     adafactor_eps: '(1e-30, 0.001)'\n18:13:18 |     adam_eps: 1e-08\n18:13:18 |     add_p1_after_newln: False\n18:13:18 |     aggregate_micro: False\n18:13:18 |     allow_missing_init_opts: False\n18:13:18 |     attention_dropout: 0.1\n18:13:18 |     batchsize: 20\n18:13:18 |     betas: '(0.9, 0.999)'\n18:13:18 |     bpe_add_prefix_space: None\n18:13:18 |     bpe_debug: False\n18:13:18 |     bpe_dropout: None\n18:13:18 |     bpe_merge: None\n18:13:18 |     bpe_vocab: None\n18:13:18 |     candidates: inline\n18:13:18 |     cap_num_predictions: 100\n18:13:18 |     checkpoint_activations: False\n18:13:18 |     class_weights: None\n18:13:18 |     classes: \"['__notok__', '__ok__']\"\n18:13:18 |     classes_from_file: None\n18:13:18 |     data_parallel: True\n18:13:18 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:13:18 |     datatype: train\n18:13:18 |     delimiter: '\\n'\n18:13:18 |     dict_class: parlai.core.dict:DictionaryAgent\n18:13:18 |     dict_endtoken: __start__\n18:13:18 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:13:18 |     dict_include_test: False\n18:13:18 |     dict_include_valid: False\n18:13:18 |     dict_initpath: None\n18:13:18 |     dict_language: english\n18:13:18 |     dict_loaded: True\n18:13:18 |     dict_lower: True\n18:13:18 |     dict_max_ngram_size: -1\n18:13:18 |     dict_maxexs: -1\n18:13:18 |     dict_maxtokens: -1\n18:13:18 |     dict_minfreq: 0\n18:13:18 |     dict_nulltoken: __null__\n18:13:18 |     dict_starttoken: __start__\n18:13:18 |     dict_textfields: text,labels\n18:13:18 |     dict_tokenizer: bpe\n18:13:18 |     dict_unktoken: __unk__\n18:13:18 |     display_examples: False\n18:13:18 |     download_path: None\n18:13:18 |     dropout: 0.1\n18:13:18 |     dynamic_batching: None\n18:13:18 |     embedding_projection: random\n18:13:18 |     embedding_size: 768\n18:13:18 |     embedding_type: random\n18:13:18 |     embeddings_scale: False\n18:13:18 |     encode_candidate_vecs: True\n18:13:18 |     encode_candidate_vecs_batchsize: 256\n18:13:18 |     eval_batchsize: None\n18:13:18 |     eval_candidates: inline\n18:13:18 |     eval_dynamic_batching: None\n18:13:18 |     evaltask: None\n18:13:18 |     ffn_size: 3072\n18:13:18 |     final_extra_opt: \n18:13:18 |     fixed_candidate_vecs: reuse\n18:13:18 |     fixed_candidates_path: None\n18:13:18 |     force_fp16_tokens: False\n18:13:18 |     fp16: True\n18:13:18 |     fp16_impl: safe\n18:13:18 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt\n18:13:18 |     fromfile_datatype_extension: False\n18:13:18 |     gpu: -1\n18:13:18 |     gradient_clip: 0.1\n18:13:18 |     hide_labels: False\n18:13:18 |     history_add_global_end_token: None\n18:13:18 |     history_reversed: False\n18:13:18 |     history_size: 20\n18:13:18 |     ignore_bad_candidates: False\n18:13:18 |     ignore_labels: None\n18:13:18 |     image_cropsize: 224\n18:13:18 |     image_mode: raw\n18:13:18 |     image_size: 256\n18:13:18 |     inference: max\n18:13:18 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:13:18 |     init_opt: None\n18:13:18 |     interactive_candidates: fixed\n18:13:18 |     interactive_mode: False\n18:13:18 |     invsqrt_lr_decay_gamma: -1\n18:13:18 |     is_debug: False\n18:13:18 |     label_truncate: 72\n18:13:18 |     learn_embeddings: True\n18:13:18 |     learn_positional_embeddings: True\n18:13:18 |     learningrate: 5e-05\n18:13:18 |     load_from_checkpoint: False\n18:13:18 |     load_from_pretrained_ranker: True\n18:13:18 |     log_every_n_secs: 10.0\n18:13:18 |     log_every_n_steps: 50\n18:13:18 |     log_keep_fields: all\n18:13:18 |     loglevel: info\n18:13:18 |     lr_scheduler: reduceonplateau\n18:13:18 |     lr_scheduler_decay: 0.5\n18:13:18 |     lr_scheduler_patience: 3\n18:13:18 |     max_train_steps: -1\n18:13:18 |     max_train_time: 7200.0\n18:13:18 |     memory_attention: sqrt\n18:13:18 |     metrics: default\n18:13:18 |     model: transformer/classifier\n18:13:18 |     model_file: /tmp/model5\n18:13:18 |     model_parallel: False\n18:13:18 |     momentum: 0\n18:13:18 |     multitask_weights: [1]\n18:13:18 |     mutators: None\n18:13:18 |     n_decoder_layers: -1\n18:13:18 |     n_encoder_layers: -1\n18:13:18 |     n_heads: 12\n18:13:18 |     n_layers: 12\n18:13:18 |     n_positions: 1024\n18:13:18 |     n_segments: 2\n18:13:18 |     nesterov: True\n18:13:18 |     no_cuda: False\n18:13:18 |     normalize_sent_emb: False\n18:13:18 |     num_epochs: -1\n18:13:18 |     num_workers: 0\n18:13:18 |     nus: (0.7,)\n18:13:18 |     optimizer: adamax\n18:13:18 |     output_scaling: 0.06\n18:13:18 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n18:13:18 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:13:18 |     person_tokens: False\n18:13:18 |     print_scores: False\n18:13:18 |     rank_candidates: False\n18:13:18 |     rank_top_k: -1\n18:13:18 |     reduction_type: mean\n18:13:18 |     ref_class: None\n18:13:18 |     relu_dropout: 0.0\n18:13:18 |     repeat_blocking_heuristic: True\n18:13:18 |     return_cand_scores: False\n18:13:18 |     save_after_valid: True\n18:13:18 |     save_every_n_secs: -1\n18:13:18 |     save_format: conversations\n18:13:18 |     share_encoders: False\n18:13:18 |     share_word_embeddings: False\n18:13:18 |     short_final_eval: False\n18:13:18 |     special_tok_lst: None\n18:13:18 |     split_lines: False\n18:13:18 |     starttime: Dec03_18-13\n18:13:18 |     task: fromfile:parlaiformat\n18:13:18 |     tensorboard_log: False\n18:13:18 |     tensorboard_logdir: None\n18:13:18 |     text_truncate: 360\n18:13:18 |     threshold: 0.5\n18:13:18 |     topk: 5\n18:13:18 |     train_predict: False\n18:13:18 |     truncate: 1024\n18:13:18 |     update_classifier_head_only: False\n18:13:18 |     update_freq: 1\n18:13:18 |     use_memories: False\n18:13:18 |     use_reply: none\n18:13:18 |     validation_cutoff: 1.0\n18:13:18 |     validation_every_n_epochs: -1\n18:13:18 |     validation_every_n_secs: 20.0\n18:13:18 |     validation_every_n_steps: -1\n18:13:18 |     validation_max_exs: -1\n18:13:18 |     validation_metric: accuracy\n18:13:18 |     validation_metric_mode: max\n18:13:18 |     validation_patience: 30\n18:13:18 |     validation_share_agent: False\n18:13:18 |     variant: xlm\n18:13:18 |     verbose: False\n18:13:18 |     wandb_entity: None\n18:13:18 |     wandb_log: False\n18:13:18 |     wandb_name: None\n18:13:18 |     wandb_project: None\n18:13:18 |     warmup_rate: 0.0001\n18:13:18 |     warmup_updates: 1000\n18:13:18 |     weight_decay: None\n18:13:18 |     world_logs: \n18:13:18 |     wrap_memory_encoder: False\n18:13:18 | creating task(s): fromfile:parlaiformat\n18:13:18 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt\n18:13:19 | training...\n18:13:29 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4738 4.738e-10               .3775                 .4621   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .3190            .5443              .4800   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6286 11.06     1 261.2 535.7       0          0 41.01  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4738             32768  2.843    .1206     6 .6937 1.055e-06   120 246.1   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 381.2 781.8 2.055        .4609\n\n18:13:39 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7569 7.569e-10               .7664                 .7778   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7553            .7467              .7350   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7588 10.69     1 253.8 943.5       0          0 74.36  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7569             32768  2.701    .1207 6.056 .6452 2.855e-06 121.1 450.3   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 374.9 1394 3.727        .7571\n\n18:13:39 | creating task(s): fromfile:parlaiformat\n18:13:39 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:13:39 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt\n18:13:39 | running eval: valid\n18:13:39 | eval completed in 0.20s\n18:13:39 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1671       0          0 131.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5787 2.855e-06    72 791.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  224 2462            1\n\u001b[0m\n18:13:39 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n18:13:39 | saving best valid model: /tmp/model5\n18:13:39 | Saving dictionary to /tmp/model5.dict\n18:13:42 | task solved! stopping.\n18:13:42 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:13:42 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:13:42 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:13:42 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:13:42 | Using CUDA\n18:13:43 | loading dictionary from /tmp/model5.dict\n18:13:43 | num words = 54944\n18:13:48 | Loading existing model parameters from /tmp/model5\n18:13:49 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:13:51 | creating task(s): fromfile:parlaiformat\n18:13:51 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:13:51 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt\n18:13:51 | running eval: valid\n18:13:51 | eval completed in 0.21s\n18:13:51 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1621       0          0 127.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5787 2.855e-06    72 767.7       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  224 2389            1\n\u001b[0m\n18:13:51 | creating task(s): fromfile:parlaiformat\n18:13:51 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:13:51 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt\n18:13:51 | running eval: test\n18:13:51 | eval completed in 0.20s\n18:13:51 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1656       0          0 130.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5787 2.855e-06    72 784.5       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  224 2441            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:13:53.414964Z","iopub.execute_input":"2022-12-03T18:13:53.415352Z","iopub.status.idle":"2022-12-03T18:14:21.017171Z","shell.execute_reply.started":"2022-12-03T18:13:53.415313Z","shell.execute_reply":"2022-12-03T18:14:21.016025Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"18:14:00 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt)\u001b[0m\n18:14:00 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:14:00 | Using CUDA\n18:14:00 | loading dictionary from /tmp/model5.dict\n18:14:00 | num words = 54944\n18:14:05 | Loading existing model parameters from /tmp/model5\n18:14:11 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:14:13 | Opt:\n18:14:13 |     activation: gelu\n18:14:13 |     adafactor_eps: '[1e-30, 0.001]'\n18:14:13 |     adam_eps: 1e-08\n18:14:13 |     add_p1_after_newln: False\n18:14:13 |     aggregate_micro: False\n18:14:13 |     allow_missing_init_opts: False\n18:14:13 |     area_under_curve_class: None\n18:14:13 |     area_under_curve_digits: -1\n18:14:13 |     attention_dropout: 0.1\n18:14:13 |     batchsize: 40\n18:14:13 |     betas: '[0.9, 0.999]'\n18:14:13 |     bpe_add_prefix_space: None\n18:14:13 |     bpe_debug: False\n18:14:13 |     bpe_dropout: None\n18:14:13 |     bpe_merge: None\n18:14:13 |     bpe_vocab: None\n18:14:13 |     candidates: inline\n18:14:13 |     cap_num_predictions: 100\n18:14:13 |     checkpoint_activations: False\n18:14:13 |     class_weights: None\n18:14:13 |     classes: \"['__notok__', '__ok__']\"\n18:14:13 |     classes_from_file: None\n18:14:13 |     data_parallel: True\n18:14:13 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:14:13 |     datatype: train\n18:14:13 |     delimiter: '\\n'\n18:14:13 |     dict_class: parlai.core.dict:DictionaryAgent\n18:14:13 |     dict_endtoken: __start__\n18:14:13 |     dict_file: /tmp/model5.dict\n18:14:13 |     dict_include_test: False\n18:14:13 |     dict_include_valid: False\n18:14:13 |     dict_initpath: None\n18:14:13 |     dict_language: english\n18:14:13 |     dict_loaded: True\n18:14:13 |     dict_lower: True\n18:14:13 |     dict_max_ngram_size: -1\n18:14:13 |     dict_maxexs: -1\n18:14:13 |     dict_maxtokens: -1\n18:14:13 |     dict_minfreq: 0\n18:14:13 |     dict_nulltoken: __null__\n18:14:13 |     dict_starttoken: __start__\n18:14:13 |     dict_textfields: text,labels\n18:14:13 |     dict_tokenizer: bpe\n18:14:13 |     dict_unktoken: __unk__\n18:14:13 |     display_examples: False\n18:14:13 |     download_path: None\n18:14:13 |     dropout: 0.1\n18:14:13 |     dynamic_batching: None\n18:14:13 |     embedding_projection: random\n18:14:13 |     embedding_size: 768\n18:14:13 |     embedding_type: random\n18:14:13 |     embeddings_scale: False\n18:14:13 |     encode_candidate_vecs: True\n18:14:13 |     encode_candidate_vecs_batchsize: 256\n18:14:13 |     eval_batchsize: None\n18:14:13 |     eval_candidates: inline\n18:14:13 |     eval_dynamic_batching: None\n18:14:13 |     evaltask: None\n18:14:13 |     ffn_size: 3072\n18:14:13 |     final_extra_opt: \n18:14:13 |     fixed_candidate_vecs: reuse\n18:14:13 |     fixed_candidates_path: None\n18:14:13 |     force_fp16_tokens: True\n18:14:13 |     fp16: True\n18:14:13 |     fp16_impl: safe\n18:14:13 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-a.txt\n18:14:13 |     fromfile_datatype_extension: False\n18:14:13 |     gpu: -1\n18:14:13 |     gradient_clip: 0.1\n18:14:13 |     hide_labels: False\n18:14:13 |     history_add_global_end_token: None\n18:14:13 |     history_reversed: False\n18:14:13 |     history_size: 20\n18:14:13 |     ignore_bad_candidates: False\n18:14:13 |     ignore_labels: None\n18:14:13 |     image_cropsize: 224\n18:14:13 |     image_mode: raw\n18:14:13 |     image_size: 256\n18:14:13 |     inference: max\n18:14:13 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:14:13 |     init_opt: None\n18:14:13 |     interactive_candidates: fixed\n18:14:13 |     interactive_mode: False\n18:14:13 |     invsqrt_lr_decay_gamma: -1\n18:14:13 |     is_debug: False\n18:14:13 |     label_truncate: 72\n18:14:13 |     learn_embeddings: True\n18:14:13 |     learn_positional_embeddings: True\n18:14:13 |     learningrate: 5e-05\n18:14:13 |     load_from_pretrained_ranker: True\n18:14:13 |     log_every_n_secs: 10.0\n18:14:13 |     log_every_n_steps: 50\n18:14:13 |     log_keep_fields: all\n18:14:13 |     loglevel: info\n18:14:13 |     lr_scheduler: reduceonplateau\n18:14:13 |     lr_scheduler_decay: 0.5\n18:14:13 |     lr_scheduler_patience: 3\n18:14:13 |     max_train_steps: -1\n18:14:13 |     max_train_time: 7200.0\n18:14:13 |     memory_attention: sqrt\n18:14:13 |     metrics: default\n18:14:13 |     model: transformer/classifier\n18:14:13 |     model_file: /tmp/model5\n18:14:13 |     model_parallel: False\n18:14:13 |     momentum: 0\n18:14:13 |     multitask_weights: [1]\n18:14:13 |     mutators: None\n18:14:13 |     n_decoder_layers: -1\n18:14:13 |     n_encoder_layers: -1\n18:14:13 |     n_heads: 12\n18:14:13 |     n_layers: 12\n18:14:13 |     n_positions: 1024\n18:14:13 |     n_segments: 2\n18:14:13 |     nesterov: True\n18:14:13 |     no_cuda: False\n18:14:13 |     normalize_sent_emb: False\n18:14:13 |     num_epochs: -1\n18:14:13 |     num_examples: -1\n18:14:13 |     num_workers: 0\n18:14:13 |     nus: [0.7]\n18:14:13 |     optimizer: adamax\n18:14:13 |     output_scaling: 0.06\n18:14:13 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n18:14:13 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:14:13 |     person_tokens: False\n18:14:13 |     print_scores: False\n18:14:13 |     rank_candidates: False\n18:14:13 |     rank_top_k: -1\n18:14:13 |     reduction_type: mean\n18:14:13 |     ref_class: None\n18:14:13 |     relu_dropout: 0.0\n18:14:13 |     repeat_blocking_heuristic: True\n18:14:13 |     report_filename: \n18:14:13 |     return_cand_scores: False\n18:14:13 |     save_after_valid: True\n18:14:13 |     save_every_n_secs: -1\n18:14:13 |     save_format: conversations\n18:14:13 |     share_encoders: False\n18:14:13 |     share_word_embeddings: False\n18:14:13 |     short_final_eval: False\n18:14:13 |     special_tok_lst: None\n18:14:13 |     split_lines: False\n18:14:13 |     starttime: Dec03_18-13\n18:14:13 |     task: fromfile:parlaiformat\n18:14:13 |     tensorboard_log: False\n18:14:13 |     tensorboard_logdir: None\n18:14:13 |     text_truncate: 360\n18:14:13 |     threshold: 0.5\n18:14:13 |     topk: 5\n18:14:13 |     train_predict: False\n18:14:13 |     truncate: 1024\n18:14:13 |     update_classifier_head_only: False\n18:14:13 |     update_freq: 1\n18:14:13 |     use_memories: False\n18:14:13 |     use_reply: none\n18:14:13 |     validation_cutoff: 1.0\n18:14:13 |     validation_every_n_epochs: -1\n18:14:13 |     validation_every_n_secs: 20.0\n18:14:13 |     validation_every_n_steps: -1\n18:14:13 |     validation_max_exs: -1\n18:14:13 |     validation_metric: accuracy\n18:14:13 |     validation_metric_mode: max\n18:14:13 |     validation_patience: 30\n18:14:13 |     validation_share_agent: False\n18:14:13 |     variant: xlm\n18:14:13 |     verbose: False\n18:14:13 |     wandb_entity: None\n18:14:13 |     wandb_log: False\n18:14:13 |     wandb_name: None\n18:14:13 |     wandb_project: None\n18:14:13 |     warmup_rate: 0.0001\n18:14:13 |     warmup_updates: 1000\n18:14:13 |     weight_decay: None\n18:14:13 |     world_logs: \n18:14:13 |     wrap_memory_encoder: False\n18:14:13 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:14:13 | creating task(s): fromfile:parlaiformat\n18:14:13 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:14:13 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-a.txt\n18:14:19 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3050 3.05e-10               .3822                 .3413   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4343            .2057              .2432   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1782 11.79 551.8 495.4       0          0 35.91  200 .3050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .7504 2.855e-06 239.6 215.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 791.4 710.5        .2931\u001b[0m\n18:14:19 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .3050 3.05e-10               .3822                 .3413   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4343            .2057              .2432   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1782 11.79 551.8 495.4       0          0 35.91  200 .3050   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .7504 2.855e-06 239.6 215.1       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 791.4 710.5        .2931\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:14:21.019235Z","iopub.execute_input":"2022-12-03T18:14:21.019659Z","iopub.status.idle":"2022-12-03T18:14:47.678760Z","shell.execute_reply.started":"2022-12-03T18:14:21.019618Z","shell.execute_reply":"2022-12-03T18:14:47.677547Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"18:14:28 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_valid.txt)\u001b[0m\n18:14:28 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:14:28 | Using CUDA\n18:14:28 | loading dictionary from /tmp/model5.dict\n18:14:28 | num words = 54944\n18:14:32 | Loading existing model parameters from /tmp/model5\n18:14:38 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:14:39 | Opt:\n18:14:39 |     activation: gelu\n18:14:39 |     adafactor_eps: '[1e-30, 0.001]'\n18:14:39 |     adam_eps: 1e-08\n18:14:39 |     add_p1_after_newln: False\n18:14:39 |     aggregate_micro: False\n18:14:39 |     allow_missing_init_opts: False\n18:14:39 |     area_under_curve_class: None\n18:14:39 |     area_under_curve_digits: -1\n18:14:39 |     attention_dropout: 0.1\n18:14:39 |     batchsize: 40\n18:14:39 |     betas: '[0.9, 0.999]'\n18:14:39 |     bpe_add_prefix_space: None\n18:14:39 |     bpe_debug: False\n18:14:39 |     bpe_dropout: None\n18:14:39 |     bpe_merge: None\n18:14:39 |     bpe_vocab: None\n18:14:39 |     candidates: inline\n18:14:39 |     cap_num_predictions: 100\n18:14:39 |     checkpoint_activations: False\n18:14:39 |     class_weights: None\n18:14:39 |     classes: \"['__notok__', '__ok__']\"\n18:14:39 |     classes_from_file: None\n18:14:39 |     data_parallel: True\n18:14:39 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:14:39 |     datatype: train\n18:14:39 |     delimiter: '\\n'\n18:14:39 |     dict_class: parlai.core.dict:DictionaryAgent\n18:14:39 |     dict_endtoken: __start__\n18:14:39 |     dict_file: /tmp/model5.dict\n18:14:39 |     dict_include_test: False\n18:14:39 |     dict_include_valid: False\n18:14:39 |     dict_initpath: None\n18:14:39 |     dict_language: english\n18:14:39 |     dict_loaded: True\n18:14:39 |     dict_lower: True\n18:14:39 |     dict_max_ngram_size: -1\n18:14:39 |     dict_maxexs: -1\n18:14:39 |     dict_maxtokens: -1\n18:14:39 |     dict_minfreq: 0\n18:14:39 |     dict_nulltoken: __null__\n18:14:39 |     dict_starttoken: __start__\n18:14:39 |     dict_textfields: text,labels\n18:14:39 |     dict_tokenizer: bpe\n18:14:39 |     dict_unktoken: __unk__\n18:14:39 |     display_examples: False\n18:14:39 |     download_path: None\n18:14:39 |     dropout: 0.1\n18:14:39 |     dynamic_batching: None\n18:14:39 |     embedding_projection: random\n18:14:39 |     embedding_size: 768\n18:14:39 |     embedding_type: random\n18:14:39 |     embeddings_scale: False\n18:14:39 |     encode_candidate_vecs: True\n18:14:39 |     encode_candidate_vecs_batchsize: 256\n18:14:39 |     eval_batchsize: None\n18:14:39 |     eval_candidates: inline\n18:14:39 |     eval_dynamic_batching: None\n18:14:39 |     evaltask: None\n18:14:39 |     ffn_size: 3072\n18:14:39 |     final_extra_opt: \n18:14:39 |     fixed_candidate_vecs: reuse\n18:14:39 |     fixed_candidates_path: None\n18:14:39 |     force_fp16_tokens: True\n18:14:39 |     fp16: True\n18:14:39 |     fp16_impl: safe\n18:14:39 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-b.txt\n18:14:39 |     fromfile_datatype_extension: False\n18:14:39 |     gpu: -1\n18:14:39 |     gradient_clip: 0.1\n18:14:39 |     hide_labels: False\n18:14:39 |     history_add_global_end_token: None\n18:14:39 |     history_reversed: False\n18:14:39 |     history_size: 20\n18:14:39 |     ignore_bad_candidates: False\n18:14:39 |     ignore_labels: None\n18:14:39 |     image_cropsize: 224\n18:14:39 |     image_mode: raw\n18:14:39 |     image_size: 256\n18:14:39 |     inference: max\n18:14:39 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:14:39 |     init_opt: None\n18:14:39 |     interactive_candidates: fixed\n18:14:39 |     interactive_mode: False\n18:14:39 |     invsqrt_lr_decay_gamma: -1\n18:14:39 |     is_debug: False\n18:14:39 |     label_truncate: 72\n18:14:39 |     learn_embeddings: True\n18:14:39 |     learn_positional_embeddings: True\n18:14:39 |     learningrate: 5e-05\n18:14:39 |     load_from_pretrained_ranker: True\n18:14:39 |     log_every_n_secs: 10.0\n18:14:39 |     log_every_n_steps: 50\n18:14:39 |     log_keep_fields: all\n18:14:39 |     loglevel: info\n18:14:39 |     lr_scheduler: reduceonplateau\n18:14:39 |     lr_scheduler_decay: 0.5\n18:14:39 |     lr_scheduler_patience: 3\n18:14:39 |     max_train_steps: -1\n18:14:39 |     max_train_time: 7200.0\n18:14:39 |     memory_attention: sqrt\n18:14:39 |     metrics: default\n18:14:39 |     model: transformer/classifier\n18:14:39 |     model_file: /tmp/model5\n18:14:39 |     model_parallel: False\n18:14:39 |     momentum: 0\n18:14:39 |     multitask_weights: [1]\n18:14:39 |     mutators: None\n18:14:39 |     n_decoder_layers: -1\n18:14:39 |     n_encoder_layers: -1\n18:14:39 |     n_heads: 12\n18:14:39 |     n_layers: 12\n18:14:39 |     n_positions: 1024\n18:14:39 |     n_segments: 2\n18:14:39 |     nesterov: True\n18:14:39 |     no_cuda: False\n18:14:39 |     normalize_sent_emb: False\n18:14:39 |     num_epochs: -1\n18:14:39 |     num_examples: -1\n18:14:39 |     num_workers: 0\n18:14:39 |     nus: [0.7]\n18:14:39 |     optimizer: adamax\n18:14:39 |     output_scaling: 0.06\n18:14:39 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n18:14:39 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:14:39 |     person_tokens: False\n18:14:39 |     print_scores: False\n18:14:39 |     rank_candidates: False\n18:14:39 |     rank_top_k: -1\n18:14:39 |     reduction_type: mean\n18:14:39 |     ref_class: None\n18:14:39 |     relu_dropout: 0.0\n18:14:39 |     repeat_blocking_heuristic: True\n18:14:39 |     report_filename: \n18:14:39 |     return_cand_scores: False\n18:14:39 |     save_after_valid: True\n18:14:39 |     save_every_n_secs: -1\n18:14:39 |     save_format: conversations\n18:14:39 |     share_encoders: False\n18:14:39 |     share_word_embeddings: False\n18:14:39 |     short_final_eval: False\n18:14:39 |     special_tok_lst: None\n18:14:39 |     split_lines: False\n18:14:39 |     starttime: Dec03_18-13\n18:14:39 |     task: fromfile:parlaiformat\n18:14:39 |     tensorboard_log: False\n18:14:39 |     tensorboard_logdir: None\n18:14:39 |     text_truncate: 360\n18:14:39 |     threshold: 0.5\n18:14:39 |     topk: 5\n18:14:39 |     train_predict: False\n18:14:39 |     truncate: 1024\n18:14:39 |     update_classifier_head_only: False\n18:14:39 |     update_freq: 1\n18:14:39 |     use_memories: False\n18:14:39 |     use_reply: none\n18:14:39 |     validation_cutoff: 1.0\n18:14:39 |     validation_every_n_epochs: -1\n18:14:39 |     validation_every_n_secs: 20.0\n18:14:39 |     validation_every_n_steps: -1\n18:14:39 |     validation_max_exs: -1\n18:14:39 |     validation_metric: accuracy\n18:14:39 |     validation_metric_mode: max\n18:14:39 |     validation_patience: 30\n18:14:39 |     validation_share_agent: False\n18:14:39 |     variant: xlm\n18:14:39 |     verbose: False\n18:14:39 |     wandb_entity: None\n18:14:39 |     wandb_log: False\n18:14:39 |     wandb_name: None\n18:14:39 |     wandb_project: None\n18:14:39 |     warmup_rate: 0.0001\n18:14:39 |     warmup_updates: 1000\n18:14:39 |     weight_decay: None\n18:14:39 |     world_logs: \n18:14:39 |     wrap_memory_encoder: False\n18:14:40 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:14:40 | creating task(s): fromfile:parlaiformat\n18:14:40 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:14:40 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type1/run5/data_train-b.txt\n18:14:46 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6950 6.95e-10               .7313                 .6587   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8218            .6474              .7568   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5657 11.79 551.8 506.3       0          0  36.7  200 .6950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .6453 2.855e-06 240.4 220.6       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 792.2 726.9        .6898\u001b[0m\n18:14:46 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .6950 6.95e-10               .7313                 .6587   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8218            .6474              .7568   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .5657 11.79 551.8 506.3       0          0  36.7  200 .6950   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .6453 2.855e-06 240.4 220.6       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 792.2 726.9        .6898\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:14:47.681077Z","iopub.execute_input":"2022-12-03T18:14:47.681470Z","iopub.status.idle":"2022-12-03T18:14:48.786435Z","shell.execute_reply.started":"2022-12-03T18:14:47.681430Z","shell.execute_reply":"2022-12-03T18:14:48.785122Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"Choose completion prev2corr2type2","metadata":{}},{"cell_type":"markdown","source":"run 1","metadata":{}},{"cell_type":"code","source":"# run 1 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model1","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:14:48.788696Z","iopub.execute_input":"2022-12-03T18:14:48.789095Z","iopub.status.idle":"2022-12-03T18:16:10.109843Z","shell.execute_reply.started":"2022-12-03T18:14:48.789056Z","shell.execute_reply":"2022-12-03T18:16:10.108616Z"},"trusted":true},"execution_count":147,"outputs":[{"name":"stdout","text":"18:14:55 | building dictionary first...\n18:14:55 | No model with opt yet at: /tmp/model1(.opt)\n18:14:55 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:14:55 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:14:55 | Using CUDA\n18:14:55 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:14:56 | num words = 54944\n18:15:00 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:15:10 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:15:10 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:15:10 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:15:10 | Opt:\n18:15:10 |     activation: gelu\n18:15:10 |     adafactor_eps: '(1e-30, 0.001)'\n18:15:10 |     adam_eps: 1e-08\n18:15:10 |     add_p1_after_newln: False\n18:15:10 |     aggregate_micro: False\n18:15:10 |     allow_missing_init_opts: False\n18:15:10 |     attention_dropout: 0.1\n18:15:10 |     batchsize: 20\n18:15:10 |     betas: '(0.9, 0.999)'\n18:15:10 |     bpe_add_prefix_space: None\n18:15:10 |     bpe_debug: False\n18:15:10 |     bpe_dropout: None\n18:15:10 |     bpe_merge: None\n18:15:10 |     bpe_vocab: None\n18:15:10 |     candidates: inline\n18:15:10 |     cap_num_predictions: 100\n18:15:10 |     checkpoint_activations: False\n18:15:10 |     class_weights: None\n18:15:10 |     classes: \"['__notok__', '__ok__']\"\n18:15:10 |     classes_from_file: None\n18:15:10 |     data_parallel: True\n18:15:10 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:15:10 |     datatype: train\n18:15:10 |     delimiter: '\\n'\n18:15:10 |     dict_class: parlai.core.dict:DictionaryAgent\n18:15:10 |     dict_endtoken: __start__\n18:15:10 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:15:10 |     dict_include_test: False\n18:15:10 |     dict_include_valid: False\n18:15:10 |     dict_initpath: None\n18:15:10 |     dict_language: english\n18:15:10 |     dict_loaded: True\n18:15:10 |     dict_lower: True\n18:15:10 |     dict_max_ngram_size: -1\n18:15:10 |     dict_maxexs: -1\n18:15:10 |     dict_maxtokens: -1\n18:15:10 |     dict_minfreq: 0\n18:15:10 |     dict_nulltoken: __null__\n18:15:10 |     dict_starttoken: __start__\n18:15:10 |     dict_textfields: text,labels\n18:15:10 |     dict_tokenizer: bpe\n18:15:10 |     dict_unktoken: __unk__\n18:15:10 |     display_examples: False\n18:15:10 |     download_path: None\n18:15:10 |     dropout: 0.1\n18:15:10 |     dynamic_batching: None\n18:15:10 |     embedding_projection: random\n18:15:10 |     embedding_size: 768\n18:15:10 |     embedding_type: random\n18:15:10 |     embeddings_scale: False\n18:15:10 |     encode_candidate_vecs: True\n18:15:10 |     encode_candidate_vecs_batchsize: 256\n18:15:10 |     eval_batchsize: None\n18:15:10 |     eval_candidates: inline\n18:15:10 |     eval_dynamic_batching: None\n18:15:10 |     evaltask: None\n18:15:10 |     ffn_size: 3072\n18:15:10 |     final_extra_opt: \n18:15:10 |     fixed_candidate_vecs: reuse\n18:15:10 |     fixed_candidates_path: None\n18:15:10 |     force_fp16_tokens: False\n18:15:10 |     fp16: True\n18:15:10 |     fp16_impl: safe\n18:15:10 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt\n18:15:10 |     fromfile_datatype_extension: False\n18:15:10 |     gpu: -1\n18:15:10 |     gradient_clip: 0.1\n18:15:10 |     hide_labels: False\n18:15:10 |     history_add_global_end_token: None\n18:15:10 |     history_reversed: False\n18:15:10 |     history_size: 20\n18:15:10 |     ignore_bad_candidates: False\n18:15:10 |     ignore_labels: None\n18:15:10 |     image_cropsize: 224\n18:15:10 |     image_mode: raw\n18:15:10 |     image_size: 256\n18:15:10 |     inference: max\n18:15:10 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:15:10 |     init_opt: None\n18:15:10 |     interactive_candidates: fixed\n18:15:10 |     interactive_mode: False\n18:15:10 |     invsqrt_lr_decay_gamma: -1\n18:15:10 |     is_debug: False\n18:15:10 |     label_truncate: 72\n18:15:10 |     learn_embeddings: True\n18:15:10 |     learn_positional_embeddings: True\n18:15:10 |     learningrate: 5e-05\n18:15:10 |     load_from_checkpoint: False\n18:15:10 |     load_from_pretrained_ranker: True\n18:15:10 |     log_every_n_secs: 10.0\n18:15:10 |     log_every_n_steps: 50\n18:15:10 |     log_keep_fields: all\n18:15:10 |     loglevel: info\n18:15:10 |     lr_scheduler: reduceonplateau\n18:15:10 |     lr_scheduler_decay: 0.5\n18:15:10 |     lr_scheduler_patience: 3\n18:15:10 |     max_train_steps: -1\n18:15:10 |     max_train_time: 7200.0\n18:15:10 |     memory_attention: sqrt\n18:15:10 |     metrics: default\n18:15:10 |     model: transformer/classifier\n18:15:10 |     model_file: /tmp/model1\n18:15:10 |     model_parallel: False\n18:15:10 |     momentum: 0\n18:15:10 |     multitask_weights: [1]\n18:15:10 |     mutators: None\n18:15:10 |     n_decoder_layers: -1\n18:15:10 |     n_encoder_layers: -1\n18:15:10 |     n_heads: 12\n18:15:10 |     n_layers: 12\n18:15:10 |     n_positions: 1024\n18:15:10 |     n_segments: 2\n18:15:10 |     nesterov: True\n18:15:10 |     no_cuda: False\n18:15:10 |     normalize_sent_emb: False\n18:15:10 |     num_epochs: -1\n18:15:10 |     num_workers: 0\n18:15:10 |     nus: (0.7,)\n18:15:10 |     optimizer: adamax\n18:15:10 |     output_scaling: 0.06\n18:15:10 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model1'}\"\n18:15:10 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:15:10 |     person_tokens: False\n18:15:10 |     print_scores: False\n18:15:10 |     rank_candidates: False\n18:15:10 |     rank_top_k: -1\n18:15:10 |     reduction_type: mean\n18:15:10 |     ref_class: None\n18:15:10 |     relu_dropout: 0.0\n18:15:10 |     repeat_blocking_heuristic: True\n18:15:10 |     return_cand_scores: False\n18:15:10 |     save_after_valid: True\n18:15:10 |     save_every_n_secs: -1\n18:15:10 |     save_format: conversations\n18:15:10 |     share_encoders: False\n18:15:10 |     share_word_embeddings: False\n18:15:10 |     short_final_eval: False\n18:15:10 |     special_tok_lst: None\n18:15:10 |     split_lines: False\n18:15:10 |     starttime: Dec03_18-14\n18:15:10 |     task: fromfile:parlaiformat\n18:15:10 |     tensorboard_log: False\n18:15:10 |     tensorboard_logdir: None\n18:15:10 |     text_truncate: 360\n18:15:10 |     threshold: 0.5\n18:15:10 |     topk: 5\n18:15:10 |     train_predict: False\n18:15:10 |     truncate: 1024\n18:15:10 |     update_classifier_head_only: False\n18:15:10 |     update_freq: 1\n18:15:10 |     use_memories: False\n18:15:10 |     use_reply: none\n18:15:10 |     validation_cutoff: 1.0\n18:15:10 |     validation_every_n_epochs: -1\n18:15:10 |     validation_every_n_secs: 20.0\n18:15:10 |     validation_every_n_steps: -1\n18:15:10 |     validation_max_exs: -1\n18:15:10 |     validation_metric: accuracy\n18:15:10 |     validation_metric_mode: max\n18:15:10 |     validation_patience: 30\n18:15:10 |     validation_share_agent: False\n18:15:10 |     variant: xlm\n18:15:10 |     verbose: False\n18:15:10 |     wandb_entity: None\n18:15:10 |     wandb_log: False\n18:15:10 |     wandb_name: None\n18:15:10 |     wandb_project: None\n18:15:10 |     warmup_rate: 0.0001\n18:15:10 |     warmup_updates: 1000\n18:15:10 |     weight_decay: None\n18:15:10 |     world_logs: \n18:15:10 |     wrap_memory_encoder: False\n18:15:11 | creating task(s): fromfile:parlaiformat\n18:15:11 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt\n18:15:11 | training...\n18:15:21 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5762 5.762e-10               .5550                 .5722   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .5388            .5955              .5796   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6121  11.6     1 272.1 561.3       0          0 41.26  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5762             32768  2.441    .1189 5.981 .6871 1.055e-06 119.6 246.8   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 391.7 808.1 2.068        .5756\n\n18:15:31 | time:20s total_exs:1180 total_steps:59 epochs:49.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7224 7.224e-10               .7106                 .7155   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7057            .7332              .7286   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7379 11.37     1 267.5  1038       0          0 77.63  760   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7224             32768  2.712    .1189 5.966 .6473 2.955e-06 119.3 463.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   59 386.8 1501 3.891        .7223\n\n18:15:31 | creating task(s): fromfile:parlaiformat\n18:15:31 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:15:31 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt\n18:15:31 | running eval: valid\n18:15:31 | eval completed in 0.19s\n18:15:31 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9167 9.167e-10               .9167                 .9167   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9167            .9167              .9167   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 11.46 161.5  1893       0          0 140.6   24 .9167   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5912 2.955e-06    72 843.7       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     59 233.5 2737        .9167\n\u001b[0m\n18:15:31 | \u001b[1;32mnew best accuracy: 0.9167\u001b[0m\n18:15:31 | saving best valid model: /tmp/model1\n18:15:31 | Saving dictionary to /tmp/model1.dict\n18:15:34 | saving model checkpoint: /tmp/model1.checkpoint\n18:15:34 | Saving dictionary to /tmp/model1.checkpoint.dict\n18:15:52 | time:41s total_exs:1920 total_steps:96 epochs:80.00\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9432 9.432e-10               .9400                 .9792   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9038            .9462              .9134   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9814 11.43     1 268.5 971.5       0          0 72.36  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9432             32768  2.852    .1189 5.984 .5465 4.805e-06 119.7   433   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   96 388.2 1405 3.626        .9431\n\n18:15:55 | time:44s total_exs:2140 total_steps:107 epochs:89.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9864 9.864e-10               .9854                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9712            .9872              .9748   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.36     1 267.2  1067       0          0 79.86  220   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9864             32768  3.044    .1189 5.945 .4281 5.354e-06 118.9 474.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  107 386.1 1542 4.026        .9864\n\n18:15:55 | running eval: valid\n18:15:55 | eval completed in 0.19s\n18:15:55 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1846       0          0 137.1   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .3531 5.354e-06    72 822.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    107 233.5 2669            1\n\u001b[0m\n18:15:55 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9167)\u001b[0m\n18:15:55 | saving best valid model: /tmp/model1\n18:15:59 | task solved! stopping.\n18:15:59 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:15:59 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:15:59 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:15:59 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:15:59 | Using CUDA\n18:15:59 | loading dictionary from /tmp/model1.dict\n18:16:00 | num words = 54944\n18:16:04 | Loading existing model parameters from /tmp/model1\n18:16:06 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:16:07 | creating task(s): fromfile:parlaiformat\n18:16:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:16:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt\n18:16:07 | running eval: valid\n18:16:08 | eval completed in 0.24s\n18:16:08 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1533       0          0 113.8   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3531 5.354e-06    72 683.3       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    107 233.5 2216            1\n\u001b[0m\n18:16:08 | creating task(s): fromfile:parlaiformat\n18:16:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:16:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt\n18:16:08 | running eval: test\n18:16:08 | eval completed in 0.23s\n18:16:08 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.46 161.5  1564       0          0 116.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .3531 5.354e-06    72 697.2       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    107 233.5 2261            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-a.txt -m transformer/classifier -mf /tmp/model1 -bs 40\n","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:16:10.113418Z","iopub.execute_input":"2022-12-03T18:16:10.113844Z","iopub.status.idle":"2022-12-03T18:16:37.674691Z","shell.execute_reply.started":"2022-12-03T18:16:10.113803Z","shell.execute_reply":"2022-12-03T18:16:37.673434Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"18:16:17 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt)\u001b[0m\n18:16:17 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:16:17 | Using CUDA\n18:16:17 | loading dictionary from /tmp/model1.dict\n18:16:17 | num words = 54944\n18:16:21 | Loading existing model parameters from /tmp/model1\n18:16:28 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:16:30 | Opt:\n18:16:30 |     activation: gelu\n18:16:30 |     adafactor_eps: '[1e-30, 0.001]'\n18:16:30 |     adam_eps: 1e-08\n18:16:30 |     add_p1_after_newln: False\n18:16:30 |     aggregate_micro: False\n18:16:30 |     allow_missing_init_opts: False\n18:16:30 |     area_under_curve_class: None\n18:16:30 |     area_under_curve_digits: -1\n18:16:30 |     attention_dropout: 0.1\n18:16:30 |     batchsize: 40\n18:16:30 |     betas: '[0.9, 0.999]'\n18:16:30 |     bpe_add_prefix_space: None\n18:16:30 |     bpe_debug: False\n18:16:30 |     bpe_dropout: None\n18:16:30 |     bpe_merge: None\n18:16:30 |     bpe_vocab: None\n18:16:30 |     candidates: inline\n18:16:30 |     cap_num_predictions: 100\n18:16:30 |     checkpoint_activations: False\n18:16:30 |     class_weights: None\n18:16:30 |     classes: \"['__notok__', '__ok__']\"\n18:16:30 |     classes_from_file: None\n18:16:30 |     data_parallel: True\n18:16:30 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:16:30 |     datatype: train\n18:16:30 |     delimiter: '\\n'\n18:16:30 |     dict_class: parlai.core.dict:DictionaryAgent\n18:16:30 |     dict_endtoken: __start__\n18:16:30 |     dict_file: /tmp/model1.dict\n18:16:30 |     dict_include_test: False\n18:16:30 |     dict_include_valid: False\n18:16:30 |     dict_initpath: None\n18:16:30 |     dict_language: english\n18:16:30 |     dict_loaded: True\n18:16:30 |     dict_lower: True\n18:16:30 |     dict_max_ngram_size: -1\n18:16:30 |     dict_maxexs: -1\n18:16:30 |     dict_maxtokens: -1\n18:16:30 |     dict_minfreq: 0\n18:16:30 |     dict_nulltoken: __null__\n18:16:30 |     dict_starttoken: __start__\n18:16:30 |     dict_textfields: text,labels\n18:16:30 |     dict_tokenizer: bpe\n18:16:30 |     dict_unktoken: __unk__\n18:16:30 |     display_examples: False\n18:16:30 |     download_path: None\n18:16:30 |     dropout: 0.1\n18:16:30 |     dynamic_batching: None\n18:16:30 |     embedding_projection: random\n18:16:30 |     embedding_size: 768\n18:16:30 |     embedding_type: random\n18:16:30 |     embeddings_scale: False\n18:16:30 |     encode_candidate_vecs: True\n18:16:30 |     encode_candidate_vecs_batchsize: 256\n18:16:30 |     eval_batchsize: None\n18:16:30 |     eval_candidates: inline\n18:16:30 |     eval_dynamic_batching: None\n18:16:30 |     evaltask: None\n18:16:30 |     ffn_size: 3072\n18:16:30 |     final_extra_opt: \n18:16:30 |     fixed_candidate_vecs: reuse\n18:16:30 |     fixed_candidates_path: None\n18:16:30 |     force_fp16_tokens: True\n18:16:30 |     fp16: True\n18:16:30 |     fp16_impl: safe\n18:16:30 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-a.txt\n18:16:30 |     fromfile_datatype_extension: False\n18:16:30 |     gpu: -1\n18:16:30 |     gradient_clip: 0.1\n18:16:30 |     hide_labels: False\n18:16:30 |     history_add_global_end_token: None\n18:16:30 |     history_reversed: False\n18:16:30 |     history_size: 20\n18:16:30 |     ignore_bad_candidates: False\n18:16:30 |     ignore_labels: None\n18:16:30 |     image_cropsize: 224\n18:16:30 |     image_mode: raw\n18:16:30 |     image_size: 256\n18:16:30 |     inference: max\n18:16:30 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:16:30 |     init_opt: None\n18:16:30 |     interactive_candidates: fixed\n18:16:30 |     interactive_mode: False\n18:16:30 |     invsqrt_lr_decay_gamma: -1\n18:16:30 |     is_debug: False\n18:16:30 |     label_truncate: 72\n18:16:30 |     learn_embeddings: True\n18:16:30 |     learn_positional_embeddings: True\n18:16:30 |     learningrate: 5e-05\n18:16:30 |     load_from_pretrained_ranker: True\n18:16:30 |     log_every_n_secs: 10.0\n18:16:30 |     log_every_n_steps: 50\n18:16:30 |     log_keep_fields: all\n18:16:30 |     loglevel: info\n18:16:30 |     lr_scheduler: reduceonplateau\n18:16:30 |     lr_scheduler_decay: 0.5\n18:16:30 |     lr_scheduler_patience: 3\n18:16:30 |     max_train_steps: -1\n18:16:30 |     max_train_time: 7200.0\n18:16:30 |     memory_attention: sqrt\n18:16:30 |     metrics: default\n18:16:30 |     model: transformer/classifier\n18:16:30 |     model_file: /tmp/model1\n18:16:30 |     model_parallel: False\n18:16:30 |     momentum: 0\n18:16:30 |     multitask_weights: [1]\n18:16:30 |     mutators: None\n18:16:30 |     n_decoder_layers: -1\n18:16:30 |     n_encoder_layers: -1\n18:16:30 |     n_heads: 12\n18:16:30 |     n_layers: 12\n18:16:30 |     n_positions: 1024\n18:16:30 |     n_segments: 2\n18:16:30 |     nesterov: True\n18:16:30 |     no_cuda: False\n18:16:30 |     normalize_sent_emb: False\n18:16:30 |     num_epochs: -1\n18:16:30 |     num_examples: -1\n18:16:30 |     num_workers: 0\n18:16:30 |     nus: [0.7]\n18:16:30 |     optimizer: adamax\n18:16:30 |     output_scaling: 0.06\n18:16:30 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n18:16:30 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:16:30 |     person_tokens: False\n18:16:30 |     print_scores: False\n18:16:30 |     rank_candidates: False\n18:16:30 |     rank_top_k: -1\n18:16:30 |     reduction_type: mean\n18:16:30 |     ref_class: None\n18:16:30 |     relu_dropout: 0.0\n18:16:30 |     repeat_blocking_heuristic: True\n18:16:30 |     report_filename: \n18:16:30 |     return_cand_scores: False\n18:16:30 |     save_after_valid: True\n18:16:30 |     save_every_n_secs: -1\n18:16:30 |     save_format: conversations\n18:16:30 |     share_encoders: False\n18:16:30 |     share_word_embeddings: False\n18:16:30 |     short_final_eval: False\n18:16:30 |     special_tok_lst: None\n18:16:30 |     split_lines: False\n18:16:30 |     starttime: Dec03_18-14\n18:16:30 |     task: fromfile:parlaiformat\n18:16:30 |     tensorboard_log: False\n18:16:30 |     tensorboard_logdir: None\n18:16:30 |     text_truncate: 360\n18:16:30 |     threshold: 0.5\n18:16:30 |     topk: 5\n18:16:30 |     train_predict: False\n18:16:30 |     truncate: 1024\n18:16:30 |     update_classifier_head_only: False\n18:16:30 |     update_freq: 1\n18:16:30 |     use_memories: False\n18:16:30 |     use_reply: none\n18:16:30 |     validation_cutoff: 1.0\n18:16:30 |     validation_every_n_epochs: -1\n18:16:30 |     validation_every_n_secs: 20.0\n18:16:30 |     validation_every_n_steps: -1\n18:16:30 |     validation_max_exs: -1\n18:16:30 |     validation_metric: accuracy\n18:16:30 |     validation_metric_mode: max\n18:16:30 |     validation_patience: 30\n18:16:30 |     validation_share_agent: False\n18:16:30 |     variant: xlm\n18:16:30 |     verbose: False\n18:16:30 |     wandb_entity: None\n18:16:30 |     wandb_log: False\n18:16:30 |     wandb_name: None\n18:16:30 |     wandb_project: None\n18:16:30 |     warmup_rate: 0.0001\n18:16:30 |     warmup_updates: 1000\n18:16:30 |     weight_decay: None\n18:16:30 |     world_logs: \n18:16:30 |     wrap_memory_encoder: False\n18:16:30 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:16:30 | creating task(s): fromfile:parlaiformat\n18:16:30 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:16:30 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-a.txt\n18:16:36 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1000   1e-10               .0625                .06522   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .0600            .1346              .1296   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1400 11.13 525.2 488.5       0          0  37.2  200 .1000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .8715 5.354e-06   240 223.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 765.2 711.7       .09856\u001b[0m\n18:16:36 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1000   1e-10               .0625                .06522   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .0600            .1346              .1296   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1400 11.13 525.2 488.5       0          0  37.2  200 .1000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .8715 5.354e-06   240 223.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 765.2 711.7       .09856\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-b.txt -m transformer/classifier -mf /tmp/model1 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:16:37.676629Z","iopub.execute_input":"2022-12-03T18:16:37.677041Z","iopub.status.idle":"2022-12-03T18:17:03.969559Z","shell.execute_reply.started":"2022-12-03T18:16:37.677000Z","shell.execute_reply":"2022-12-03T18:17:03.968163Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"18:16:45 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_valid.txt)\u001b[0m\n18:16:45 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:16:45 | Using CUDA\n18:16:45 | loading dictionary from /tmp/model1.dict\n18:16:45 | num words = 54944\n18:16:49 | Loading existing model parameters from /tmp/model1\n18:16:55 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:16:56 | Opt:\n18:16:56 |     activation: gelu\n18:16:56 |     adafactor_eps: '[1e-30, 0.001]'\n18:16:56 |     adam_eps: 1e-08\n18:16:56 |     add_p1_after_newln: False\n18:16:56 |     aggregate_micro: False\n18:16:56 |     allow_missing_init_opts: False\n18:16:56 |     area_under_curve_class: None\n18:16:56 |     area_under_curve_digits: -1\n18:16:56 |     attention_dropout: 0.1\n18:16:56 |     batchsize: 40\n18:16:56 |     betas: '[0.9, 0.999]'\n18:16:56 |     bpe_add_prefix_space: None\n18:16:56 |     bpe_debug: False\n18:16:56 |     bpe_dropout: None\n18:16:56 |     bpe_merge: None\n18:16:56 |     bpe_vocab: None\n18:16:56 |     candidates: inline\n18:16:56 |     cap_num_predictions: 100\n18:16:56 |     checkpoint_activations: False\n18:16:56 |     class_weights: None\n18:16:56 |     classes: \"['__notok__', '__ok__']\"\n18:16:56 |     classes_from_file: None\n18:16:56 |     data_parallel: True\n18:16:56 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:16:56 |     datatype: train\n18:16:56 |     delimiter: '\\n'\n18:16:56 |     dict_class: parlai.core.dict:DictionaryAgent\n18:16:56 |     dict_endtoken: __start__\n18:16:56 |     dict_file: /tmp/model1.dict\n18:16:56 |     dict_include_test: False\n18:16:56 |     dict_include_valid: False\n18:16:56 |     dict_initpath: None\n18:16:56 |     dict_language: english\n18:16:56 |     dict_loaded: True\n18:16:56 |     dict_lower: True\n18:16:56 |     dict_max_ngram_size: -1\n18:16:56 |     dict_maxexs: -1\n18:16:56 |     dict_maxtokens: -1\n18:16:56 |     dict_minfreq: 0\n18:16:56 |     dict_nulltoken: __null__\n18:16:56 |     dict_starttoken: __start__\n18:16:56 |     dict_textfields: text,labels\n18:16:56 |     dict_tokenizer: bpe\n18:16:56 |     dict_unktoken: __unk__\n18:16:56 |     display_examples: False\n18:16:56 |     download_path: None\n18:16:56 |     dropout: 0.1\n18:16:56 |     dynamic_batching: None\n18:16:56 |     embedding_projection: random\n18:16:56 |     embedding_size: 768\n18:16:56 |     embedding_type: random\n18:16:56 |     embeddings_scale: False\n18:16:56 |     encode_candidate_vecs: True\n18:16:56 |     encode_candidate_vecs_batchsize: 256\n18:16:56 |     eval_batchsize: None\n18:16:56 |     eval_candidates: inline\n18:16:56 |     eval_dynamic_batching: None\n18:16:56 |     evaltask: None\n18:16:56 |     ffn_size: 3072\n18:16:56 |     final_extra_opt: \n18:16:56 |     fixed_candidate_vecs: reuse\n18:16:56 |     fixed_candidates_path: None\n18:16:56 |     force_fp16_tokens: True\n18:16:56 |     fp16: True\n18:16:56 |     fp16_impl: safe\n18:16:56 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-b.txt\n18:16:56 |     fromfile_datatype_extension: False\n18:16:56 |     gpu: -1\n18:16:56 |     gradient_clip: 0.1\n18:16:56 |     hide_labels: False\n18:16:56 |     history_add_global_end_token: None\n18:16:56 |     history_reversed: False\n18:16:56 |     history_size: 20\n18:16:56 |     ignore_bad_candidates: False\n18:16:56 |     ignore_labels: None\n18:16:56 |     image_cropsize: 224\n18:16:56 |     image_mode: raw\n18:16:56 |     image_size: 256\n18:16:56 |     inference: max\n18:16:56 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:16:56 |     init_opt: None\n18:16:56 |     interactive_candidates: fixed\n18:16:56 |     interactive_mode: False\n18:16:56 |     invsqrt_lr_decay_gamma: -1\n18:16:56 |     is_debug: False\n18:16:56 |     label_truncate: 72\n18:16:56 |     learn_embeddings: True\n18:16:56 |     learn_positional_embeddings: True\n18:16:56 |     learningrate: 5e-05\n18:16:56 |     load_from_pretrained_ranker: True\n18:16:56 |     log_every_n_secs: 10.0\n18:16:56 |     log_every_n_steps: 50\n18:16:56 |     log_keep_fields: all\n18:16:56 |     loglevel: info\n18:16:56 |     lr_scheduler: reduceonplateau\n18:16:56 |     lr_scheduler_decay: 0.5\n18:16:56 |     lr_scheduler_patience: 3\n18:16:56 |     max_train_steps: -1\n18:16:56 |     max_train_time: 7200.0\n18:16:56 |     memory_attention: sqrt\n18:16:56 |     metrics: default\n18:16:56 |     model: transformer/classifier\n18:16:56 |     model_file: /tmp/model1\n18:16:56 |     model_parallel: False\n18:16:56 |     momentum: 0\n18:16:56 |     multitask_weights: [1]\n18:16:56 |     mutators: None\n18:16:56 |     n_decoder_layers: -1\n18:16:56 |     n_encoder_layers: -1\n18:16:56 |     n_heads: 12\n18:16:56 |     n_layers: 12\n18:16:56 |     n_positions: 1024\n18:16:56 |     n_segments: 2\n18:16:56 |     nesterov: True\n18:16:56 |     no_cuda: False\n18:16:56 |     normalize_sent_emb: False\n18:16:56 |     num_epochs: -1\n18:16:56 |     num_examples: -1\n18:16:56 |     num_workers: 0\n18:16:56 |     nus: [0.7]\n18:16:56 |     optimizer: adamax\n18:16:56 |     output_scaling: 0.06\n18:16:56 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model1', 'batchsize': 40}\"\n18:16:56 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:16:56 |     person_tokens: False\n18:16:56 |     print_scores: False\n18:16:56 |     rank_candidates: False\n18:16:56 |     rank_top_k: -1\n18:16:56 |     reduction_type: mean\n18:16:56 |     ref_class: None\n18:16:56 |     relu_dropout: 0.0\n18:16:56 |     repeat_blocking_heuristic: True\n18:16:56 |     report_filename: \n18:16:56 |     return_cand_scores: False\n18:16:56 |     save_after_valid: True\n18:16:56 |     save_every_n_secs: -1\n18:16:56 |     save_format: conversations\n18:16:56 |     share_encoders: False\n18:16:56 |     share_word_embeddings: False\n18:16:56 |     short_final_eval: False\n18:16:56 |     special_tok_lst: None\n18:16:56 |     split_lines: False\n18:16:56 |     starttime: Dec03_18-14\n18:16:56 |     task: fromfile:parlaiformat\n18:16:56 |     tensorboard_log: False\n18:16:56 |     tensorboard_logdir: None\n18:16:56 |     text_truncate: 360\n18:16:56 |     threshold: 0.5\n18:16:56 |     topk: 5\n18:16:56 |     train_predict: False\n18:16:56 |     truncate: 1024\n18:16:56 |     update_classifier_head_only: False\n18:16:56 |     update_freq: 1\n18:16:56 |     use_memories: False\n18:16:56 |     use_reply: none\n18:16:56 |     validation_cutoff: 1.0\n18:16:56 |     validation_every_n_epochs: -1\n18:16:56 |     validation_every_n_secs: 20.0\n18:16:56 |     validation_every_n_steps: -1\n18:16:56 |     validation_max_exs: -1\n18:16:56 |     validation_metric: accuracy\n18:16:56 |     validation_metric_mode: max\n18:16:56 |     validation_patience: 30\n18:16:56 |     validation_share_agent: False\n18:16:56 |     variant: xlm\n18:16:56 |     verbose: False\n18:16:56 |     wandb_entity: None\n18:16:56 |     wandb_log: False\n18:16:56 |     wandb_name: None\n18:16:56 |     wandb_project: None\n18:16:56 |     warmup_rate: 0.0001\n18:16:56 |     warmup_updates: 1000\n18:16:56 |     weight_decay: None\n18:16:56 |     world_logs: \n18:16:56 |     wrap_memory_encoder: False\n18:16:56 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:16:56 | creating task(s): fromfile:parlaiformat\n18:16:56 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:16:56 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run1/data_train-b.txt\n18:17:02 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .9000   9e-10               .8958                 .9348   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8600            .9038              .8704   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9400 11.13 525.2 505.9       0          0 38.53  200 .9000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .5577 5.354e-06   240 231.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 765.2 737.2        .8998\u001b[0m\n18:17:02 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .9000   9e-10               .8958                 .9348   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8600            .9038              .8704   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9400 11.13 525.2 505.9       0          0 38.53  200 .9000   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0824     6 .5577 5.354e-06   240 231.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 765.2 737.2        .8998\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model1*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:17:03.971994Z","iopub.execute_input":"2022-12-03T18:17:03.972387Z","iopub.status.idle":"2022-12-03T18:17:05.116018Z","shell.execute_reply.started":"2022-12-03T18:17:03.972344Z","shell.execute_reply":"2022-12-03T18:17:05.114658Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"markdown","source":"run 2","metadata":{}},{"cell_type":"code","source":"# run 2 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model2","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:17:05.119353Z","iopub.execute_input":"2022-12-03T18:17:05.119752Z","iopub.status.idle":"2022-12-03T18:18:22.359566Z","shell.execute_reply.started":"2022-12-03T18:17:05.119721Z","shell.execute_reply":"2022-12-03T18:18:22.358372Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"18:17:12 | building dictionary first...\n18:17:12 | No model with opt yet at: /tmp/model2(.opt)\n18:17:12 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:17:12 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:17:12 | Using CUDA\n18:17:12 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:17:12 | num words = 54944\n18:17:17 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:17:22 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:17:22 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:17:22 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:17:22 | Opt:\n18:17:22 |     activation: gelu\n18:17:22 |     adafactor_eps: '(1e-30, 0.001)'\n18:17:22 |     adam_eps: 1e-08\n18:17:22 |     add_p1_after_newln: False\n18:17:22 |     aggregate_micro: False\n18:17:22 |     allow_missing_init_opts: False\n18:17:22 |     attention_dropout: 0.1\n18:17:22 |     batchsize: 20\n18:17:22 |     betas: '(0.9, 0.999)'\n18:17:22 |     bpe_add_prefix_space: None\n18:17:22 |     bpe_debug: False\n18:17:22 |     bpe_dropout: None\n18:17:22 |     bpe_merge: None\n18:17:22 |     bpe_vocab: None\n18:17:22 |     candidates: inline\n18:17:22 |     cap_num_predictions: 100\n18:17:22 |     checkpoint_activations: False\n18:17:22 |     class_weights: None\n18:17:22 |     classes: \"['__notok__', '__ok__']\"\n18:17:22 |     classes_from_file: None\n18:17:22 |     data_parallel: True\n18:17:22 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:17:22 |     datatype: train\n18:17:22 |     delimiter: '\\n'\n18:17:22 |     dict_class: parlai.core.dict:DictionaryAgent\n18:17:22 |     dict_endtoken: __start__\n18:17:22 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:17:22 |     dict_include_test: False\n18:17:22 |     dict_include_valid: False\n18:17:22 |     dict_initpath: None\n18:17:22 |     dict_language: english\n18:17:22 |     dict_loaded: True\n18:17:22 |     dict_lower: True\n18:17:22 |     dict_max_ngram_size: -1\n18:17:22 |     dict_maxexs: -1\n18:17:22 |     dict_maxtokens: -1\n18:17:22 |     dict_minfreq: 0\n18:17:22 |     dict_nulltoken: __null__\n18:17:22 |     dict_starttoken: __start__\n18:17:22 |     dict_textfields: text,labels\n18:17:22 |     dict_tokenizer: bpe\n18:17:22 |     dict_unktoken: __unk__\n18:17:22 |     display_examples: False\n18:17:22 |     download_path: None\n18:17:22 |     dropout: 0.1\n18:17:22 |     dynamic_batching: None\n18:17:22 |     embedding_projection: random\n18:17:22 |     embedding_size: 768\n18:17:22 |     embedding_type: random\n18:17:22 |     embeddings_scale: False\n18:17:22 |     encode_candidate_vecs: True\n18:17:22 |     encode_candidate_vecs_batchsize: 256\n18:17:22 |     eval_batchsize: None\n18:17:22 |     eval_candidates: inline\n18:17:22 |     eval_dynamic_batching: None\n18:17:22 |     evaltask: None\n18:17:22 |     ffn_size: 3072\n18:17:22 |     final_extra_opt: \n18:17:22 |     fixed_candidate_vecs: reuse\n18:17:22 |     fixed_candidates_path: None\n18:17:22 |     force_fp16_tokens: False\n18:17:22 |     fp16: True\n18:17:22 |     fp16_impl: safe\n18:17:22 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt\n18:17:22 |     fromfile_datatype_extension: False\n18:17:22 |     gpu: -1\n18:17:22 |     gradient_clip: 0.1\n18:17:22 |     hide_labels: False\n18:17:22 |     history_add_global_end_token: None\n18:17:22 |     history_reversed: False\n18:17:22 |     history_size: 20\n18:17:22 |     ignore_bad_candidates: False\n18:17:22 |     ignore_labels: None\n18:17:22 |     image_cropsize: 224\n18:17:22 |     image_mode: raw\n18:17:22 |     image_size: 256\n18:17:22 |     inference: max\n18:17:22 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:17:22 |     init_opt: None\n18:17:22 |     interactive_candidates: fixed\n18:17:22 |     interactive_mode: False\n18:17:22 |     invsqrt_lr_decay_gamma: -1\n18:17:22 |     is_debug: False\n18:17:22 |     label_truncate: 72\n18:17:22 |     learn_embeddings: True\n18:17:22 |     learn_positional_embeddings: True\n18:17:22 |     learningrate: 5e-05\n18:17:22 |     load_from_checkpoint: False\n18:17:22 |     load_from_pretrained_ranker: True\n18:17:22 |     log_every_n_secs: 10.0\n18:17:22 |     log_every_n_steps: 50\n18:17:22 |     log_keep_fields: all\n18:17:22 |     loglevel: info\n18:17:22 |     lr_scheduler: reduceonplateau\n18:17:22 |     lr_scheduler_decay: 0.5\n18:17:22 |     lr_scheduler_patience: 3\n18:17:22 |     max_train_steps: -1\n18:17:22 |     max_train_time: 7200.0\n18:17:22 |     memory_attention: sqrt\n18:17:22 |     metrics: default\n18:17:22 |     model: transformer/classifier\n18:17:22 |     model_file: /tmp/model2\n18:17:22 |     model_parallel: False\n18:17:22 |     momentum: 0\n18:17:22 |     multitask_weights: [1]\n18:17:22 |     mutators: None\n18:17:22 |     n_decoder_layers: -1\n18:17:22 |     n_encoder_layers: -1\n18:17:22 |     n_heads: 12\n18:17:22 |     n_layers: 12\n18:17:22 |     n_positions: 1024\n18:17:22 |     n_segments: 2\n18:17:22 |     nesterov: True\n18:17:22 |     no_cuda: False\n18:17:22 |     normalize_sent_emb: False\n18:17:22 |     num_epochs: -1\n18:17:22 |     num_workers: 0\n18:17:22 |     nus: (0.7,)\n18:17:22 |     optimizer: adamax\n18:17:22 |     output_scaling: 0.06\n18:17:22 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model2'}\"\n18:17:22 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:17:22 |     person_tokens: False\n18:17:22 |     print_scores: False\n18:17:22 |     rank_candidates: False\n18:17:22 |     rank_top_k: -1\n18:17:22 |     reduction_type: mean\n18:17:22 |     ref_class: None\n18:17:22 |     relu_dropout: 0.0\n18:17:22 |     repeat_blocking_heuristic: True\n18:17:22 |     return_cand_scores: False\n18:17:22 |     save_after_valid: True\n18:17:22 |     save_every_n_secs: -1\n18:17:22 |     save_format: conversations\n18:17:22 |     share_encoders: False\n18:17:22 |     share_word_embeddings: False\n18:17:22 |     short_final_eval: False\n18:17:22 |     special_tok_lst: None\n18:17:22 |     split_lines: False\n18:17:22 |     starttime: Dec03_18-17\n18:17:22 |     task: fromfile:parlaiformat\n18:17:22 |     tensorboard_log: False\n18:17:22 |     tensorboard_logdir: None\n18:17:22 |     text_truncate: 360\n18:17:22 |     threshold: 0.5\n18:17:22 |     topk: 5\n18:17:22 |     train_predict: False\n18:17:22 |     truncate: 1024\n18:17:22 |     update_classifier_head_only: False\n18:17:22 |     update_freq: 1\n18:17:22 |     use_memories: False\n18:17:22 |     use_reply: none\n18:17:22 |     validation_cutoff: 1.0\n18:17:22 |     validation_every_n_epochs: -1\n18:17:22 |     validation_every_n_secs: 20.0\n18:17:22 |     validation_every_n_steps: -1\n18:17:22 |     validation_max_exs: -1\n18:17:22 |     validation_metric: accuracy\n18:17:22 |     validation_metric_mode: max\n18:17:22 |     validation_patience: 30\n18:17:22 |     validation_share_agent: False\n18:17:22 |     variant: xlm\n18:17:22 |     verbose: False\n18:17:22 |     wandb_entity: None\n18:17:22 |     wandb_log: False\n18:17:22 |     wandb_name: None\n18:17:22 |     wandb_project: None\n18:17:22 |     warmup_rate: 0.0001\n18:17:22 |     warmup_updates: 1000\n18:17:22 |     weight_decay: None\n18:17:22 |     world_logs: \n18:17:22 |     wrap_memory_encoder: False\n18:17:23 | creating task(s): fromfile:parlaiformat\n18:17:23 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt\n18:17:23 | training...\n18:17:33 | time:10s total_exs:420 total_steps:21 epochs:17.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .4429 4.429e-10               .3276                 .4524   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .2568            .5244              .4388   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6515 10.72     1 254.4 528.7       0          0 41.57  420   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .4429             32768   2.85    .1206 6.057 .7039 1.055e-06 121.1 251.8   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   21 375.5 780.5 2.083        .4204\n\n18:17:43 | time:20s total_exs:1160 total_steps:58 epochs:48.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6959 6.959e-10               .6725                 .7476   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6111            .7163              .6589   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .7845 10.95     1   259 976.2       0          0 75.38  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6959             32768  2.583    .1207 6.022 .6549 2.905e-06 120.4 453.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   58 379.4 1430 3.778        .6939\n\n18:17:43 | creating task(s): fromfile:parlaiformat\n18:17:43 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:17:43 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt\n18:17:43 | running eval: valid\n18:17:43 | eval completed in 0.20s\n18:17:43 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9167 9.167e-10               .9167                 .9167   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9167            .9167              .9167   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167    11   156  1768       0          0   136   24 .9167   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5867 2.905e-06    72   816       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     58  228 2584        .9167\n\u001b[0m\n18:17:43 | \u001b[1;32mnew best accuracy: 0.9167\u001b[0m\n18:17:43 | saving best valid model: /tmp/model2\n18:17:43 | Saving dictionary to /tmp/model2.dict\n18:17:47 | saving model checkpoint: /tmp/model2.checkpoint\n18:17:47 | Saving dictionary to /tmp/model2.checkpoint.dict\n18:18:03 | time:41s total_exs:1880 total_steps:94 epochs:78.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9708 9.708e-10               .9714                 .9889   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9544            .9703              .9528   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9885  10.8     1 256.1 911.7       0          0  71.2  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9708             32768    3.1    .1207 6.036 .5108 4.705e-06 120.7 429.8   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   94 376.8 1342 3.568        .9708\n\n18:18:07 | time:44s total_exs:2140 total_steps:107 epochs:89.17\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9923 9.923e-10               .9927                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9855            .9919              .9839   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 11.09     1 261.8  1021       0          0 77.98  260   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9923             32768  2.861    .1207 6.062 .3658 5.354e-06 121.2 472.7   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                  107  383 1493 3.925        .9923\n\n18:18:07 | running eval: valid\n18:18:07 | eval completed in 0.19s\n18:18:07 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1801       0          0 138.5   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .2953 5.354e-06    72 831.2       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    107  228 2632            1\n\u001b[0m\n18:18:07 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9167)\u001b[0m\n18:18:07 | saving best valid model: /tmp/model2\n18:18:11 | task solved! stopping.\n18:18:11 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:18:11 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:18:11 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:18:11 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:18:11 | Using CUDA\n18:18:12 | loading dictionary from /tmp/model2.dict\n18:18:12 | num words = 54944\n18:18:17 | Loading existing model parameters from /tmp/model2\n18:18:19 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:18:20 | creating task(s): fromfile:parlaiformat\n18:18:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:18:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt\n18:18:20 | running eval: valid\n18:18:20 | eval completed in 0.21s\n18:18:20 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1616       0          0 124.3   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2953 5.354e-06    72 745.9       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    107  228 2362            1\n\u001b[0m\n18:18:20 | creating task(s): fromfile:parlaiformat\n18:18:20 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:18:20 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt\n18:18:20 | running eval: test\n18:18:20 | eval completed in 0.20s\n18:18:20 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1    11   156  1741       0          0 133.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2953 5.354e-06    72 803.7       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    107  228 2545            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-a.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:18:22.364120Z","iopub.execute_input":"2022-12-03T18:18:22.364432Z","iopub.status.idle":"2022-12-03T18:18:49.915052Z","shell.execute_reply.started":"2022-12-03T18:18:22.364401Z","shell.execute_reply":"2022-12-03T18:18:49.913831Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stdout","text":"18:18:29 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt)\u001b[0m\n18:18:29 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:18:29 | Using CUDA\n18:18:29 | loading dictionary from /tmp/model2.dict\n18:18:29 | num words = 54944\n18:18:33 | Loading existing model parameters from /tmp/model2\n18:18:40 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:18:41 | Opt:\n18:18:41 |     activation: gelu\n18:18:41 |     adafactor_eps: '[1e-30, 0.001]'\n18:18:41 |     adam_eps: 1e-08\n18:18:41 |     add_p1_after_newln: False\n18:18:41 |     aggregate_micro: False\n18:18:41 |     allow_missing_init_opts: False\n18:18:41 |     area_under_curve_class: None\n18:18:41 |     area_under_curve_digits: -1\n18:18:41 |     attention_dropout: 0.1\n18:18:41 |     batchsize: 40\n18:18:41 |     betas: '[0.9, 0.999]'\n18:18:41 |     bpe_add_prefix_space: None\n18:18:41 |     bpe_debug: False\n18:18:41 |     bpe_dropout: None\n18:18:41 |     bpe_merge: None\n18:18:41 |     bpe_vocab: None\n18:18:41 |     candidates: inline\n18:18:41 |     cap_num_predictions: 100\n18:18:41 |     checkpoint_activations: False\n18:18:41 |     class_weights: None\n18:18:41 |     classes: \"['__notok__', '__ok__']\"\n18:18:41 |     classes_from_file: None\n18:18:41 |     data_parallel: True\n18:18:41 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:18:41 |     datatype: train\n18:18:41 |     delimiter: '\\n'\n18:18:41 |     dict_class: parlai.core.dict:DictionaryAgent\n18:18:41 |     dict_endtoken: __start__\n18:18:41 |     dict_file: /tmp/model2.dict\n18:18:41 |     dict_include_test: False\n18:18:41 |     dict_include_valid: False\n18:18:41 |     dict_initpath: None\n18:18:41 |     dict_language: english\n18:18:41 |     dict_loaded: True\n18:18:41 |     dict_lower: True\n18:18:41 |     dict_max_ngram_size: -1\n18:18:41 |     dict_maxexs: -1\n18:18:41 |     dict_maxtokens: -1\n18:18:41 |     dict_minfreq: 0\n18:18:41 |     dict_nulltoken: __null__\n18:18:41 |     dict_starttoken: __start__\n18:18:41 |     dict_textfields: text,labels\n18:18:41 |     dict_tokenizer: bpe\n18:18:41 |     dict_unktoken: __unk__\n18:18:41 |     display_examples: False\n18:18:41 |     download_path: None\n18:18:41 |     dropout: 0.1\n18:18:41 |     dynamic_batching: None\n18:18:41 |     embedding_projection: random\n18:18:41 |     embedding_size: 768\n18:18:41 |     embedding_type: random\n18:18:41 |     embeddings_scale: False\n18:18:41 |     encode_candidate_vecs: True\n18:18:41 |     encode_candidate_vecs_batchsize: 256\n18:18:41 |     eval_batchsize: None\n18:18:41 |     eval_candidates: inline\n18:18:41 |     eval_dynamic_batching: None\n18:18:41 |     evaltask: None\n18:18:41 |     ffn_size: 3072\n18:18:41 |     final_extra_opt: \n18:18:41 |     fixed_candidate_vecs: reuse\n18:18:41 |     fixed_candidates_path: None\n18:18:41 |     force_fp16_tokens: True\n18:18:41 |     fp16: True\n18:18:41 |     fp16_impl: safe\n18:18:41 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-a.txt\n18:18:41 |     fromfile_datatype_extension: False\n18:18:41 |     gpu: -1\n18:18:41 |     gradient_clip: 0.1\n18:18:41 |     hide_labels: False\n18:18:41 |     history_add_global_end_token: None\n18:18:41 |     history_reversed: False\n18:18:41 |     history_size: 20\n18:18:41 |     ignore_bad_candidates: False\n18:18:41 |     ignore_labels: None\n18:18:41 |     image_cropsize: 224\n18:18:41 |     image_mode: raw\n18:18:41 |     image_size: 256\n18:18:41 |     inference: max\n18:18:41 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:18:41 |     init_opt: None\n18:18:41 |     interactive_candidates: fixed\n18:18:41 |     interactive_mode: False\n18:18:41 |     invsqrt_lr_decay_gamma: -1\n18:18:41 |     is_debug: False\n18:18:41 |     label_truncate: 72\n18:18:41 |     learn_embeddings: True\n18:18:41 |     learn_positional_embeddings: True\n18:18:41 |     learningrate: 5e-05\n18:18:41 |     load_from_pretrained_ranker: True\n18:18:41 |     log_every_n_secs: 10.0\n18:18:41 |     log_every_n_steps: 50\n18:18:41 |     log_keep_fields: all\n18:18:41 |     loglevel: info\n18:18:41 |     lr_scheduler: reduceonplateau\n18:18:41 |     lr_scheduler_decay: 0.5\n18:18:41 |     lr_scheduler_patience: 3\n18:18:41 |     max_train_steps: -1\n18:18:41 |     max_train_time: 7200.0\n18:18:41 |     memory_attention: sqrt\n18:18:41 |     metrics: default\n18:18:41 |     model: transformer/classifier\n18:18:41 |     model_file: /tmp/model2\n18:18:41 |     model_parallel: False\n18:18:41 |     momentum: 0\n18:18:41 |     multitask_weights: [1]\n18:18:41 |     mutators: None\n18:18:41 |     n_decoder_layers: -1\n18:18:41 |     n_encoder_layers: -1\n18:18:41 |     n_heads: 12\n18:18:41 |     n_layers: 12\n18:18:41 |     n_positions: 1024\n18:18:41 |     n_segments: 2\n18:18:41 |     nesterov: True\n18:18:41 |     no_cuda: False\n18:18:41 |     normalize_sent_emb: False\n18:18:41 |     num_epochs: -1\n18:18:41 |     num_examples: -1\n18:18:41 |     num_workers: 0\n18:18:41 |     nus: [0.7]\n18:18:41 |     optimizer: adamax\n18:18:41 |     output_scaling: 0.06\n18:18:41 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n18:18:41 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:18:41 |     person_tokens: False\n18:18:41 |     print_scores: False\n18:18:41 |     rank_candidates: False\n18:18:41 |     rank_top_k: -1\n18:18:41 |     reduction_type: mean\n18:18:41 |     ref_class: None\n18:18:41 |     relu_dropout: 0.0\n18:18:41 |     repeat_blocking_heuristic: True\n18:18:41 |     report_filename: \n18:18:41 |     return_cand_scores: False\n18:18:41 |     save_after_valid: True\n18:18:41 |     save_every_n_secs: -1\n18:18:41 |     save_format: conversations\n18:18:41 |     share_encoders: False\n18:18:41 |     share_word_embeddings: False\n18:18:41 |     short_final_eval: False\n18:18:41 |     special_tok_lst: None\n18:18:41 |     split_lines: False\n18:18:41 |     starttime: Dec03_18-17\n18:18:41 |     task: fromfile:parlaiformat\n18:18:41 |     tensorboard_log: False\n18:18:41 |     tensorboard_logdir: None\n18:18:41 |     text_truncate: 360\n18:18:41 |     threshold: 0.5\n18:18:41 |     topk: 5\n18:18:41 |     train_predict: False\n18:18:41 |     truncate: 1024\n18:18:41 |     update_classifier_head_only: False\n18:18:41 |     update_freq: 1\n18:18:41 |     use_memories: False\n18:18:41 |     use_reply: none\n18:18:41 |     validation_cutoff: 1.0\n18:18:41 |     validation_every_n_epochs: -1\n18:18:41 |     validation_every_n_secs: 20.0\n18:18:41 |     validation_every_n_steps: -1\n18:18:41 |     validation_max_exs: -1\n18:18:41 |     validation_metric: accuracy\n18:18:41 |     validation_metric_mode: max\n18:18:41 |     validation_patience: 30\n18:18:41 |     validation_share_agent: False\n18:18:41 |     variant: xlm\n18:18:41 |     verbose: False\n18:18:41 |     wandb_entity: None\n18:18:41 |     wandb_log: False\n18:18:41 |     wandb_name: None\n18:18:41 |     wandb_project: None\n18:18:41 |     warmup_rate: 0.0001\n18:18:41 |     warmup_updates: 1000\n18:18:41 |     weight_decay: None\n18:18:41 |     world_logs: \n18:18:41 |     wrap_memory_encoder: False\n18:18:42 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:18:42 | creating task(s): fromfile:parlaiformat\n18:18:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:18:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-a.txt\n18:18:48 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1400 1.4e-10               .1485                 .1456   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1515            .1313              .1340   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1287  11.7 547.8   465       0          0 33.96  200 .1400   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9166 5.354e-06 239.6 203.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 787.4 668.5        .1398\u001b[0m\n18:18:48 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .1400 1.4e-10               .1485                 .1456   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1515            .1313              .1340   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .1287  11.7 547.8   465       0          0 33.96  200 .1400   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9166 5.354e-06 239.6 203.4       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 787.4 668.5        .1398\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-b.txt -m transformer/classifier -mf /tmp/model2 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:18:49.917150Z","iopub.execute_input":"2022-12-03T18:18:49.917577Z","iopub.status.idle":"2022-12-03T18:19:15.628146Z","shell.execute_reply.started":"2022-12-03T18:18:49.917536Z","shell.execute_reply":"2022-12-03T18:19:15.626951Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"18:18:57 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_valid.txt)\u001b[0m\n18:18:57 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:18:57 | Using CUDA\n18:18:57 | loading dictionary from /tmp/model2.dict\n18:18:57 | num words = 54944\n18:19:01 | Loading existing model parameters from /tmp/model2\n18:19:06 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:19:08 | Opt:\n18:19:08 |     activation: gelu\n18:19:08 |     adafactor_eps: '[1e-30, 0.001]'\n18:19:08 |     adam_eps: 1e-08\n18:19:08 |     add_p1_after_newln: False\n18:19:08 |     aggregate_micro: False\n18:19:08 |     allow_missing_init_opts: False\n18:19:08 |     area_under_curve_class: None\n18:19:08 |     area_under_curve_digits: -1\n18:19:08 |     attention_dropout: 0.1\n18:19:08 |     batchsize: 40\n18:19:08 |     betas: '[0.9, 0.999]'\n18:19:08 |     bpe_add_prefix_space: None\n18:19:08 |     bpe_debug: False\n18:19:08 |     bpe_dropout: None\n18:19:08 |     bpe_merge: None\n18:19:08 |     bpe_vocab: None\n18:19:08 |     candidates: inline\n18:19:08 |     cap_num_predictions: 100\n18:19:08 |     checkpoint_activations: False\n18:19:08 |     class_weights: None\n18:19:08 |     classes: \"['__notok__', '__ok__']\"\n18:19:08 |     classes_from_file: None\n18:19:08 |     data_parallel: True\n18:19:08 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:19:08 |     datatype: train\n18:19:08 |     delimiter: '\\n'\n18:19:08 |     dict_class: parlai.core.dict:DictionaryAgent\n18:19:08 |     dict_endtoken: __start__\n18:19:08 |     dict_file: /tmp/model2.dict\n18:19:08 |     dict_include_test: False\n18:19:08 |     dict_include_valid: False\n18:19:08 |     dict_initpath: None\n18:19:08 |     dict_language: english\n18:19:08 |     dict_loaded: True\n18:19:08 |     dict_lower: True\n18:19:08 |     dict_max_ngram_size: -1\n18:19:08 |     dict_maxexs: -1\n18:19:08 |     dict_maxtokens: -1\n18:19:08 |     dict_minfreq: 0\n18:19:08 |     dict_nulltoken: __null__\n18:19:08 |     dict_starttoken: __start__\n18:19:08 |     dict_textfields: text,labels\n18:19:08 |     dict_tokenizer: bpe\n18:19:08 |     dict_unktoken: __unk__\n18:19:08 |     display_examples: False\n18:19:08 |     download_path: None\n18:19:08 |     dropout: 0.1\n18:19:08 |     dynamic_batching: None\n18:19:08 |     embedding_projection: random\n18:19:08 |     embedding_size: 768\n18:19:08 |     embedding_type: random\n18:19:08 |     embeddings_scale: False\n18:19:08 |     encode_candidate_vecs: True\n18:19:08 |     encode_candidate_vecs_batchsize: 256\n18:19:08 |     eval_batchsize: None\n18:19:08 |     eval_candidates: inline\n18:19:08 |     eval_dynamic_batching: None\n18:19:08 |     evaltask: None\n18:19:08 |     ffn_size: 3072\n18:19:08 |     final_extra_opt: \n18:19:08 |     fixed_candidate_vecs: reuse\n18:19:08 |     fixed_candidates_path: None\n18:19:08 |     force_fp16_tokens: True\n18:19:08 |     fp16: True\n18:19:08 |     fp16_impl: safe\n18:19:08 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-b.txt\n18:19:08 |     fromfile_datatype_extension: False\n18:19:08 |     gpu: -1\n18:19:08 |     gradient_clip: 0.1\n18:19:08 |     hide_labels: False\n18:19:08 |     history_add_global_end_token: None\n18:19:08 |     history_reversed: False\n18:19:08 |     history_size: 20\n18:19:08 |     ignore_bad_candidates: False\n18:19:08 |     ignore_labels: None\n18:19:08 |     image_cropsize: 224\n18:19:08 |     image_mode: raw\n18:19:08 |     image_size: 256\n18:19:08 |     inference: max\n18:19:08 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:19:08 |     init_opt: None\n18:19:08 |     interactive_candidates: fixed\n18:19:08 |     interactive_mode: False\n18:19:08 |     invsqrt_lr_decay_gamma: -1\n18:19:08 |     is_debug: False\n18:19:08 |     label_truncate: 72\n18:19:08 |     learn_embeddings: True\n18:19:08 |     learn_positional_embeddings: True\n18:19:08 |     learningrate: 5e-05\n18:19:08 |     load_from_pretrained_ranker: True\n18:19:08 |     log_every_n_secs: 10.0\n18:19:08 |     log_every_n_steps: 50\n18:19:08 |     log_keep_fields: all\n18:19:08 |     loglevel: info\n18:19:08 |     lr_scheduler: reduceonplateau\n18:19:08 |     lr_scheduler_decay: 0.5\n18:19:08 |     lr_scheduler_patience: 3\n18:19:08 |     max_train_steps: -1\n18:19:08 |     max_train_time: 7200.0\n18:19:08 |     memory_attention: sqrt\n18:19:08 |     metrics: default\n18:19:08 |     model: transformer/classifier\n18:19:08 |     model_file: /tmp/model2\n18:19:08 |     model_parallel: False\n18:19:08 |     momentum: 0\n18:19:08 |     multitask_weights: [1]\n18:19:08 |     mutators: None\n18:19:08 |     n_decoder_layers: -1\n18:19:08 |     n_encoder_layers: -1\n18:19:08 |     n_heads: 12\n18:19:08 |     n_layers: 12\n18:19:08 |     n_positions: 1024\n18:19:08 |     n_segments: 2\n18:19:08 |     nesterov: True\n18:19:08 |     no_cuda: False\n18:19:08 |     normalize_sent_emb: False\n18:19:08 |     num_epochs: -1\n18:19:08 |     num_examples: -1\n18:19:08 |     num_workers: 0\n18:19:08 |     nus: [0.7]\n18:19:08 |     optimizer: adamax\n18:19:08 |     output_scaling: 0.06\n18:19:08 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model2', 'batchsize': 40}\"\n18:19:08 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:19:08 |     person_tokens: False\n18:19:08 |     print_scores: False\n18:19:08 |     rank_candidates: False\n18:19:08 |     rank_top_k: -1\n18:19:08 |     reduction_type: mean\n18:19:08 |     ref_class: None\n18:19:08 |     relu_dropout: 0.0\n18:19:08 |     repeat_blocking_heuristic: True\n18:19:08 |     report_filename: \n18:19:08 |     return_cand_scores: False\n18:19:08 |     save_after_valid: True\n18:19:08 |     save_every_n_secs: -1\n18:19:08 |     save_format: conversations\n18:19:08 |     share_encoders: False\n18:19:08 |     share_word_embeddings: False\n18:19:08 |     short_final_eval: False\n18:19:08 |     special_tok_lst: None\n18:19:08 |     split_lines: False\n18:19:08 |     starttime: Dec03_18-17\n18:19:08 |     task: fromfile:parlaiformat\n18:19:08 |     tensorboard_log: False\n18:19:08 |     tensorboard_logdir: None\n18:19:08 |     text_truncate: 360\n18:19:08 |     threshold: 0.5\n18:19:08 |     topk: 5\n18:19:08 |     train_predict: False\n18:19:08 |     truncate: 1024\n18:19:08 |     update_classifier_head_only: False\n18:19:08 |     update_freq: 1\n18:19:08 |     use_memories: False\n18:19:08 |     use_reply: none\n18:19:08 |     validation_cutoff: 1.0\n18:19:08 |     validation_every_n_epochs: -1\n18:19:08 |     validation_every_n_secs: 20.0\n18:19:08 |     validation_every_n_steps: -1\n18:19:08 |     validation_max_exs: -1\n18:19:08 |     validation_metric: accuracy\n18:19:08 |     validation_metric_mode: max\n18:19:08 |     validation_patience: 30\n18:19:08 |     validation_share_agent: False\n18:19:08 |     variant: xlm\n18:19:08 |     verbose: False\n18:19:08 |     wandb_entity: None\n18:19:08 |     wandb_log: False\n18:19:08 |     wandb_name: None\n18:19:08 |     wandb_project: None\n18:19:08 |     warmup_rate: 0.0001\n18:19:08 |     warmup_updates: 1000\n18:19:08 |     weight_decay: None\n18:19:08 |     world_logs: \n18:19:08 |     wrap_memory_encoder: False\n18:19:08 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:19:08 | creating task(s): fromfile:parlaiformat\n18:19:08 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:19:08 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run2/data_train-b.txt\n18:19:14 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8600 8.6e-10               .8627                 .8544   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8713            .8571              .8660   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8485  11.7 547.8 529.1       0          0 38.63  200 .8600   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5338 5.354e-06 240.4 232.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 788.2 761.3        .8600\u001b[0m\n18:19:14 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n       .8600 8.6e-10               .8627                 .8544   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8713            .8571              .8660   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8485  11.7 547.8 529.1       0          0 38.63  200 .8600   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5338 5.354e-06 240.4 232.2       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    107 788.2 761.3        .8600\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model2*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:19:15.630007Z","iopub.execute_input":"2022-12-03T18:19:15.630425Z","iopub.status.idle":"2022-12-03T18:19:16.762934Z","shell.execute_reply.started":"2022-12-03T18:19:15.630385Z","shell.execute_reply":"2022-12-03T18:19:16.761652Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":"run 3","metadata":{}},{"cell_type":"code","source":"# run 3 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model3","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:19:16.765063Z","iopub.execute_input":"2022-12-03T18:19:16.765454Z","iopub.status.idle":"2022-12-03T18:20:08.963015Z","shell.execute_reply.started":"2022-12-03T18:19:16.765413Z","shell.execute_reply":"2022-12-03T18:20:08.961814Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stdout","text":"18:19:24 | building dictionary first...\n18:19:24 | No model with opt yet at: /tmp/model3(.opt)\n18:19:24 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:19:24 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:19:24 | Using CUDA\n18:19:24 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:19:24 | num words = 54944\n18:19:28 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:19:34 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:19:34 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:19:34 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:19:34 | Opt:\n18:19:34 |     activation: gelu\n18:19:34 |     adafactor_eps: '(1e-30, 0.001)'\n18:19:34 |     adam_eps: 1e-08\n18:19:34 |     add_p1_after_newln: False\n18:19:34 |     aggregate_micro: False\n18:19:34 |     allow_missing_init_opts: False\n18:19:34 |     attention_dropout: 0.1\n18:19:34 |     batchsize: 20\n18:19:34 |     betas: '(0.9, 0.999)'\n18:19:34 |     bpe_add_prefix_space: None\n18:19:34 |     bpe_debug: False\n18:19:34 |     bpe_dropout: None\n18:19:34 |     bpe_merge: None\n18:19:34 |     bpe_vocab: None\n18:19:34 |     candidates: inline\n18:19:34 |     cap_num_predictions: 100\n18:19:34 |     checkpoint_activations: False\n18:19:34 |     class_weights: None\n18:19:34 |     classes: \"['__notok__', '__ok__']\"\n18:19:34 |     classes_from_file: None\n18:19:34 |     data_parallel: True\n18:19:34 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:19:34 |     datatype: train\n18:19:34 |     delimiter: '\\n'\n18:19:34 |     dict_class: parlai.core.dict:DictionaryAgent\n18:19:34 |     dict_endtoken: __start__\n18:19:34 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:19:34 |     dict_include_test: False\n18:19:34 |     dict_include_valid: False\n18:19:34 |     dict_initpath: None\n18:19:34 |     dict_language: english\n18:19:34 |     dict_loaded: True\n18:19:34 |     dict_lower: True\n18:19:34 |     dict_max_ngram_size: -1\n18:19:34 |     dict_maxexs: -1\n18:19:34 |     dict_maxtokens: -1\n18:19:34 |     dict_minfreq: 0\n18:19:34 |     dict_nulltoken: __null__\n18:19:34 |     dict_starttoken: __start__\n18:19:34 |     dict_textfields: text,labels\n18:19:34 |     dict_tokenizer: bpe\n18:19:34 |     dict_unktoken: __unk__\n18:19:34 |     display_examples: False\n18:19:34 |     download_path: None\n18:19:34 |     dropout: 0.1\n18:19:34 |     dynamic_batching: None\n18:19:34 |     embedding_projection: random\n18:19:34 |     embedding_size: 768\n18:19:34 |     embedding_type: random\n18:19:34 |     embeddings_scale: False\n18:19:34 |     encode_candidate_vecs: True\n18:19:34 |     encode_candidate_vecs_batchsize: 256\n18:19:34 |     eval_batchsize: None\n18:19:34 |     eval_candidates: inline\n18:19:34 |     eval_dynamic_batching: None\n18:19:34 |     evaltask: None\n18:19:34 |     ffn_size: 3072\n18:19:34 |     final_extra_opt: \n18:19:34 |     fixed_candidate_vecs: reuse\n18:19:34 |     fixed_candidates_path: None\n18:19:34 |     force_fp16_tokens: False\n18:19:34 |     fp16: True\n18:19:34 |     fp16_impl: safe\n18:19:34 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt\n18:19:34 |     fromfile_datatype_extension: False\n18:19:34 |     gpu: -1\n18:19:34 |     gradient_clip: 0.1\n18:19:34 |     hide_labels: False\n18:19:34 |     history_add_global_end_token: None\n18:19:34 |     history_reversed: False\n18:19:34 |     history_size: 20\n18:19:34 |     ignore_bad_candidates: False\n18:19:34 |     ignore_labels: None\n18:19:34 |     image_cropsize: 224\n18:19:34 |     image_mode: raw\n18:19:34 |     image_size: 256\n18:19:34 |     inference: max\n18:19:34 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:19:34 |     init_opt: None\n18:19:34 |     interactive_candidates: fixed\n18:19:34 |     interactive_mode: False\n18:19:34 |     invsqrt_lr_decay_gamma: -1\n18:19:34 |     is_debug: False\n18:19:34 |     label_truncate: 72\n18:19:34 |     learn_embeddings: True\n18:19:34 |     learn_positional_embeddings: True\n18:19:34 |     learningrate: 5e-05\n18:19:34 |     load_from_checkpoint: False\n18:19:34 |     load_from_pretrained_ranker: True\n18:19:34 |     log_every_n_secs: 10.0\n18:19:34 |     log_every_n_steps: 50\n18:19:34 |     log_keep_fields: all\n18:19:34 |     loglevel: info\n18:19:34 |     lr_scheduler: reduceonplateau\n18:19:34 |     lr_scheduler_decay: 0.5\n18:19:34 |     lr_scheduler_patience: 3\n18:19:34 |     max_train_steps: -1\n18:19:34 |     max_train_time: 7200.0\n18:19:34 |     memory_attention: sqrt\n18:19:34 |     metrics: default\n18:19:34 |     model: transformer/classifier\n18:19:34 |     model_file: /tmp/model3\n18:19:34 |     model_parallel: False\n18:19:34 |     momentum: 0\n18:19:34 |     multitask_weights: [1]\n18:19:34 |     mutators: None\n18:19:34 |     n_decoder_layers: -1\n18:19:34 |     n_encoder_layers: -1\n18:19:34 |     n_heads: 12\n18:19:34 |     n_layers: 12\n18:19:34 |     n_positions: 1024\n18:19:34 |     n_segments: 2\n18:19:34 |     nesterov: True\n18:19:34 |     no_cuda: False\n18:19:34 |     normalize_sent_emb: False\n18:19:34 |     num_epochs: -1\n18:19:34 |     num_workers: 0\n18:19:34 |     nus: (0.7,)\n18:19:34 |     optimizer: adamax\n18:19:34 |     output_scaling: 0.06\n18:19:34 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model3'}\"\n18:19:34 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:19:34 |     person_tokens: False\n18:19:34 |     print_scores: False\n18:19:34 |     rank_candidates: False\n18:19:34 |     rank_top_k: -1\n18:19:34 |     reduction_type: mean\n18:19:34 |     ref_class: None\n18:19:34 |     relu_dropout: 0.0\n18:19:34 |     repeat_blocking_heuristic: True\n18:19:34 |     return_cand_scores: False\n18:19:34 |     save_after_valid: True\n18:19:34 |     save_every_n_secs: -1\n18:19:34 |     save_format: conversations\n18:19:34 |     share_encoders: False\n18:19:34 |     share_word_embeddings: False\n18:19:34 |     short_final_eval: False\n18:19:34 |     special_tok_lst: None\n18:19:34 |     split_lines: False\n18:19:34 |     starttime: Dec03_18-19\n18:19:34 |     task: fromfile:parlaiformat\n18:19:34 |     tensorboard_log: False\n18:19:34 |     tensorboard_logdir: None\n18:19:34 |     text_truncate: 360\n18:19:34 |     threshold: 0.5\n18:19:34 |     topk: 5\n18:19:34 |     train_predict: False\n18:19:34 |     truncate: 1024\n18:19:34 |     update_classifier_head_only: False\n18:19:34 |     update_freq: 1\n18:19:34 |     use_memories: False\n18:19:34 |     use_reply: none\n18:19:34 |     validation_cutoff: 1.0\n18:19:34 |     validation_every_n_epochs: -1\n18:19:34 |     validation_every_n_secs: 20.0\n18:19:34 |     validation_every_n_steps: -1\n18:19:34 |     validation_max_exs: -1\n18:19:34 |     validation_metric: accuracy\n18:19:34 |     validation_metric_mode: max\n18:19:34 |     validation_patience: 30\n18:19:34 |     validation_share_agent: False\n18:19:34 |     variant: xlm\n18:19:34 |     verbose: False\n18:19:34 |     wandb_entity: None\n18:19:34 |     wandb_log: False\n18:19:34 |     wandb_name: None\n18:19:34 |     wandb_project: None\n18:19:34 |     warmup_rate: 0.0001\n18:19:34 |     warmup_updates: 1000\n18:19:34 |     weight_decay: None\n18:19:34 |     world_logs: \n18:19:34 |     wrap_memory_encoder: False\n18:19:34 | creating task(s): fromfile:parlaiformat\n18:19:34 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt\n18:19:34 | training...\n18:19:45 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6425 6.425e-10               .6925                 .5941   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8299            .5731              .7442   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .4660    12     1 280.1 555.1       0          0 39.64  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6425             32768  2.586    .1206  5.97 .6796 1.005e-06 119.4 236.7   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 399.4 791.7 1.986        .6310\n\n18:19:55 | time:20s total_exs:1120 total_steps:56 epochs:46.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8556 8.556e-10               .8571                 .8298   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8864            .8539              .8837   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .8261 11.75     1 275.1  1016       0          0 73.84  720   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8556             32768  2.659    .1207 5.978 .6228 2.805e-06 119.6 441.4   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   56 394.6 1457 3.701        .8555\n\n18:19:55 | creating task(s): fromfile:parlaiformat\n18:19:55 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:19:55 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt\n18:19:55 | running eval: valid\n18:19:55 | eval completed in 0.20s\n18:19:55 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1806       0          0 131.7   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5580 2.805e-06    72 790.5       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     56 236.5 2597            1\n\u001b[0m\n18:19:55 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n18:19:55 | saving best valid model: /tmp/model3\n18:19:55 | Saving dictionary to /tmp/model3.dict\n18:19:58 | task solved! stopping.\n18:19:58 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:19:58 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:19:58 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:19:58 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:19:58 | Using CUDA\n18:19:58 | loading dictionary from /tmp/model3.dict\n18:19:59 | num words = 54944\n18:20:04 | Loading existing model parameters from /tmp/model3\n18:20:05 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:20:06 | creating task(s): fromfile:parlaiformat\n18:20:06 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:20:06 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt\n18:20:06 | running eval: valid\n18:20:07 | eval completed in 0.21s\n18:20:07 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1791       0          0 130.6   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5580 2.805e-06    72 783.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     56 236.5 2575            1\n\u001b[0m\n18:20:07 | creating task(s): fromfile:parlaiformat\n18:20:07 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:20:07 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt\n18:20:07 | running eval: test\n18:20:07 | eval completed in 0.20s\n18:20:07 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 11.71 164.5  1799       0          0 131.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5580 2.805e-06    72 787.4       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     56 236.5 2587            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-a.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:20:08.966658Z","iopub.execute_input":"2022-12-03T18:20:08.966986Z","iopub.status.idle":"2022-12-03T18:20:36.754116Z","shell.execute_reply.started":"2022-12-03T18:20:08.966957Z","shell.execute_reply":"2022-12-03T18:20:36.752804Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"18:20:16 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt)\u001b[0m\n18:20:16 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:20:16 | Using CUDA\n18:20:16 | loading dictionary from /tmp/model3.dict\n18:20:16 | num words = 54944\n18:20:20 | Loading existing model parameters from /tmp/model3\n18:20:27 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:20:29 | Opt:\n18:20:29 |     activation: gelu\n18:20:29 |     adafactor_eps: '[1e-30, 0.001]'\n18:20:29 |     adam_eps: 1e-08\n18:20:29 |     add_p1_after_newln: False\n18:20:29 |     aggregate_micro: False\n18:20:29 |     allow_missing_init_opts: False\n18:20:29 |     area_under_curve_class: None\n18:20:29 |     area_under_curve_digits: -1\n18:20:29 |     attention_dropout: 0.1\n18:20:29 |     batchsize: 40\n18:20:29 |     betas: '[0.9, 0.999]'\n18:20:29 |     bpe_add_prefix_space: None\n18:20:29 |     bpe_debug: False\n18:20:29 |     bpe_dropout: None\n18:20:29 |     bpe_merge: None\n18:20:29 |     bpe_vocab: None\n18:20:29 |     candidates: inline\n18:20:29 |     cap_num_predictions: 100\n18:20:29 |     checkpoint_activations: False\n18:20:29 |     class_weights: None\n18:20:29 |     classes: \"['__notok__', '__ok__']\"\n18:20:29 |     classes_from_file: None\n18:20:29 |     data_parallel: True\n18:20:29 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:20:29 |     datatype: train\n18:20:29 |     delimiter: '\\n'\n18:20:29 |     dict_class: parlai.core.dict:DictionaryAgent\n18:20:29 |     dict_endtoken: __start__\n18:20:29 |     dict_file: /tmp/model3.dict\n18:20:29 |     dict_include_test: False\n18:20:29 |     dict_include_valid: False\n18:20:29 |     dict_initpath: None\n18:20:29 |     dict_language: english\n18:20:29 |     dict_loaded: True\n18:20:29 |     dict_lower: True\n18:20:29 |     dict_max_ngram_size: -1\n18:20:29 |     dict_maxexs: -1\n18:20:29 |     dict_maxtokens: -1\n18:20:29 |     dict_minfreq: 0\n18:20:29 |     dict_nulltoken: __null__\n18:20:29 |     dict_starttoken: __start__\n18:20:29 |     dict_textfields: text,labels\n18:20:29 |     dict_tokenizer: bpe\n18:20:29 |     dict_unktoken: __unk__\n18:20:29 |     display_examples: False\n18:20:29 |     download_path: None\n18:20:29 |     dropout: 0.1\n18:20:29 |     dynamic_batching: None\n18:20:29 |     embedding_projection: random\n18:20:29 |     embedding_size: 768\n18:20:29 |     embedding_type: random\n18:20:29 |     embeddings_scale: False\n18:20:29 |     encode_candidate_vecs: True\n18:20:29 |     encode_candidate_vecs_batchsize: 256\n18:20:29 |     eval_batchsize: None\n18:20:29 |     eval_candidates: inline\n18:20:29 |     eval_dynamic_batching: None\n18:20:29 |     evaltask: None\n18:20:29 |     ffn_size: 3072\n18:20:29 |     final_extra_opt: \n18:20:29 |     fixed_candidate_vecs: reuse\n18:20:29 |     fixed_candidates_path: None\n18:20:29 |     force_fp16_tokens: True\n18:20:29 |     fp16: True\n18:20:29 |     fp16_impl: safe\n18:20:29 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-a.txt\n18:20:29 |     fromfile_datatype_extension: False\n18:20:29 |     gpu: -1\n18:20:29 |     gradient_clip: 0.1\n18:20:29 |     hide_labels: False\n18:20:29 |     history_add_global_end_token: None\n18:20:29 |     history_reversed: False\n18:20:29 |     history_size: 20\n18:20:29 |     ignore_bad_candidates: False\n18:20:29 |     ignore_labels: None\n18:20:29 |     image_cropsize: 224\n18:20:29 |     image_mode: raw\n18:20:29 |     image_size: 256\n18:20:29 |     inference: max\n18:20:29 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:20:29 |     init_opt: None\n18:20:29 |     interactive_candidates: fixed\n18:20:29 |     interactive_mode: False\n18:20:29 |     invsqrt_lr_decay_gamma: -1\n18:20:29 |     is_debug: False\n18:20:29 |     label_truncate: 72\n18:20:29 |     learn_embeddings: True\n18:20:29 |     learn_positional_embeddings: True\n18:20:29 |     learningrate: 5e-05\n18:20:29 |     load_from_pretrained_ranker: True\n18:20:29 |     log_every_n_secs: 10.0\n18:20:29 |     log_every_n_steps: 50\n18:20:29 |     log_keep_fields: all\n18:20:29 |     loglevel: info\n18:20:29 |     lr_scheduler: reduceonplateau\n18:20:29 |     lr_scheduler_decay: 0.5\n18:20:29 |     lr_scheduler_patience: 3\n18:20:29 |     max_train_steps: -1\n18:20:29 |     max_train_time: 7200.0\n18:20:29 |     memory_attention: sqrt\n18:20:29 |     metrics: default\n18:20:29 |     model: transformer/classifier\n18:20:29 |     model_file: /tmp/model3\n18:20:29 |     model_parallel: False\n18:20:29 |     momentum: 0\n18:20:29 |     multitask_weights: [1]\n18:20:29 |     mutators: None\n18:20:29 |     n_decoder_layers: -1\n18:20:29 |     n_encoder_layers: -1\n18:20:29 |     n_heads: 12\n18:20:29 |     n_layers: 12\n18:20:29 |     n_positions: 1024\n18:20:29 |     n_segments: 2\n18:20:29 |     nesterov: True\n18:20:29 |     no_cuda: False\n18:20:29 |     normalize_sent_emb: False\n18:20:29 |     num_epochs: -1\n18:20:29 |     num_examples: -1\n18:20:29 |     num_workers: 0\n18:20:29 |     nus: [0.7]\n18:20:29 |     optimizer: adamax\n18:20:29 |     output_scaling: 0.06\n18:20:29 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n18:20:29 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:20:29 |     person_tokens: False\n18:20:29 |     print_scores: False\n18:20:29 |     rank_candidates: False\n18:20:29 |     rank_top_k: -1\n18:20:29 |     reduction_type: mean\n18:20:29 |     ref_class: None\n18:20:29 |     relu_dropout: 0.0\n18:20:29 |     repeat_blocking_heuristic: True\n18:20:29 |     report_filename: \n18:20:29 |     return_cand_scores: False\n18:20:29 |     save_after_valid: True\n18:20:29 |     save_every_n_secs: -1\n18:20:29 |     save_format: conversations\n18:20:29 |     share_encoders: False\n18:20:29 |     share_word_embeddings: False\n18:20:29 |     short_final_eval: False\n18:20:29 |     special_tok_lst: None\n18:20:29 |     split_lines: False\n18:20:29 |     starttime: Dec03_18-19\n18:20:29 |     task: fromfile:parlaiformat\n18:20:29 |     tensorboard_log: False\n18:20:29 |     tensorboard_logdir: None\n18:20:29 |     text_truncate: 360\n18:20:29 |     threshold: 0.5\n18:20:29 |     topk: 5\n18:20:29 |     train_predict: False\n18:20:29 |     truncate: 1024\n18:20:29 |     update_classifier_head_only: False\n18:20:29 |     update_freq: 1\n18:20:29 |     use_memories: False\n18:20:29 |     use_reply: none\n18:20:29 |     validation_cutoff: 1.0\n18:20:29 |     validation_every_n_epochs: -1\n18:20:29 |     validation_every_n_secs: 20.0\n18:20:29 |     validation_every_n_steps: -1\n18:20:29 |     validation_max_exs: -1\n18:20:29 |     validation_metric: accuracy\n18:20:29 |     validation_metric_mode: max\n18:20:29 |     validation_patience: 30\n18:20:29 |     validation_share_agent: False\n18:20:29 |     variant: xlm\n18:20:29 |     verbose: False\n18:20:29 |     wandb_entity: None\n18:20:29 |     wandb_log: False\n18:20:29 |     wandb_name: None\n18:20:29 |     wandb_project: None\n18:20:29 |     warmup_rate: 0.0001\n18:20:29 |     warmup_updates: 1000\n18:20:29 |     weight_decay: None\n18:20:29 |     world_logs: \n18:20:29 |     wrap_memory_encoder: False\n18:20:29 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:20:29 | creating task(s): fromfile:parlaiformat\n18:20:29 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:20:29 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-a.txt\n18:20:35 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1750 1.75e-10               .1176                 .1264   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1100            .2254              .2124   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2400 11.47   539 508.4       0          0 37.73  200 .1750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7549 2.805e-06   240 226.4       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     56  779 734.8        .1715\u001b[0m\n18:20:35 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1750 1.75e-10               .1176                 .1264   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1100            .2254              .2124   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .2400 11.47   539 508.4       0          0 37.73  200 .1750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .7549 2.805e-06   240 226.4       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     56  779 734.8        .1715\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-b.txt -m transformer/classifier -mf /tmp/model3 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:20:36.756175Z","iopub.execute_input":"2022-12-03T18:20:36.756672Z","iopub.status.idle":"2022-12-03T18:21:03.176407Z","shell.execute_reply.started":"2022-12-03T18:20:36.756626Z","shell.execute_reply":"2022-12-03T18:21:03.175211Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"18:20:43 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_valid.txt)\u001b[0m\n18:20:43 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:20:43 | Using CUDA\n18:20:43 | loading dictionary from /tmp/model3.dict\n18:20:43 | num words = 54944\n18:20:48 | Loading existing model parameters from /tmp/model3\n18:20:54 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:20:55 | Opt:\n18:20:55 |     activation: gelu\n18:20:55 |     adafactor_eps: '[1e-30, 0.001]'\n18:20:55 |     adam_eps: 1e-08\n18:20:55 |     add_p1_after_newln: False\n18:20:55 |     aggregate_micro: False\n18:20:55 |     allow_missing_init_opts: False\n18:20:55 |     area_under_curve_class: None\n18:20:55 |     area_under_curve_digits: -1\n18:20:55 |     attention_dropout: 0.1\n18:20:55 |     batchsize: 40\n18:20:55 |     betas: '[0.9, 0.999]'\n18:20:55 |     bpe_add_prefix_space: None\n18:20:55 |     bpe_debug: False\n18:20:55 |     bpe_dropout: None\n18:20:55 |     bpe_merge: None\n18:20:55 |     bpe_vocab: None\n18:20:55 |     candidates: inline\n18:20:55 |     cap_num_predictions: 100\n18:20:55 |     checkpoint_activations: False\n18:20:55 |     class_weights: None\n18:20:55 |     classes: \"['__notok__', '__ok__']\"\n18:20:55 |     classes_from_file: None\n18:20:55 |     data_parallel: True\n18:20:55 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:20:55 |     datatype: train\n18:20:55 |     delimiter: '\\n'\n18:20:55 |     dict_class: parlai.core.dict:DictionaryAgent\n18:20:55 |     dict_endtoken: __start__\n18:20:55 |     dict_file: /tmp/model3.dict\n18:20:55 |     dict_include_test: False\n18:20:55 |     dict_include_valid: False\n18:20:55 |     dict_initpath: None\n18:20:55 |     dict_language: english\n18:20:55 |     dict_loaded: True\n18:20:55 |     dict_lower: True\n18:20:55 |     dict_max_ngram_size: -1\n18:20:55 |     dict_maxexs: -1\n18:20:55 |     dict_maxtokens: -1\n18:20:55 |     dict_minfreq: 0\n18:20:55 |     dict_nulltoken: __null__\n18:20:55 |     dict_starttoken: __start__\n18:20:55 |     dict_textfields: text,labels\n18:20:55 |     dict_tokenizer: bpe\n18:20:55 |     dict_unktoken: __unk__\n18:20:55 |     display_examples: False\n18:20:55 |     download_path: None\n18:20:55 |     dropout: 0.1\n18:20:55 |     dynamic_batching: None\n18:20:55 |     embedding_projection: random\n18:20:55 |     embedding_size: 768\n18:20:55 |     embedding_type: random\n18:20:55 |     embeddings_scale: False\n18:20:55 |     encode_candidate_vecs: True\n18:20:55 |     encode_candidate_vecs_batchsize: 256\n18:20:55 |     eval_batchsize: None\n18:20:55 |     eval_candidates: inline\n18:20:55 |     eval_dynamic_batching: None\n18:20:55 |     evaltask: None\n18:20:55 |     ffn_size: 3072\n18:20:55 |     final_extra_opt: \n18:20:55 |     fixed_candidate_vecs: reuse\n18:20:55 |     fixed_candidates_path: None\n18:20:55 |     force_fp16_tokens: True\n18:20:55 |     fp16: True\n18:20:55 |     fp16_impl: safe\n18:20:55 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-b.txt\n18:20:55 |     fromfile_datatype_extension: False\n18:20:55 |     gpu: -1\n18:20:55 |     gradient_clip: 0.1\n18:20:55 |     hide_labels: False\n18:20:55 |     history_add_global_end_token: None\n18:20:55 |     history_reversed: False\n18:20:55 |     history_size: 20\n18:20:55 |     ignore_bad_candidates: False\n18:20:55 |     ignore_labels: None\n18:20:55 |     image_cropsize: 224\n18:20:55 |     image_mode: raw\n18:20:55 |     image_size: 256\n18:20:55 |     inference: max\n18:20:55 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:20:55 |     init_opt: None\n18:20:55 |     interactive_candidates: fixed\n18:20:55 |     interactive_mode: False\n18:20:55 |     invsqrt_lr_decay_gamma: -1\n18:20:55 |     is_debug: False\n18:20:55 |     label_truncate: 72\n18:20:55 |     learn_embeddings: True\n18:20:55 |     learn_positional_embeddings: True\n18:20:55 |     learningrate: 5e-05\n18:20:55 |     load_from_pretrained_ranker: True\n18:20:55 |     log_every_n_secs: 10.0\n18:20:55 |     log_every_n_steps: 50\n18:20:55 |     log_keep_fields: all\n18:20:55 |     loglevel: info\n18:20:55 |     lr_scheduler: reduceonplateau\n18:20:55 |     lr_scheduler_decay: 0.5\n18:20:55 |     lr_scheduler_patience: 3\n18:20:55 |     max_train_steps: -1\n18:20:55 |     max_train_time: 7200.0\n18:20:55 |     memory_attention: sqrt\n18:20:55 |     metrics: default\n18:20:55 |     model: transformer/classifier\n18:20:55 |     model_file: /tmp/model3\n18:20:55 |     model_parallel: False\n18:20:55 |     momentum: 0\n18:20:55 |     multitask_weights: [1]\n18:20:55 |     mutators: None\n18:20:55 |     n_decoder_layers: -1\n18:20:55 |     n_encoder_layers: -1\n18:20:55 |     n_heads: 12\n18:20:55 |     n_layers: 12\n18:20:55 |     n_positions: 1024\n18:20:55 |     n_segments: 2\n18:20:55 |     nesterov: True\n18:20:55 |     no_cuda: False\n18:20:55 |     normalize_sent_emb: False\n18:20:55 |     num_epochs: -1\n18:20:55 |     num_examples: -1\n18:20:55 |     num_workers: 0\n18:20:55 |     nus: [0.7]\n18:20:55 |     optimizer: adamax\n18:20:55 |     output_scaling: 0.06\n18:20:55 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model3', 'batchsize': 40}\"\n18:20:55 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:20:55 |     person_tokens: False\n18:20:55 |     print_scores: False\n18:20:55 |     rank_candidates: False\n18:20:55 |     rank_top_k: -1\n18:20:55 |     reduction_type: mean\n18:20:55 |     ref_class: None\n18:20:55 |     relu_dropout: 0.0\n18:20:55 |     repeat_blocking_heuristic: True\n18:20:55 |     report_filename: \n18:20:55 |     return_cand_scores: False\n18:20:55 |     save_after_valid: True\n18:20:55 |     save_every_n_secs: -1\n18:20:55 |     save_format: conversations\n18:20:55 |     share_encoders: False\n18:20:55 |     share_word_embeddings: False\n18:20:55 |     short_final_eval: False\n18:20:55 |     special_tok_lst: None\n18:20:55 |     split_lines: False\n18:20:55 |     starttime: Dec03_18-19\n18:20:55 |     task: fromfile:parlaiformat\n18:20:55 |     tensorboard_log: False\n18:20:55 |     tensorboard_logdir: None\n18:20:55 |     text_truncate: 360\n18:20:55 |     threshold: 0.5\n18:20:55 |     topk: 5\n18:20:55 |     train_predict: False\n18:20:55 |     truncate: 1024\n18:20:55 |     update_classifier_head_only: False\n18:20:55 |     update_freq: 1\n18:20:55 |     use_memories: False\n18:20:55 |     use_reply: none\n18:20:55 |     validation_cutoff: 1.0\n18:20:55 |     validation_every_n_epochs: -1\n18:20:55 |     validation_every_n_secs: 20.0\n18:20:55 |     validation_every_n_steps: -1\n18:20:55 |     validation_max_exs: -1\n18:20:55 |     validation_metric: accuracy\n18:20:55 |     validation_metric_mode: max\n18:20:55 |     validation_patience: 30\n18:20:55 |     validation_share_agent: False\n18:20:55 |     variant: xlm\n18:20:55 |     verbose: False\n18:20:55 |     wandb_entity: None\n18:20:55 |     wandb_log: False\n18:20:55 |     wandb_name: None\n18:20:55 |     wandb_project: None\n18:20:55 |     warmup_rate: 0.0001\n18:20:55 |     warmup_updates: 1000\n18:20:55 |     weight_decay: None\n18:20:55 |     world_logs: \n18:20:55 |     wrap_memory_encoder: False\n18:20:55 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:20:55 | creating task(s): fromfile:parlaiformat\n18:20:55 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:20:55 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run3/data_train-b.txt\n18:21:01 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8250 8.25e-10               .8128                 .8736   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7600            .8357              .7876   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8900 11.47   539   498       0          0 36.95  200 .8250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6393 2.805e-06   240 221.7       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     56  779 719.7        .8243\u001b[0m\n18:21:01 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8250 8.25e-10               .8128                 .8736   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7600            .8357              .7876   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8900 11.47   539   498       0          0 36.95  200 .8250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267     6 .6393 2.805e-06   240 221.7       0          0   \n    total_train_updates  tpb   tps  weighted_f1  \n                     56  779 719.7        .8243\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model3*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:21:03.178197Z","iopub.execute_input":"2022-12-03T18:21:03.178619Z","iopub.status.idle":"2022-12-03T18:21:04.277182Z","shell.execute_reply.started":"2022-12-03T18:21:03.178579Z","shell.execute_reply":"2022-12-03T18:21:04.275905Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"run 4","metadata":{}},{"cell_type":"code","source":"# run 4 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model4","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:21:04.280971Z","iopub.execute_input":"2022-12-03T18:21:04.281282Z","iopub.status.idle":"2022-12-03T18:21:58.206229Z","shell.execute_reply.started":"2022-12-03T18:21:04.281253Z","shell.execute_reply":"2022-12-03T18:21:58.205043Z"},"trusted":true},"execution_count":159,"outputs":[{"name":"stdout","text":"18:21:11 | building dictionary first...\n18:21:11 | No model with opt yet at: /tmp/model4(.opt)\n18:21:11 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:21:11 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:21:11 | Using CUDA\n18:21:11 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:21:11 | num words = 54944\n18:21:15 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:21:21 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:21:21 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:21:21 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:21:21 | Opt:\n18:21:21 |     activation: gelu\n18:21:21 |     adafactor_eps: '(1e-30, 0.001)'\n18:21:21 |     adam_eps: 1e-08\n18:21:21 |     add_p1_after_newln: False\n18:21:21 |     aggregate_micro: False\n18:21:21 |     allow_missing_init_opts: False\n18:21:21 |     attention_dropout: 0.1\n18:21:21 |     batchsize: 20\n18:21:21 |     betas: '(0.9, 0.999)'\n18:21:21 |     bpe_add_prefix_space: None\n18:21:21 |     bpe_debug: False\n18:21:21 |     bpe_dropout: None\n18:21:21 |     bpe_merge: None\n18:21:21 |     bpe_vocab: None\n18:21:21 |     candidates: inline\n18:21:21 |     cap_num_predictions: 100\n18:21:21 |     checkpoint_activations: False\n18:21:21 |     class_weights: None\n18:21:21 |     classes: \"['__notok__', '__ok__']\"\n18:21:21 |     classes_from_file: None\n18:21:21 |     data_parallel: True\n18:21:21 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:21:21 |     datatype: train\n18:21:21 |     delimiter: '\\n'\n18:21:21 |     dict_class: parlai.core.dict:DictionaryAgent\n18:21:21 |     dict_endtoken: __start__\n18:21:21 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:21:21 |     dict_include_test: False\n18:21:21 |     dict_include_valid: False\n18:21:21 |     dict_initpath: None\n18:21:21 |     dict_language: english\n18:21:21 |     dict_loaded: True\n18:21:21 |     dict_lower: True\n18:21:21 |     dict_max_ngram_size: -1\n18:21:21 |     dict_maxexs: -1\n18:21:21 |     dict_maxtokens: -1\n18:21:21 |     dict_minfreq: 0\n18:21:21 |     dict_nulltoken: __null__\n18:21:21 |     dict_starttoken: __start__\n18:21:21 |     dict_textfields: text,labels\n18:21:21 |     dict_tokenizer: bpe\n18:21:21 |     dict_unktoken: __unk__\n18:21:21 |     display_examples: False\n18:21:21 |     download_path: None\n18:21:21 |     dropout: 0.1\n18:21:21 |     dynamic_batching: None\n18:21:21 |     embedding_projection: random\n18:21:21 |     embedding_size: 768\n18:21:21 |     embedding_type: random\n18:21:21 |     embeddings_scale: False\n18:21:21 |     encode_candidate_vecs: True\n18:21:21 |     encode_candidate_vecs_batchsize: 256\n18:21:21 |     eval_batchsize: None\n18:21:21 |     eval_candidates: inline\n18:21:21 |     eval_dynamic_batching: None\n18:21:21 |     evaltask: None\n18:21:21 |     ffn_size: 3072\n18:21:21 |     final_extra_opt: \n18:21:21 |     fixed_candidate_vecs: reuse\n18:21:21 |     fixed_candidates_path: None\n18:21:21 |     force_fp16_tokens: False\n18:21:21 |     fp16: True\n18:21:21 |     fp16_impl: safe\n18:21:21 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt\n18:21:21 |     fromfile_datatype_extension: False\n18:21:21 |     gpu: -1\n18:21:21 |     gradient_clip: 0.1\n18:21:21 |     hide_labels: False\n18:21:21 |     history_add_global_end_token: None\n18:21:21 |     history_reversed: False\n18:21:21 |     history_size: 20\n18:21:21 |     ignore_bad_candidates: False\n18:21:21 |     ignore_labels: None\n18:21:21 |     image_cropsize: 224\n18:21:21 |     image_mode: raw\n18:21:21 |     image_size: 256\n18:21:21 |     inference: max\n18:21:21 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:21:21 |     init_opt: None\n18:21:21 |     interactive_candidates: fixed\n18:21:21 |     interactive_mode: False\n18:21:21 |     invsqrt_lr_decay_gamma: -1\n18:21:21 |     is_debug: False\n18:21:21 |     label_truncate: 72\n18:21:21 |     learn_embeddings: True\n18:21:21 |     learn_positional_embeddings: True\n18:21:21 |     learningrate: 5e-05\n18:21:21 |     load_from_checkpoint: False\n18:21:21 |     load_from_pretrained_ranker: True\n18:21:21 |     log_every_n_secs: 10.0\n18:21:21 |     log_every_n_steps: 50\n18:21:21 |     log_keep_fields: all\n18:21:21 |     loglevel: info\n18:21:21 |     lr_scheduler: reduceonplateau\n18:21:21 |     lr_scheduler_decay: 0.5\n18:21:21 |     lr_scheduler_patience: 3\n18:21:21 |     max_train_steps: -1\n18:21:21 |     max_train_time: 7200.0\n18:21:21 |     memory_attention: sqrt\n18:21:21 |     metrics: default\n18:21:21 |     model: transformer/classifier\n18:21:21 |     model_file: /tmp/model4\n18:21:21 |     model_parallel: False\n18:21:21 |     momentum: 0\n18:21:21 |     multitask_weights: [1]\n18:21:21 |     mutators: None\n18:21:21 |     n_decoder_layers: -1\n18:21:21 |     n_encoder_layers: -1\n18:21:21 |     n_heads: 12\n18:21:21 |     n_layers: 12\n18:21:21 |     n_positions: 1024\n18:21:21 |     n_segments: 2\n18:21:21 |     nesterov: True\n18:21:21 |     no_cuda: False\n18:21:21 |     normalize_sent_emb: False\n18:21:21 |     num_epochs: -1\n18:21:21 |     num_workers: 0\n18:21:21 |     nus: (0.7,)\n18:21:21 |     optimizer: adamax\n18:21:21 |     output_scaling: 0.06\n18:21:21 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model4'}\"\n18:21:21 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:21:21 |     person_tokens: False\n18:21:21 |     print_scores: False\n18:21:21 |     rank_candidates: False\n18:21:21 |     rank_top_k: -1\n18:21:21 |     reduction_type: mean\n18:21:21 |     ref_class: None\n18:21:21 |     relu_dropout: 0.0\n18:21:21 |     repeat_blocking_heuristic: True\n18:21:21 |     return_cand_scores: False\n18:21:21 |     save_after_valid: True\n18:21:21 |     save_every_n_secs: -1\n18:21:21 |     save_format: conversations\n18:21:21 |     share_encoders: False\n18:21:21 |     share_word_embeddings: False\n18:21:21 |     short_final_eval: False\n18:21:21 |     special_tok_lst: None\n18:21:21 |     split_lines: False\n18:21:21 |     starttime: Dec03_18-21\n18:21:21 |     task: fromfile:parlaiformat\n18:21:21 |     tensorboard_log: False\n18:21:21 |     tensorboard_logdir: None\n18:21:21 |     text_truncate: 360\n18:21:21 |     threshold: 0.5\n18:21:21 |     topk: 5\n18:21:21 |     train_predict: False\n18:21:21 |     truncate: 1024\n18:21:21 |     update_classifier_head_only: False\n18:21:21 |     update_freq: 1\n18:21:21 |     use_memories: False\n18:21:21 |     use_reply: none\n18:21:21 |     validation_cutoff: 1.0\n18:21:21 |     validation_every_n_epochs: -1\n18:21:21 |     validation_every_n_secs: 20.0\n18:21:21 |     validation_every_n_steps: -1\n18:21:21 |     validation_max_exs: -1\n18:21:21 |     validation_metric: accuracy\n18:21:21 |     validation_metric_mode: max\n18:21:21 |     validation_patience: 30\n18:21:21 |     validation_share_agent: False\n18:21:21 |     variant: xlm\n18:21:21 |     verbose: False\n18:21:21 |     wandb_entity: None\n18:21:21 |     wandb_log: False\n18:21:21 |     wandb_name: None\n18:21:21 |     wandb_project: None\n18:21:21 |     warmup_rate: 0.0001\n18:21:21 |     warmup_updates: 1000\n18:21:21 |     weight_decay: None\n18:21:21 |     world_logs: \n18:21:21 |     wrap_memory_encoder: False\n18:21:22 | creating task(s): fromfile:parlaiformat\n18:21:22 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt\n18:21:22 | training...\n18:21:32 | time:10s total_exs:360 total_steps:18 epochs:15.00\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .6639 6.639e-10               .5434                 .8276   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .4045            .7341              .6117   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9176 12.02     1 280.4 505.4       0          0 36.05  360   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .6639             32768  2.456    .1189 5.989 .6676 9.049e-07 119.8 215.9   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   18 400.2 721.3 1.806        .6398\n\n18:21:42 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .8692 8.692e-10               .8547                 .9494   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7772            .8811              .8147   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9594 12.11     1 282.2  1110       0          0 78.65  780   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .8692             32768  2.578    .1189  5.99 .6219 2.855e-06 119.8 471.1   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 401.9 1581 3.942        .8680\n\n18:21:42 | creating task(s): fromfile:parlaiformat\n18:21:42 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:21:42 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt\n18:21:42 | running eval: valid\n18:21:42 | eval completed in 0.19s\n18:21:42 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1973       0          0 140.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08088     6 .5514 2.855e-06    72 842.9       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2816            1\n\u001b[0m\n18:21:42 | \u001b[1;32mnew best accuracy: 1\u001b[0m\n18:21:42 | saving best valid model: /tmp/model4\n18:21:42 | Saving dictionary to /tmp/model4.dict\n18:21:45 | task solved! stopping.\n18:21:45 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:21:45 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:21:45 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:21:45 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:21:45 | Using CUDA\n18:21:45 | loading dictionary from /tmp/model4.dict\n18:21:46 | num words = 54944\n18:21:51 | Loading existing model parameters from /tmp/model4\n18:21:54 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:21:55 | creating task(s): fromfile:parlaiformat\n18:21:55 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:21:55 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt\n18:21:56 | running eval: valid\n18:21:56 | eval completed in 0.23s\n18:21:56 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1720       0          0 122.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5514 2.855e-06    72   735       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2455            1\n\u001b[0m\n18:21:56 | creating task(s): fromfile:parlaiformat\n18:21:56 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:21:56 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt\n18:21:56 | running eval: test\n18:21:56 | eval completed in 0.22s\n18:21:56 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 12.04 168.5  1727       0          0 122.9   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .5514 2.855e-06    72 737.8       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                     57 240.5 2465            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-a.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:21:58.209931Z","iopub.execute_input":"2022-12-03T18:21:58.210250Z","iopub.status.idle":"2022-12-03T18:22:25.722543Z","shell.execute_reply.started":"2022-12-03T18:21:58.210220Z","shell.execute_reply":"2022-12-03T18:22:25.720970Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stdout","text":"18:22:05 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt)\u001b[0m\n18:22:05 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:22:05 | Using CUDA\n18:22:05 | loading dictionary from /tmp/model4.dict\n18:22:05 | num words = 54944\n18:22:09 | Loading existing model parameters from /tmp/model4\n18:22:16 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:22:18 | Opt:\n18:22:18 |     activation: gelu\n18:22:18 |     adafactor_eps: '[1e-30, 0.001]'\n18:22:18 |     adam_eps: 1e-08\n18:22:18 |     add_p1_after_newln: False\n18:22:18 |     aggregate_micro: False\n18:22:18 |     allow_missing_init_opts: False\n18:22:18 |     area_under_curve_class: None\n18:22:18 |     area_under_curve_digits: -1\n18:22:18 |     attention_dropout: 0.1\n18:22:18 |     batchsize: 40\n18:22:18 |     betas: '[0.9, 0.999]'\n18:22:18 |     bpe_add_prefix_space: None\n18:22:18 |     bpe_debug: False\n18:22:18 |     bpe_dropout: None\n18:22:18 |     bpe_merge: None\n18:22:18 |     bpe_vocab: None\n18:22:18 |     candidates: inline\n18:22:18 |     cap_num_predictions: 100\n18:22:18 |     checkpoint_activations: False\n18:22:18 |     class_weights: None\n18:22:18 |     classes: \"['__notok__', '__ok__']\"\n18:22:18 |     classes_from_file: None\n18:22:18 |     data_parallel: True\n18:22:18 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:22:18 |     datatype: train\n18:22:18 |     delimiter: '\\n'\n18:22:18 |     dict_class: parlai.core.dict:DictionaryAgent\n18:22:18 |     dict_endtoken: __start__\n18:22:18 |     dict_file: /tmp/model4.dict\n18:22:18 |     dict_include_test: False\n18:22:18 |     dict_include_valid: False\n18:22:18 |     dict_initpath: None\n18:22:18 |     dict_language: english\n18:22:18 |     dict_loaded: True\n18:22:18 |     dict_lower: True\n18:22:18 |     dict_max_ngram_size: -1\n18:22:18 |     dict_maxexs: -1\n18:22:18 |     dict_maxtokens: -1\n18:22:18 |     dict_minfreq: 0\n18:22:18 |     dict_nulltoken: __null__\n18:22:18 |     dict_starttoken: __start__\n18:22:18 |     dict_textfields: text,labels\n18:22:18 |     dict_tokenizer: bpe\n18:22:18 |     dict_unktoken: __unk__\n18:22:18 |     display_examples: False\n18:22:18 |     download_path: None\n18:22:18 |     dropout: 0.1\n18:22:18 |     dynamic_batching: None\n18:22:18 |     embedding_projection: random\n18:22:18 |     embedding_size: 768\n18:22:18 |     embedding_type: random\n18:22:18 |     embeddings_scale: False\n18:22:18 |     encode_candidate_vecs: True\n18:22:18 |     encode_candidate_vecs_batchsize: 256\n18:22:18 |     eval_batchsize: None\n18:22:18 |     eval_candidates: inline\n18:22:18 |     eval_dynamic_batching: None\n18:22:18 |     evaltask: None\n18:22:18 |     ffn_size: 3072\n18:22:18 |     final_extra_opt: \n18:22:18 |     fixed_candidate_vecs: reuse\n18:22:18 |     fixed_candidates_path: None\n18:22:18 |     force_fp16_tokens: True\n18:22:18 |     fp16: True\n18:22:18 |     fp16_impl: safe\n18:22:18 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-a.txt\n18:22:18 |     fromfile_datatype_extension: False\n18:22:18 |     gpu: -1\n18:22:18 |     gradient_clip: 0.1\n18:22:18 |     hide_labels: False\n18:22:18 |     history_add_global_end_token: None\n18:22:18 |     history_reversed: False\n18:22:18 |     history_size: 20\n18:22:18 |     ignore_bad_candidates: False\n18:22:18 |     ignore_labels: None\n18:22:18 |     image_cropsize: 224\n18:22:18 |     image_mode: raw\n18:22:18 |     image_size: 256\n18:22:18 |     inference: max\n18:22:18 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:22:18 |     init_opt: None\n18:22:18 |     interactive_candidates: fixed\n18:22:18 |     interactive_mode: False\n18:22:18 |     invsqrt_lr_decay_gamma: -1\n18:22:18 |     is_debug: False\n18:22:18 |     label_truncate: 72\n18:22:18 |     learn_embeddings: True\n18:22:18 |     learn_positional_embeddings: True\n18:22:18 |     learningrate: 5e-05\n18:22:18 |     load_from_pretrained_ranker: True\n18:22:18 |     log_every_n_secs: 10.0\n18:22:18 |     log_every_n_steps: 50\n18:22:18 |     log_keep_fields: all\n18:22:18 |     loglevel: info\n18:22:18 |     lr_scheduler: reduceonplateau\n18:22:18 |     lr_scheduler_decay: 0.5\n18:22:18 |     lr_scheduler_patience: 3\n18:22:18 |     max_train_steps: -1\n18:22:18 |     max_train_time: 7200.0\n18:22:18 |     memory_attention: sqrt\n18:22:18 |     metrics: default\n18:22:18 |     model: transformer/classifier\n18:22:18 |     model_file: /tmp/model4\n18:22:18 |     model_parallel: False\n18:22:18 |     momentum: 0\n18:22:18 |     multitask_weights: [1]\n18:22:18 |     mutators: None\n18:22:18 |     n_decoder_layers: -1\n18:22:18 |     n_encoder_layers: -1\n18:22:18 |     n_heads: 12\n18:22:18 |     n_layers: 12\n18:22:18 |     n_positions: 1024\n18:22:18 |     n_segments: 2\n18:22:18 |     nesterov: True\n18:22:18 |     no_cuda: False\n18:22:18 |     normalize_sent_emb: False\n18:22:18 |     num_epochs: -1\n18:22:18 |     num_examples: -1\n18:22:18 |     num_workers: 0\n18:22:18 |     nus: [0.7]\n18:22:18 |     optimizer: adamax\n18:22:18 |     output_scaling: 0.06\n18:22:18 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n18:22:18 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:22:18 |     person_tokens: False\n18:22:18 |     print_scores: False\n18:22:18 |     rank_candidates: False\n18:22:18 |     rank_top_k: -1\n18:22:18 |     reduction_type: mean\n18:22:18 |     ref_class: None\n18:22:18 |     relu_dropout: 0.0\n18:22:18 |     repeat_blocking_heuristic: True\n18:22:18 |     report_filename: \n18:22:18 |     return_cand_scores: False\n18:22:18 |     save_after_valid: True\n18:22:18 |     save_every_n_secs: -1\n18:22:18 |     save_format: conversations\n18:22:18 |     share_encoders: False\n18:22:18 |     share_word_embeddings: False\n18:22:18 |     short_final_eval: False\n18:22:18 |     special_tok_lst: None\n18:22:18 |     split_lines: False\n18:22:18 |     starttime: Dec03_18-21\n18:22:18 |     task: fromfile:parlaiformat\n18:22:18 |     tensorboard_log: False\n18:22:18 |     tensorboard_logdir: None\n18:22:18 |     text_truncate: 360\n18:22:18 |     threshold: 0.5\n18:22:18 |     topk: 5\n18:22:18 |     train_predict: False\n18:22:18 |     truncate: 1024\n18:22:18 |     update_classifier_head_only: False\n18:22:18 |     update_freq: 1\n18:22:18 |     use_memories: False\n18:22:18 |     use_reply: none\n18:22:18 |     validation_cutoff: 1.0\n18:22:18 |     validation_every_n_epochs: -1\n18:22:18 |     validation_every_n_secs: 20.0\n18:22:18 |     validation_every_n_steps: -1\n18:22:18 |     validation_max_exs: -1\n18:22:18 |     validation_metric: accuracy\n18:22:18 |     validation_metric_mode: max\n18:22:18 |     validation_patience: 30\n18:22:18 |     validation_share_agent: False\n18:22:18 |     variant: xlm\n18:22:18 |     verbose: False\n18:22:18 |     wandb_entity: None\n18:22:18 |     wandb_log: False\n18:22:18 |     wandb_name: None\n18:22:18 |     wandb_project: None\n18:22:18 |     warmup_rate: 0.0001\n18:22:18 |     warmup_updates: 1000\n18:22:18 |     weight_decay: None\n18:22:18 |     world_logs: \n18:22:18 |     wrap_memory_encoder: False\n18:22:18 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:22:18 | creating task(s): fromfile:parlaiformat\n18:22:18 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:22:18 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-a.txt\n18:22:24 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2250 2.25e-10               .1040                 .1250   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                    .08911            .3172              .2812   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3636 11.46 538.2 507.7       0          0 37.73  200 .2250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .7484 2.855e-06 240.4 226.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 778.6 734.5        .2095\u001b[0m\n18:22:24 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .2250 2.25e-10               .1040                 .1250   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                    .08911            .3172              .2812   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .3636 11.46 538.2 507.7       0          0 37.73  200 .2250   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  6.01 .7484 2.855e-06 240.4 226.8       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 778.6 734.5        .2095\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-b.txt -m transformer/classifier -mf /tmp/model4 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:22:25.724281Z","iopub.execute_input":"2022-12-03T18:22:25.726436Z","iopub.status.idle":"2022-12-03T18:22:52.458724Z","shell.execute_reply.started":"2022-12-03T18:22:25.726387Z","shell.execute_reply":"2022-12-03T18:22:52.457523Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"18:22:33 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_valid.txt)\u001b[0m\n18:22:33 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:22:33 | Using CUDA\n18:22:33 | loading dictionary from /tmp/model4.dict\n18:22:33 | num words = 54944\n18:22:37 | Loading existing model parameters from /tmp/model4\n18:22:43 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:22:44 | Opt:\n18:22:44 |     activation: gelu\n18:22:44 |     adafactor_eps: '[1e-30, 0.001]'\n18:22:44 |     adam_eps: 1e-08\n18:22:44 |     add_p1_after_newln: False\n18:22:44 |     aggregate_micro: False\n18:22:44 |     allow_missing_init_opts: False\n18:22:44 |     area_under_curve_class: None\n18:22:44 |     area_under_curve_digits: -1\n18:22:44 |     attention_dropout: 0.1\n18:22:44 |     batchsize: 40\n18:22:44 |     betas: '[0.9, 0.999]'\n18:22:44 |     bpe_add_prefix_space: None\n18:22:44 |     bpe_debug: False\n18:22:44 |     bpe_dropout: None\n18:22:44 |     bpe_merge: None\n18:22:44 |     bpe_vocab: None\n18:22:44 |     candidates: inline\n18:22:44 |     cap_num_predictions: 100\n18:22:44 |     checkpoint_activations: False\n18:22:44 |     class_weights: None\n18:22:44 |     classes: \"['__notok__', '__ok__']\"\n18:22:44 |     classes_from_file: None\n18:22:44 |     data_parallel: True\n18:22:44 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:22:44 |     datatype: train\n18:22:44 |     delimiter: '\\n'\n18:22:44 |     dict_class: parlai.core.dict:DictionaryAgent\n18:22:44 |     dict_endtoken: __start__\n18:22:44 |     dict_file: /tmp/model4.dict\n18:22:44 |     dict_include_test: False\n18:22:44 |     dict_include_valid: False\n18:22:44 |     dict_initpath: None\n18:22:44 |     dict_language: english\n18:22:44 |     dict_loaded: True\n18:22:44 |     dict_lower: True\n18:22:44 |     dict_max_ngram_size: -1\n18:22:44 |     dict_maxexs: -1\n18:22:44 |     dict_maxtokens: -1\n18:22:44 |     dict_minfreq: 0\n18:22:44 |     dict_nulltoken: __null__\n18:22:44 |     dict_starttoken: __start__\n18:22:44 |     dict_textfields: text,labels\n18:22:44 |     dict_tokenizer: bpe\n18:22:44 |     dict_unktoken: __unk__\n18:22:44 |     display_examples: False\n18:22:44 |     download_path: None\n18:22:44 |     dropout: 0.1\n18:22:44 |     dynamic_batching: None\n18:22:44 |     embedding_projection: random\n18:22:44 |     embedding_size: 768\n18:22:44 |     embedding_type: random\n18:22:44 |     embeddings_scale: False\n18:22:44 |     encode_candidate_vecs: True\n18:22:44 |     encode_candidate_vecs_batchsize: 256\n18:22:44 |     eval_batchsize: None\n18:22:44 |     eval_candidates: inline\n18:22:44 |     eval_dynamic_batching: None\n18:22:44 |     evaltask: None\n18:22:44 |     ffn_size: 3072\n18:22:44 |     final_extra_opt: \n18:22:44 |     fixed_candidate_vecs: reuse\n18:22:44 |     fixed_candidates_path: None\n18:22:44 |     force_fp16_tokens: True\n18:22:44 |     fp16: True\n18:22:44 |     fp16_impl: safe\n18:22:44 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-b.txt\n18:22:44 |     fromfile_datatype_extension: False\n18:22:44 |     gpu: -1\n18:22:44 |     gradient_clip: 0.1\n18:22:44 |     hide_labels: False\n18:22:44 |     history_add_global_end_token: None\n18:22:44 |     history_reversed: False\n18:22:44 |     history_size: 20\n18:22:44 |     ignore_bad_candidates: False\n18:22:44 |     ignore_labels: None\n18:22:44 |     image_cropsize: 224\n18:22:44 |     image_mode: raw\n18:22:44 |     image_size: 256\n18:22:44 |     inference: max\n18:22:44 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:22:44 |     init_opt: None\n18:22:44 |     interactive_candidates: fixed\n18:22:44 |     interactive_mode: False\n18:22:44 |     invsqrt_lr_decay_gamma: -1\n18:22:44 |     is_debug: False\n18:22:44 |     label_truncate: 72\n18:22:44 |     learn_embeddings: True\n18:22:44 |     learn_positional_embeddings: True\n18:22:44 |     learningrate: 5e-05\n18:22:44 |     load_from_pretrained_ranker: True\n18:22:44 |     log_every_n_secs: 10.0\n18:22:44 |     log_every_n_steps: 50\n18:22:44 |     log_keep_fields: all\n18:22:44 |     loglevel: info\n18:22:44 |     lr_scheduler: reduceonplateau\n18:22:44 |     lr_scheduler_decay: 0.5\n18:22:44 |     lr_scheduler_patience: 3\n18:22:44 |     max_train_steps: -1\n18:22:44 |     max_train_time: 7200.0\n18:22:44 |     memory_attention: sqrt\n18:22:44 |     metrics: default\n18:22:44 |     model: transformer/classifier\n18:22:44 |     model_file: /tmp/model4\n18:22:44 |     model_parallel: False\n18:22:44 |     momentum: 0\n18:22:44 |     multitask_weights: [1]\n18:22:44 |     mutators: None\n18:22:44 |     n_decoder_layers: -1\n18:22:44 |     n_encoder_layers: -1\n18:22:44 |     n_heads: 12\n18:22:44 |     n_layers: 12\n18:22:44 |     n_positions: 1024\n18:22:44 |     n_segments: 2\n18:22:44 |     nesterov: True\n18:22:44 |     no_cuda: False\n18:22:44 |     normalize_sent_emb: False\n18:22:44 |     num_epochs: -1\n18:22:44 |     num_examples: -1\n18:22:44 |     num_workers: 0\n18:22:44 |     nus: [0.7]\n18:22:44 |     optimizer: adamax\n18:22:44 |     output_scaling: 0.06\n18:22:44 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model4', 'batchsize': 40}\"\n18:22:44 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:22:44 |     person_tokens: False\n18:22:44 |     print_scores: False\n18:22:44 |     rank_candidates: False\n18:22:44 |     rank_top_k: -1\n18:22:44 |     reduction_type: mean\n18:22:44 |     ref_class: None\n18:22:44 |     relu_dropout: 0.0\n18:22:44 |     repeat_blocking_heuristic: True\n18:22:44 |     report_filename: \n18:22:44 |     return_cand_scores: False\n18:22:44 |     save_after_valid: True\n18:22:44 |     save_every_n_secs: -1\n18:22:44 |     save_format: conversations\n18:22:44 |     share_encoders: False\n18:22:44 |     share_word_embeddings: False\n18:22:44 |     short_final_eval: False\n18:22:44 |     special_tok_lst: None\n18:22:44 |     split_lines: False\n18:22:44 |     starttime: Dec03_18-21\n18:22:44 |     task: fromfile:parlaiformat\n18:22:44 |     tensorboard_log: False\n18:22:44 |     tensorboard_logdir: None\n18:22:44 |     text_truncate: 360\n18:22:44 |     threshold: 0.5\n18:22:44 |     topk: 5\n18:22:44 |     train_predict: False\n18:22:44 |     truncate: 1024\n18:22:44 |     update_classifier_head_only: False\n18:22:44 |     update_freq: 1\n18:22:44 |     use_memories: False\n18:22:44 |     use_reply: none\n18:22:44 |     validation_cutoff: 1.0\n18:22:44 |     validation_every_n_epochs: -1\n18:22:44 |     validation_every_n_secs: 20.0\n18:22:44 |     validation_every_n_steps: -1\n18:22:44 |     validation_max_exs: -1\n18:22:44 |     validation_metric: accuracy\n18:22:44 |     validation_metric_mode: max\n18:22:44 |     validation_patience: 30\n18:22:44 |     validation_share_agent: False\n18:22:44 |     variant: xlm\n18:22:44 |     verbose: False\n18:22:44 |     wandb_entity: None\n18:22:44 |     wandb_log: False\n18:22:44 |     wandb_name: None\n18:22:44 |     wandb_project: None\n18:22:44 |     warmup_rate: 0.0001\n18:22:44 |     warmup_updates: 1000\n18:22:44 |     weight_decay: None\n18:22:44 |     world_logs: \n18:22:44 |     wrap_memory_encoder: False\n18:22:44 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:22:44 | creating task(s): fromfile:parlaiformat\n18:22:44 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:22:44 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run4/data_train-b.txt\n18:22:50 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7750 7.75e-10               .7368                 .8750   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6364            .8035              .7188   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9109 11.46 538.2 472.7       0          0 35.13  200 .7750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .6459 2.855e-06 239.6 210.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 777.8 683.2        .7705\u001b[0m\n18:22:50 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .7750 7.75e-10               .7368                 .8750   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .6364            .8035              .7188   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9109 11.46 538.2 472.7       0          0 35.13  200 .7750   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08346  5.99 .6459 2.855e-06 239.6 210.5       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                     57 777.8 683.2        .7705\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model4*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:22:52.460880Z","iopub.execute_input":"2022-12-03T18:22:52.461281Z","iopub.status.idle":"2022-12-03T18:22:53.549449Z","shell.execute_reply.started":"2022-12-03T18:22:52.461242Z","shell.execute_reply":"2022-12-03T18:22:53.548067Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"markdown","source":"run 5","metadata":{}},{"cell_type":"code","source":"# run 5 training\n!parlai train_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt --model transformer/classifier --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model --dict-file zoo:pretrained_transformers/bi_model_huge_reddit/model.dict --dict-tokenizer bpe --dict-lower True --output-scaling 0.06 --variant xlm --n-layers 12 --n-heads 12 --learn-positional-embeddings True --ffn-size 3072 --n-positions 1024 --embedding-size 768 --activation gelu  --embeddings-scale False --n-segments 2 --dict-endtoken __start__  --classes __notok__ __ok__ --reduction-type mean --learn-embeddings True --share-word-embeddings False --load-from-pretrained-ranker True --optimizer adamax --max-train-time -1 --share-encoders False -lr 5e-05 --history-size 20 --label-truncate 72 --text-truncate 360 --dropout 0.1 --attention-dropout 0.1 --gradient-clip 0.1 --validation-metric accuracy --validation-metric-mode max --validation-patience 30 --validation-every-n-secs 20 --log-every-n-secs 10 -ttim 7200 --load-from-checkpoint False --lr_scheduler reduceonplateau --lr-scheduler-patience 3 --save-after-valid true --update-freq 1 --fp16 true --betas 0.9,0.999 --warmup-updates 1000 --data-parallel true -bs 20 --model-file /tmp/model5","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:22:53.551614Z","iopub.execute_input":"2022-12-03T18:22:53.551995Z","iopub.status.idle":"2022-12-03T18:24:19.376425Z","shell.execute_reply.started":"2022-12-03T18:22:53.551955Z","shell.execute_reply":"2022-12-03T18:24:19.375214Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"18:23:01 | building dictionary first...\n18:23:01 | No model with opt yet at: /tmp/model5(.opt)\n18:23:01 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: None,verbose: False,is_debug: False,datapath: /opt/conda/lib/python3.7/site-packages/data,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,load_from_checkpoint: False,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,interactive_mode: False,fp16_impl: safe,force_fp16_tokens: False,adam_eps: 1e-08,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True\u001b[0m\n18:23:01 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:23:01 | Using CUDA\n18:23:01 | loading dictionary from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:23:01 | num words = 54944\n18:23:05 | Loading existing model parameters from /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:23:13 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:23:13 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n18:23:13 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n18:23:13 | Opt:\n18:23:13 |     activation: gelu\n18:23:13 |     adafactor_eps: '(1e-30, 0.001)'\n18:23:13 |     adam_eps: 1e-08\n18:23:13 |     add_p1_after_newln: False\n18:23:13 |     aggregate_micro: False\n18:23:13 |     allow_missing_init_opts: False\n18:23:13 |     attention_dropout: 0.1\n18:23:13 |     batchsize: 20\n18:23:13 |     betas: '(0.9, 0.999)'\n18:23:13 |     bpe_add_prefix_space: None\n18:23:13 |     bpe_debug: False\n18:23:13 |     bpe_dropout: None\n18:23:13 |     bpe_merge: None\n18:23:13 |     bpe_vocab: None\n18:23:13 |     candidates: inline\n18:23:13 |     cap_num_predictions: 100\n18:23:13 |     checkpoint_activations: False\n18:23:13 |     class_weights: None\n18:23:13 |     classes: \"['__notok__', '__ok__']\"\n18:23:13 |     classes_from_file: None\n18:23:13 |     data_parallel: True\n18:23:13 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:23:13 |     datatype: train\n18:23:13 |     delimiter: '\\n'\n18:23:13 |     dict_class: parlai.core.dict:DictionaryAgent\n18:23:13 |     dict_endtoken: __start__\n18:23:13 |     dict_file: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict\n18:23:13 |     dict_include_test: False\n18:23:13 |     dict_include_valid: False\n18:23:13 |     dict_initpath: None\n18:23:13 |     dict_language: english\n18:23:13 |     dict_loaded: True\n18:23:13 |     dict_lower: True\n18:23:13 |     dict_max_ngram_size: -1\n18:23:13 |     dict_maxexs: -1\n18:23:13 |     dict_maxtokens: -1\n18:23:13 |     dict_minfreq: 0\n18:23:13 |     dict_nulltoken: __null__\n18:23:13 |     dict_starttoken: __start__\n18:23:13 |     dict_textfields: text,labels\n18:23:13 |     dict_tokenizer: bpe\n18:23:13 |     dict_unktoken: __unk__\n18:23:13 |     display_examples: False\n18:23:13 |     download_path: None\n18:23:13 |     dropout: 0.1\n18:23:13 |     dynamic_batching: None\n18:23:13 |     embedding_projection: random\n18:23:13 |     embedding_size: 768\n18:23:13 |     embedding_type: random\n18:23:13 |     embeddings_scale: False\n18:23:13 |     encode_candidate_vecs: True\n18:23:13 |     encode_candidate_vecs_batchsize: 256\n18:23:13 |     eval_batchsize: None\n18:23:13 |     eval_candidates: inline\n18:23:13 |     eval_dynamic_batching: None\n18:23:13 |     evaltask: None\n18:23:13 |     ffn_size: 3072\n18:23:13 |     final_extra_opt: \n18:23:13 |     fixed_candidate_vecs: reuse\n18:23:13 |     fixed_candidates_path: None\n18:23:13 |     force_fp16_tokens: False\n18:23:13 |     fp16: True\n18:23:13 |     fp16_impl: safe\n18:23:13 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt\n18:23:13 |     fromfile_datatype_extension: False\n18:23:13 |     gpu: -1\n18:23:13 |     gradient_clip: 0.1\n18:23:13 |     hide_labels: False\n18:23:13 |     history_add_global_end_token: None\n18:23:13 |     history_reversed: False\n18:23:13 |     history_size: 20\n18:23:13 |     ignore_bad_candidates: False\n18:23:13 |     ignore_labels: None\n18:23:13 |     image_cropsize: 224\n18:23:13 |     image_mode: raw\n18:23:13 |     image_size: 256\n18:23:13 |     inference: max\n18:23:13 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:23:13 |     init_opt: None\n18:23:13 |     interactive_candidates: fixed\n18:23:13 |     interactive_mode: False\n18:23:13 |     invsqrt_lr_decay_gamma: -1\n18:23:13 |     is_debug: False\n18:23:13 |     label_truncate: 72\n18:23:13 |     learn_embeddings: True\n18:23:13 |     learn_positional_embeddings: True\n18:23:13 |     learningrate: 5e-05\n18:23:13 |     load_from_checkpoint: False\n18:23:13 |     load_from_pretrained_ranker: True\n18:23:13 |     log_every_n_secs: 10.0\n18:23:13 |     log_every_n_steps: 50\n18:23:13 |     log_keep_fields: all\n18:23:13 |     loglevel: info\n18:23:13 |     lr_scheduler: reduceonplateau\n18:23:13 |     lr_scheduler_decay: 0.5\n18:23:13 |     lr_scheduler_patience: 3\n18:23:13 |     max_train_steps: -1\n18:23:13 |     max_train_time: 7200.0\n18:23:13 |     memory_attention: sqrt\n18:23:13 |     metrics: default\n18:23:13 |     model: transformer/classifier\n18:23:13 |     model_file: /tmp/model5\n18:23:13 |     model_parallel: False\n18:23:13 |     momentum: 0\n18:23:13 |     multitask_weights: [1]\n18:23:13 |     mutators: None\n18:23:13 |     n_decoder_layers: -1\n18:23:13 |     n_encoder_layers: -1\n18:23:13 |     n_heads: 12\n18:23:13 |     n_layers: 12\n18:23:13 |     n_positions: 1024\n18:23:13 |     n_segments: 2\n18:23:13 |     nesterov: True\n18:23:13 |     no_cuda: False\n18:23:13 |     normalize_sent_emb: False\n18:23:13 |     num_epochs: -1\n18:23:13 |     num_workers: 0\n18:23:13 |     nus: (0.7,)\n18:23:13 |     optimizer: adamax\n18:23:13 |     output_scaling: 0.06\n18:23:13 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt', 'model': 'transformer/classifier', 'init_model': 'zoo:pretrained_transformers/bi_model_huge_reddit/model', 'dict_file': '/opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model.dict', 'dict_tokenizer': 'bpe', 'dict_lower': True, 'output_scaling': 0.06, 'variant': 'xlm', 'n_layers': 12, 'n_heads': 12, 'learn_positional_embeddings': True, 'ffn_size': 3072, 'n_positions': 1024, 'embedding_size': 768, 'activation': 'gelu', 'embeddings_scale': False, 'n_segments': 2, 'dict_endtoken': '__start__', 'classes': ['__notok__', '__ok__'], 'reduction_type': 'mean', 'learn_embeddings': True, 'share_word_embeddings': False, 'load_from_pretrained_ranker': True, 'optimizer': 'adamax', 'max_train_time': 7200.0, 'share_encoders': False, 'learningrate': 5e-05, 'history_size': 20, 'label_truncate': 72, 'text_truncate': 360, 'dropout': 0.1, 'attention_dropout': 0.1, 'gradient_clip': 0.1, 'validation_metric': 'accuracy', 'validation_metric_mode': 'max', 'validation_patience': 30, 'validation_every_n_secs': 20.0, 'log_every_n_secs': 10.0, 'load_from_checkpoint': False, 'lr_scheduler': 'reduceonplateau', 'lr_scheduler_patience': 3, 'save_after_valid': True, 'update_freq': 1, 'fp16': True, 'betas': (0.9, 0.999), 'warmup_updates': 1000, 'data_parallel': True, 'batchsize': 20, 'model_file': '/tmp/model5'}\"\n18:23:13 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:23:13 |     person_tokens: False\n18:23:13 |     print_scores: False\n18:23:13 |     rank_candidates: False\n18:23:13 |     rank_top_k: -1\n18:23:13 |     reduction_type: mean\n18:23:13 |     ref_class: None\n18:23:13 |     relu_dropout: 0.0\n18:23:13 |     repeat_blocking_heuristic: True\n18:23:13 |     return_cand_scores: False\n18:23:13 |     save_after_valid: True\n18:23:13 |     save_every_n_secs: -1\n18:23:13 |     save_format: conversations\n18:23:13 |     share_encoders: False\n18:23:13 |     share_word_embeddings: False\n18:23:13 |     short_final_eval: False\n18:23:13 |     special_tok_lst: None\n18:23:13 |     split_lines: False\n18:23:13 |     starttime: Dec03_18-23\n18:23:13 |     task: fromfile:parlaiformat\n18:23:13 |     tensorboard_log: False\n18:23:13 |     tensorboard_logdir: None\n18:23:13 |     text_truncate: 360\n18:23:13 |     threshold: 0.5\n18:23:13 |     topk: 5\n18:23:13 |     train_predict: False\n18:23:13 |     truncate: 1024\n18:23:13 |     update_classifier_head_only: False\n18:23:13 |     update_freq: 1\n18:23:13 |     use_memories: False\n18:23:13 |     use_reply: none\n18:23:13 |     validation_cutoff: 1.0\n18:23:13 |     validation_every_n_epochs: -1\n18:23:13 |     validation_every_n_secs: 20.0\n18:23:13 |     validation_every_n_steps: -1\n18:23:13 |     validation_max_exs: -1\n18:23:13 |     validation_metric: accuracy\n18:23:13 |     validation_metric_mode: max\n18:23:13 |     validation_patience: 30\n18:23:13 |     validation_share_agent: False\n18:23:13 |     variant: xlm\n18:23:13 |     verbose: False\n18:23:13 |     wandb_entity: None\n18:23:13 |     wandb_log: False\n18:23:13 |     wandb_name: None\n18:23:13 |     wandb_project: None\n18:23:13 |     warmup_rate: 0.0001\n18:23:13 |     warmup_updates: 1000\n18:23:13 |     weight_decay: None\n18:23:13 |     world_logs: \n18:23:13 |     wrap_memory_encoder: False\n18:23:14 | creating task(s): fromfile:parlaiformat\n18:23:14 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt\n18:23:14 | training...\n18:23:24 | time:10s total_exs:400 total_steps:20 epochs:16.67\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .5625 5.625e-10               .6407                 .5632   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .7429            .4409              .5610   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .3632 10.52     1 250.4 500.6       0          0 39.98  400   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .5625             32768  2.893    .1206  6.05 .6804 1.005e-06   121 241.9   \n    ltrunc  ltrunclen  total_train_updates   tpb   tps   ups  weighted_f1  \n         0          0                   20 371.4 742.5 2.003        .5458\n\n18:23:34 | time:20s total_exs:1140 total_steps:57 epochs:47.50\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .7568 7.568e-10               .7907                 .7083   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .8947            .7097              .8462   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .6111 10.91     1 258.2 956.6       0          0 74.08  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .7568             32768  2.608    .1207 6.027 .6309 2.855e-06 120.5 446.5   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                   57 378.8 1403 3.713        .7513\n\n18:23:34 | creating task(s): fromfile:parlaiformat\n18:23:34 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:23:34 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt\n18:23:34 | running eval: valid\n18:23:34 | eval completed in 0.20s\n18:23:34 | \u001b[1mvalid:\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9583 9.583e-10               .9600                 .9231   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9565                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .9167 10.67   152  1641       0          0 129.5   24 .9583   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08089     6 .5684 2.855e-06    72 777.3       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                     57  224 2419        .9583\n\u001b[0m\n18:23:34 | \u001b[1;32mnew best accuracy: 0.9583\u001b[0m\n18:23:34 | saving best valid model: /tmp/model5\n18:23:34 | Saving dictionary to /tmp/model5.dict\n18:23:38 | saving model checkpoint: /tmp/model5.checkpoint\n18:23:38 | Saving dictionary to /tmp/model5.checkpoint.dict\n18:23:56 | time:42s total_exs:1880 total_steps:94 epochs:78.33\n    accuracy    bleu-4  class___notok___f1  class___notok___prec  \\\n       .9851 9.851e-10               .9861                 .9725   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1            .9841                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                  .9687 10.65     1 252.9 916.3       0          0 72.45  740   \n      f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n   .9851             32768  3.038    .1207 6.051 .4963 4.705e-06   121 438.4   \n    ltrunc  ltrunclen  total_train_updates  tpb  tps   ups  weighted_f1  \n         0          0                   94  374 1355 3.631        .9851\n\n18:23:59 | time:45s total_exs:2100 total_steps:105 epochs:87.50\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  \\\n                      1 10.47     1 249.4   964       0          0 77.31  220   \n    f1  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n     1             32768  2.711    .1207     6 .3607 5.254e-06   120 463.9   \n    ltrunc  ltrunclen  total_train_updates   tpb  tps   ups  weighted_f1  \n         0          0                  105 369.4 1428 3.896            1\n\n18:23:59 | running eval: valid\n18:23:59 | eval completed in 0.20s\n18:23:59 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1637       0          0 129.2   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .0809     6 .2905 5.254e-06    72 775.4       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    105  224 2412            1\n\u001b[0m\n18:23:59 | \u001b[1;32mnew best accuracy: 1 (previous best was 0.9583)\u001b[0m\n18:23:59 | saving best valid model: /tmp/model5\n18:24:09 | task solved! stopping.\n18:24:10 | \u001b[33mOverriding opt[\"init_model\"] to zoo:pretrained_transformers/bi_model_huge_reddit/model (previously: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model)\u001b[0m\n18:24:10 | \u001b[33mOverriding opt[\"betas\"] to (0.9, 0.999) (previously: [0.9, 0.999])\u001b[0m\n18:24:10 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: None,is_debug: False,evaltask: None,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: 7200.0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: 20.0,validation_every_n_steps: -1,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: -1,short_final_eval: False,validation_patience: 30,validation_metric: accuracy,validation_metric_mode: max,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: 10.0,mutators: None,fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt,fromfile_datatype_extension: False,interactive_candidates: fixed,encode_candidate_vecs_batchsize: 256,rank_top_k: -1,inference: max,topk: 5,return_cand_scores: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,fp16_impl: safe,force_fp16_tokens: True,adam_eps: 1e-08,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,classes: __notok__,__ok__,class_weights: None,ref_class: None,threshold: 0.5,print_scores: False,classes_from_file: None,ignore_labels: None,update_classifier_head_only: False,load_from_pretrained_ranker: True,dict_loaded: True,load_from_checkpoint: False,download_path: None,verbose: False,datapath: /opt/conda/lib/python3.7/site-packages/data,interactive_mode: False\u001b[0m\n18:24:10 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n--show-advanced-args False --task convai2 --numthreads 1 --batchsize 512 --model transformer/biencoder --single-turn False --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 --warmup-updates 100 --rank-candidates True --use-reply label --encode-candidate-vecs False --parlai-home /private/home/edinan/ParlAI\u001b[0m\n18:24:10 | Using CUDA\n18:24:10 | loading dictionary from /tmp/model5.dict\n18:24:10 | num words = 54944\n18:24:14 | Loading existing model parameters from /tmp/model5\n18:24:15 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:24:17 | creating task(s): fromfile:parlaiformat\n18:24:17 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:24:17 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt\n18:24:17 | running eval: valid\n18:24:17 | eval completed in 0.21s\n18:24:17 | \u001b[1mvalid:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1595       0          0 125.8   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2905 5.254e-06    72 755.3       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    105  224 2350            1\n\u001b[0m\n18:24:17 | creating task(s): fromfile:parlaiformat\n18:24:17 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:24:17 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt\n18:24:17 | running eval: test\n18:24:17 | eval completed in 0.21s\n18:24:17 | \u001b[1mtest:\n    accuracy  bleu-4  class___notok___f1  class___notok___prec  \\\n           1   1e-09                   1                     1   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                         1                1                  1   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  f1  \\\n                      1 10.67   152  1614       0          0 127.4   24   1   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n      .1294     6 .2905 5.254e-06    72 764.6       0          0   \n    total_train_updates  tpb  tps  weighted_f1  \n                    105  224 2379            1\n\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion A\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-a.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:24:19.378709Z","iopub.execute_input":"2022-12-03T18:24:19.379134Z","iopub.status.idle":"2022-12-03T18:24:46.819211Z","shell.execute_reply.started":"2022-12-03T18:24:19.379081Z","shell.execute_reply":"2022-12-03T18:24:46.818009Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stdout","text":"18:24:26 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-a.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt)\u001b[0m\n18:24:26 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:24:26 | Using CUDA\n18:24:26 | loading dictionary from /tmp/model5.dict\n18:24:26 | num words = 54944\n18:24:30 | Loading existing model parameters from /tmp/model5\n18:24:38 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:24:39 | Opt:\n18:24:39 |     activation: gelu\n18:24:39 |     adafactor_eps: '[1e-30, 0.001]'\n18:24:39 |     adam_eps: 1e-08\n18:24:39 |     add_p1_after_newln: False\n18:24:39 |     aggregate_micro: False\n18:24:39 |     allow_missing_init_opts: False\n18:24:39 |     area_under_curve_class: None\n18:24:39 |     area_under_curve_digits: -1\n18:24:39 |     attention_dropout: 0.1\n18:24:39 |     batchsize: 40\n18:24:39 |     betas: '[0.9, 0.999]'\n18:24:39 |     bpe_add_prefix_space: None\n18:24:39 |     bpe_debug: False\n18:24:39 |     bpe_dropout: None\n18:24:39 |     bpe_merge: None\n18:24:39 |     bpe_vocab: None\n18:24:39 |     candidates: inline\n18:24:39 |     cap_num_predictions: 100\n18:24:39 |     checkpoint_activations: False\n18:24:39 |     class_weights: None\n18:24:39 |     classes: \"['__notok__', '__ok__']\"\n18:24:39 |     classes_from_file: None\n18:24:39 |     data_parallel: True\n18:24:39 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:24:39 |     datatype: train\n18:24:39 |     delimiter: '\\n'\n18:24:39 |     dict_class: parlai.core.dict:DictionaryAgent\n18:24:39 |     dict_endtoken: __start__\n18:24:39 |     dict_file: /tmp/model5.dict\n18:24:39 |     dict_include_test: False\n18:24:39 |     dict_include_valid: False\n18:24:39 |     dict_initpath: None\n18:24:39 |     dict_language: english\n18:24:39 |     dict_loaded: True\n18:24:39 |     dict_lower: True\n18:24:39 |     dict_max_ngram_size: -1\n18:24:39 |     dict_maxexs: -1\n18:24:39 |     dict_maxtokens: -1\n18:24:39 |     dict_minfreq: 0\n18:24:39 |     dict_nulltoken: __null__\n18:24:39 |     dict_starttoken: __start__\n18:24:39 |     dict_textfields: text,labels\n18:24:39 |     dict_tokenizer: bpe\n18:24:39 |     dict_unktoken: __unk__\n18:24:39 |     display_examples: False\n18:24:39 |     download_path: None\n18:24:39 |     dropout: 0.1\n18:24:39 |     dynamic_batching: None\n18:24:39 |     embedding_projection: random\n18:24:39 |     embedding_size: 768\n18:24:39 |     embedding_type: random\n18:24:39 |     embeddings_scale: False\n18:24:39 |     encode_candidate_vecs: True\n18:24:39 |     encode_candidate_vecs_batchsize: 256\n18:24:39 |     eval_batchsize: None\n18:24:39 |     eval_candidates: inline\n18:24:39 |     eval_dynamic_batching: None\n18:24:39 |     evaltask: None\n18:24:39 |     ffn_size: 3072\n18:24:39 |     final_extra_opt: \n18:24:39 |     fixed_candidate_vecs: reuse\n18:24:39 |     fixed_candidates_path: None\n18:24:39 |     force_fp16_tokens: True\n18:24:39 |     fp16: True\n18:24:39 |     fp16_impl: safe\n18:24:39 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-a.txt\n18:24:39 |     fromfile_datatype_extension: False\n18:24:39 |     gpu: -1\n18:24:39 |     gradient_clip: 0.1\n18:24:39 |     hide_labels: False\n18:24:39 |     history_add_global_end_token: None\n18:24:39 |     history_reversed: False\n18:24:39 |     history_size: 20\n18:24:39 |     ignore_bad_candidates: False\n18:24:39 |     ignore_labels: None\n18:24:39 |     image_cropsize: 224\n18:24:39 |     image_mode: raw\n18:24:39 |     image_size: 256\n18:24:39 |     inference: max\n18:24:39 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:24:39 |     init_opt: None\n18:24:39 |     interactive_candidates: fixed\n18:24:39 |     interactive_mode: False\n18:24:39 |     invsqrt_lr_decay_gamma: -1\n18:24:39 |     is_debug: False\n18:24:39 |     label_truncate: 72\n18:24:39 |     learn_embeddings: True\n18:24:39 |     learn_positional_embeddings: True\n18:24:39 |     learningrate: 5e-05\n18:24:39 |     load_from_pretrained_ranker: True\n18:24:39 |     log_every_n_secs: 10.0\n18:24:39 |     log_every_n_steps: 50\n18:24:39 |     log_keep_fields: all\n18:24:39 |     loglevel: info\n18:24:39 |     lr_scheduler: reduceonplateau\n18:24:39 |     lr_scheduler_decay: 0.5\n18:24:39 |     lr_scheduler_patience: 3\n18:24:39 |     max_train_steps: -1\n18:24:39 |     max_train_time: 7200.0\n18:24:39 |     memory_attention: sqrt\n18:24:39 |     metrics: default\n18:24:39 |     model: transformer/classifier\n18:24:39 |     model_file: /tmp/model5\n18:24:39 |     model_parallel: False\n18:24:39 |     momentum: 0\n18:24:39 |     multitask_weights: [1]\n18:24:39 |     mutators: None\n18:24:39 |     n_decoder_layers: -1\n18:24:39 |     n_encoder_layers: -1\n18:24:39 |     n_heads: 12\n18:24:39 |     n_layers: 12\n18:24:39 |     n_positions: 1024\n18:24:39 |     n_segments: 2\n18:24:39 |     nesterov: True\n18:24:39 |     no_cuda: False\n18:24:39 |     normalize_sent_emb: False\n18:24:39 |     num_epochs: -1\n18:24:39 |     num_examples: -1\n18:24:39 |     num_workers: 0\n18:24:39 |     nus: [0.7]\n18:24:39 |     optimizer: adamax\n18:24:39 |     output_scaling: 0.06\n18:24:39 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-a.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n18:24:39 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:24:39 |     person_tokens: False\n18:24:39 |     print_scores: False\n18:24:39 |     rank_candidates: False\n18:24:39 |     rank_top_k: -1\n18:24:39 |     reduction_type: mean\n18:24:39 |     ref_class: None\n18:24:39 |     relu_dropout: 0.0\n18:24:39 |     repeat_blocking_heuristic: True\n18:24:39 |     report_filename: \n18:24:39 |     return_cand_scores: False\n18:24:39 |     save_after_valid: True\n18:24:39 |     save_every_n_secs: -1\n18:24:39 |     save_format: conversations\n18:24:39 |     share_encoders: False\n18:24:39 |     share_word_embeddings: False\n18:24:39 |     short_final_eval: False\n18:24:39 |     special_tok_lst: None\n18:24:39 |     split_lines: False\n18:24:39 |     starttime: Dec03_18-23\n18:24:39 |     task: fromfile:parlaiformat\n18:24:39 |     tensorboard_log: False\n18:24:39 |     tensorboard_logdir: None\n18:24:39 |     text_truncate: 360\n18:24:39 |     threshold: 0.5\n18:24:39 |     topk: 5\n18:24:39 |     train_predict: False\n18:24:39 |     truncate: 1024\n18:24:39 |     update_classifier_head_only: False\n18:24:39 |     update_freq: 1\n18:24:39 |     use_memories: False\n18:24:39 |     use_reply: none\n18:24:39 |     validation_cutoff: 1.0\n18:24:39 |     validation_every_n_epochs: -1\n18:24:39 |     validation_every_n_secs: 20.0\n18:24:39 |     validation_every_n_steps: -1\n18:24:39 |     validation_max_exs: -1\n18:24:39 |     validation_metric: accuracy\n18:24:39 |     validation_metric_mode: max\n18:24:39 |     validation_patience: 30\n18:24:39 |     validation_share_agent: False\n18:24:39 |     variant: xlm\n18:24:39 |     verbose: False\n18:24:39 |     wandb_entity: None\n18:24:39 |     wandb_log: False\n18:24:39 |     wandb_name: None\n18:24:39 |     wandb_project: None\n18:24:39 |     warmup_rate: 0.0001\n18:24:39 |     warmup_updates: 1000\n18:24:39 |     weight_decay: None\n18:24:39 |     world_logs: \n18:24:39 |     wrap_memory_encoder: False\n18:24:39 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:24:39 | creating task(s): fromfile:parlaiformat\n18:24:39 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:24:39 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-a.txt\n18:24:45 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1350 1.35e-10               .1801                 .1696   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1919           .08466             .09091   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                 .07921 11.79 551.8 540.3       0          0 39.17  200 .1350   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9539 5.254e-06 239.6 234.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    105 791.4  775        .1319\u001b[0m\n18:24:45 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .1350 1.35e-10               .1801                 .1696   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .1919           .08466             .09091   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                 .07921 11.79 551.8 540.3       0          0 39.17  200 .1350   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  5.99 .9539 5.254e-06 239.6 234.6       0          0   \n    total_train_updates   tpb  tps  weighted_f1  \n                    105 791.4  775        .1319\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate completion B\n!parlai eval_model -t fromfile:parlaiformat --fromfile-datapath ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-b.txt -m transformer/classifier -mf /tmp/model5 -bs 40","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:24:46.820826Z","iopub.execute_input":"2022-12-03T18:24:46.821226Z","iopub.status.idle":"2022-12-03T18:25:13.318652Z","shell.execute_reply.started":"2022-12-03T18:24:46.821182Z","shell.execute_reply":"2022-12-03T18:25:13.317238Z"},"trusted":true},"execution_count":165,"outputs":[{"name":"stdout","text":"18:24:54 | \u001b[33mOverriding opt[\"fromfile_datapath\"] to ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-b.txt (previously: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_valid.txt)\u001b[0m\n18:24:54 | \u001b[33mOverriding opt[\"batchsize\"] to 40 (previously: 20)\u001b[0m\n18:24:54 | Using CUDA\n18:24:54 | loading dictionary from /tmp/model5.dict\n18:24:54 | num words = 54944\n18:24:58 | Loading existing model parameters from /tmp/model5\n18:25:04 | Total parameters: 128,042,498 (128,042,498 trainable)\n18:25:05 | Opt:\n18:25:05 |     activation: gelu\n18:25:05 |     adafactor_eps: '[1e-30, 0.001]'\n18:25:05 |     adam_eps: 1e-08\n18:25:05 |     add_p1_after_newln: False\n18:25:05 |     aggregate_micro: False\n18:25:05 |     allow_missing_init_opts: False\n18:25:05 |     area_under_curve_class: None\n18:25:05 |     area_under_curve_digits: -1\n18:25:05 |     attention_dropout: 0.1\n18:25:05 |     batchsize: 40\n18:25:05 |     betas: '[0.9, 0.999]'\n18:25:05 |     bpe_add_prefix_space: None\n18:25:05 |     bpe_debug: False\n18:25:05 |     bpe_dropout: None\n18:25:05 |     bpe_merge: None\n18:25:05 |     bpe_vocab: None\n18:25:05 |     candidates: inline\n18:25:05 |     cap_num_predictions: 100\n18:25:05 |     checkpoint_activations: False\n18:25:05 |     class_weights: None\n18:25:05 |     classes: \"['__notok__', '__ok__']\"\n18:25:05 |     classes_from_file: None\n18:25:05 |     data_parallel: True\n18:25:05 |     datapath: /opt/conda/lib/python3.7/site-packages/data\n18:25:05 |     datatype: train\n18:25:05 |     delimiter: '\\n'\n18:25:05 |     dict_class: parlai.core.dict:DictionaryAgent\n18:25:05 |     dict_endtoken: __start__\n18:25:05 |     dict_file: /tmp/model5.dict\n18:25:05 |     dict_include_test: False\n18:25:05 |     dict_include_valid: False\n18:25:05 |     dict_initpath: None\n18:25:05 |     dict_language: english\n18:25:05 |     dict_loaded: True\n18:25:05 |     dict_lower: True\n18:25:05 |     dict_max_ngram_size: -1\n18:25:05 |     dict_maxexs: -1\n18:25:05 |     dict_maxtokens: -1\n18:25:05 |     dict_minfreq: 0\n18:25:05 |     dict_nulltoken: __null__\n18:25:05 |     dict_starttoken: __start__\n18:25:05 |     dict_textfields: text,labels\n18:25:05 |     dict_tokenizer: bpe\n18:25:05 |     dict_unktoken: __unk__\n18:25:05 |     display_examples: False\n18:25:05 |     download_path: None\n18:25:05 |     dropout: 0.1\n18:25:05 |     dynamic_batching: None\n18:25:05 |     embedding_projection: random\n18:25:05 |     embedding_size: 768\n18:25:05 |     embedding_type: random\n18:25:05 |     embeddings_scale: False\n18:25:05 |     encode_candidate_vecs: True\n18:25:05 |     encode_candidate_vecs_batchsize: 256\n18:25:05 |     eval_batchsize: None\n18:25:05 |     eval_candidates: inline\n18:25:05 |     eval_dynamic_batching: None\n18:25:05 |     evaltask: None\n18:25:05 |     ffn_size: 3072\n18:25:05 |     final_extra_opt: \n18:25:05 |     fixed_candidate_vecs: reuse\n18:25:05 |     fixed_candidates_path: None\n18:25:05 |     force_fp16_tokens: True\n18:25:05 |     fp16: True\n18:25:05 |     fp16_impl: safe\n18:25:05 |     fromfile_datapath: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-b.txt\n18:25:05 |     fromfile_datatype_extension: False\n18:25:05 |     gpu: -1\n18:25:05 |     gradient_clip: 0.1\n18:25:05 |     hide_labels: False\n18:25:05 |     history_add_global_end_token: None\n18:25:05 |     history_reversed: False\n18:25:05 |     history_size: 20\n18:25:05 |     ignore_bad_candidates: False\n18:25:05 |     ignore_labels: None\n18:25:05 |     image_cropsize: 224\n18:25:05 |     image_mode: raw\n18:25:05 |     image_size: 256\n18:25:05 |     inference: max\n18:25:05 |     init_model: /opt/conda/lib/python3.7/site-packages/data/models/pretrained_transformers/bi_model_huge_reddit/model\n18:25:05 |     init_opt: None\n18:25:05 |     interactive_candidates: fixed\n18:25:05 |     interactive_mode: False\n18:25:05 |     invsqrt_lr_decay_gamma: -1\n18:25:05 |     is_debug: False\n18:25:05 |     label_truncate: 72\n18:25:05 |     learn_embeddings: True\n18:25:05 |     learn_positional_embeddings: True\n18:25:05 |     learningrate: 5e-05\n18:25:05 |     load_from_pretrained_ranker: True\n18:25:05 |     log_every_n_secs: 10.0\n18:25:05 |     log_every_n_steps: 50\n18:25:05 |     log_keep_fields: all\n18:25:05 |     loglevel: info\n18:25:05 |     lr_scheduler: reduceonplateau\n18:25:05 |     lr_scheduler_decay: 0.5\n18:25:05 |     lr_scheduler_patience: 3\n18:25:05 |     max_train_steps: -1\n18:25:05 |     max_train_time: 7200.0\n18:25:05 |     memory_attention: sqrt\n18:25:05 |     metrics: default\n18:25:05 |     model: transformer/classifier\n18:25:05 |     model_file: /tmp/model5\n18:25:05 |     model_parallel: False\n18:25:05 |     momentum: 0\n18:25:05 |     multitask_weights: [1]\n18:25:05 |     mutators: None\n18:25:05 |     n_decoder_layers: -1\n18:25:05 |     n_encoder_layers: -1\n18:25:05 |     n_heads: 12\n18:25:05 |     n_layers: 12\n18:25:05 |     n_positions: 1024\n18:25:05 |     n_segments: 2\n18:25:05 |     nesterov: True\n18:25:05 |     no_cuda: False\n18:25:05 |     normalize_sent_emb: False\n18:25:05 |     num_epochs: -1\n18:25:05 |     num_examples: -1\n18:25:05 |     num_workers: 0\n18:25:05 |     nus: [0.7]\n18:25:05 |     optimizer: adamax\n18:25:05 |     output_scaling: 0.06\n18:25:05 |     override: \"{'task': 'fromfile:parlaiformat', 'fromfile_datapath': '../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-b.txt', 'model': 'transformer/classifier', 'model_file': '/tmp/model5', 'batchsize': 40}\"\n18:25:05 |     parlai_home: /opt/conda/lib/python3.7/site-packages\n18:25:05 |     person_tokens: False\n18:25:05 |     print_scores: False\n18:25:05 |     rank_candidates: False\n18:25:05 |     rank_top_k: -1\n18:25:05 |     reduction_type: mean\n18:25:05 |     ref_class: None\n18:25:05 |     relu_dropout: 0.0\n18:25:05 |     repeat_blocking_heuristic: True\n18:25:05 |     report_filename: \n18:25:05 |     return_cand_scores: False\n18:25:05 |     save_after_valid: True\n18:25:05 |     save_every_n_secs: -1\n18:25:05 |     save_format: conversations\n18:25:05 |     share_encoders: False\n18:25:05 |     share_word_embeddings: False\n18:25:05 |     short_final_eval: False\n18:25:05 |     special_tok_lst: None\n18:25:05 |     split_lines: False\n18:25:05 |     starttime: Dec03_18-23\n18:25:05 |     task: fromfile:parlaiformat\n18:25:05 |     tensorboard_log: False\n18:25:05 |     tensorboard_logdir: None\n18:25:05 |     text_truncate: 360\n18:25:05 |     threshold: 0.5\n18:25:05 |     topk: 5\n18:25:05 |     train_predict: False\n18:25:05 |     truncate: 1024\n18:25:05 |     update_classifier_head_only: False\n18:25:05 |     update_freq: 1\n18:25:05 |     use_memories: False\n18:25:05 |     use_reply: none\n18:25:05 |     validation_cutoff: 1.0\n18:25:05 |     validation_every_n_epochs: -1\n18:25:05 |     validation_every_n_secs: 20.0\n18:25:05 |     validation_every_n_steps: -1\n18:25:05 |     validation_max_exs: -1\n18:25:05 |     validation_metric: accuracy\n18:25:05 |     validation_metric_mode: max\n18:25:05 |     validation_patience: 30\n18:25:05 |     validation_share_agent: False\n18:25:05 |     variant: xlm\n18:25:05 |     verbose: False\n18:25:05 |     wandb_entity: None\n18:25:05 |     wandb_log: False\n18:25:05 |     wandb_name: None\n18:25:05 |     wandb_project: None\n18:25:05 |     warmup_rate: 0.0001\n18:25:05 |     warmup_updates: 1000\n18:25:05 |     weight_decay: None\n18:25:05 |     world_logs: \n18:25:05 |     wrap_memory_encoder: False\n18:25:05 | Evaluating task fromfile:parlaiformat using datatype valid.\n18:25:05 | creating task(s): fromfile:parlaiformat\n18:25:05 | \u001b[33mYou are using this fromfile data as a valid or test set without setting fromfile_datatype_extension to true. Please be aware this uses directly the file you indicated, make sure this is not the same as your training file.\u001b[0m\n18:25:05 | Loading ParlAI text data: ../input/comp599projproposed/proposed/prev2corr2type2/run5/data_train-b.txt\n18:25:11 | \u001b[1mReport for fromfile:parlaiformat:\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8650 8.65e-10               .8732                 .8304   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9208            .8556              .9091   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8081 11.79 551.8 477.2       0          0 34.59  200 .8650   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5148 5.254e-06 240.4 207.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    105 792.2 685.1        .8645\u001b[0m\n18:25:11 | Finished evaluating tasks ['fromfile:parlaiformat'] using datatype valid\n    accuracy   bleu-4  class___notok___f1  class___notok___prec  \\\n       .8650 8.65e-10               .8732                 .8304   \n    class___notok___recall  class___ok___f1  class___ok___prec  \\\n                     .9208            .8556              .9091   \n    class___ok___recall  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs    f1  \\\n                  .8081 11.79 551.8 477.2       0          0 34.59  200 .8650   \n    gpu_mem  llen  loss        lr  ltpb  ltps  ltrunc  ltrunclen  \\\n     .08267  6.01 .5148 5.254e-06 240.4 207.9       0          0   \n    total_train_updates   tpb   tps  weighted_f1  \n                    105 792.2 685.1        .8645\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean up to make sure to not affect later runs\n!rm /tmp/model5*","metadata":{"execution":{"iopub.status.busy":"2022-12-03T18:25:13.320622Z","iopub.execute_input":"2022-12-03T18:25:13.321078Z","iopub.status.idle":"2022-12-03T18:25:14.484882Z","shell.execute_reply.started":"2022-12-03T18:25:13.321023Z","shell.execute_reply":"2022-12-03T18:25:14.483593Z"},"trusted":true},"execution_count":166,"outputs":[]}]}